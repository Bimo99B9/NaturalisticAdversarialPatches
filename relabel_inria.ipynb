{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont\n",
    "import glob\n",
    "from torchvision.transforms.functional import resize, pad\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(model_name):\n",
    "    # Define model path based on model name\n",
    "    model_path = f\"{model_name}.pt\"  # Adjust path as needed\n",
    "    yolo_model = YOLO(model_path)\n",
    "    return yolo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad(image, stride=32, max_size=640):\n",
    "    # Resize the image, maintaining aspect ratio\n",
    "    ratio = min(max_size / image.size[0], max_size / image.size[1])\n",
    "    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "    resized_image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Pad the resized image to be divisible by the stride\n",
    "    width, height = resized_image.size\n",
    "    new_width = width if width % stride == 0 else width + stride - width % stride\n",
    "    new_height = height if height % stride == 0 else height + stride - height % stride\n",
    "    padded_image = ImageOps.expand(resized_image, border=(0, 0, new_width - width, new_height - height), fill=0)\n",
    "    return padded_image, resized_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_and_save_labels(image_path, yolo_model, output_dir):\n",
    "#     image = Image.open(image_path)\n",
    "#     resized_padded_image = resize_and_pad(image)\n",
    "\n",
    "#     # Convert image to tensor\n",
    "#     transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     img_tensor = transform(resized_padded_image).unsqueeze(0)\n",
    "\n",
    "#     # Predict using the YOLO model\n",
    "#     results = yolo_model(img_tensor)\n",
    "\n",
    "#     # Ensure the first result object is correctly accessed\n",
    "#     if results:\n",
    "#         detection_results = results[0]\n",
    "\n",
    "#         # Create the label file\n",
    "#         label_file = os.path.splitext(os.path.basename(image_path))[0] + \".txt\"\n",
    "#         label_path = os.path.join(output_dir, label_file)\n",
    "\n",
    "#         with open(label_path, 'w') as f:\n",
    "#             for i, bbox in enumerate(detection_results.boxes.xyxy):\n",
    "#                 class_id = detection_results.boxes.cls[i]\n",
    "#                 class_name = results[0].names[class_id.item() if isinstance(class_id, torch.Tensor) else class_id]\n",
    "#                 conf = results[0].boxes.conf[i]\n",
    "                \n",
    "#                 x1, y1, x2, y2 = bbox.tolist()\n",
    "\n",
    "#                 # Only write 'person' detections with confidence > 0.5\n",
    "#                 if class_name == \"person\" and conf > 0.5:\n",
    "#                     scale_x = image.size[0] / resized_padded_image.size[0]\n",
    "#                     scale_y = image.size[1] / resized_padded_image.size[1]\n",
    "#                     x_center = ((x1 + x2) / 2) * scale_x\n",
    "#                     y_center = ((y1 + y2) / 2) * scale_y\n",
    "#                     width = (x2 - x1) * scale_x\n",
    "#                     height = (y2 - y1) * scale_y\n",
    "#                     f.write(f\"person {x_center:.2f} {y_center:.2f} {width:.2f} {height:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_save_labels(image_path, yolo_model, output_dir):\n",
    "    image = Image.open(image_path)\n",
    "    # Predict using the YOLO model\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    if results:\n",
    "        detection_results = results[0]\n",
    "        # orig_width, orig_height = resized_size\n",
    "\n",
    "        # Create the label file\n",
    "        label_file = os.path.splitext(os.path.basename(image_path))[0] + \".txt\"\n",
    "        label_path = os.path.join(output_dir, label_file)\n",
    "\n",
    "        labels_rescale = []\n",
    "        with open(label_path, 'w') as f:\n",
    "            for i, box in enumerate(detection_results.boxes.xyxy):\n",
    "                cls_id = detection_results.boxes.cls[i].cpu().item()\n",
    "                conf = detection_results.boxes.conf[i].cpu().item()\n",
    "                x1, y1, x2, y2 = box.cpu().tolist()\n",
    "\n",
    "                print(f\"Class: {cls_id}, Confidence: {conf:.2f}, Bounding Box: {x1:.2f}, {y1:.2f}, {x2:.2f}, {y2:.2f} for {image_path}\")\n",
    "\n",
    "                # Calculate original scale bounding box\n",
    "                # x1_orig = (x1 / resized_padded_image.size[0]) * orig_width\n",
    "                # y1_orig = (y1 / resized_padded_image.size[1]) * orig_height\n",
    "                # x2_orig = (x2 / resized_padded_image.size[0]) * orig_width\n",
    "                # y2_orig = (y2 / resized_padded_image.size[1]) * orig_height\n",
    "\n",
    "                # print(f\"Original Scale Bounding Box: {x1_orig:.2f}, {y1_orig:.2f}, {x2_orig:.2f}, {y2_orig:.2f} for {image_path}\")\n",
    "\n",
    "                # Append to list and write to file\n",
    "                # labels_rescale.append([cls_id, x1_orig, y1_orig, x2_orig, y2_orig])\n",
    "                f.write(f\"person {x1:.2f} {y1:.2f} {x2:.2f} {y2:.2f}\\n\")\n",
    "\n",
    "        # Convert labels_rescale to numpy array if needed\n",
    "        labels_rescale = np.array(labels_rescale, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x544 8 persons, 1 tie, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 265.26, 120.14, 506.55, 710.75 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 30.17, 325.73, 211.92, 716.71 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 165.59, 177.87, 294.45, 650.88 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 499.16, 286.38, 818.00, 976.00 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 2.69, 134.45, 183.69, 474.33 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 27.0, Confidence: 0.60, Bounding Box: 195.42, 274.61, 222.28, 386.64 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 0.00, 161.72, 61.35, 393.77 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 0.00, 160.27, 61.38, 493.76 for dataset/inria/Train/pos\\crop001001.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 0.00, 151.63, 66.04, 613.38 for dataset/inria/Train/pos\\crop001001.png\n",
      "\n",
      "0: 512x640 10 persons, 2 handbags, 1 potted plant, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 988.55, 56.99, 1159.81, 586.58 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 71.68, 96.33, 286.64, 604.07 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 0.00, 146.47, 104.08, 548.26 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 846.32, 98.36, 1003.59, 586.98 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 58.0, Confidence: 0.71, Bounding Box: 1102.57, 140.84, 1238.86, 585.36 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 279.57, 240.00, 705.23, 901.82 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 767.98, 162.38, 899.87, 573.74 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 581.05, 166.76, 961.84, 950.00 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 627.52, 80.05, 744.91, 274.78 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 274.90, 173.96, 963.04, 943.26 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 26.0, Confidence: 0.46, Bounding Box: 896.98, 182.76, 996.17, 368.94 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 738.83, 173.19, 792.62, 284.37 for dataset/inria/Train/pos\\crop001002.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 800.62, 314.69, 843.23, 396.40 for dataset/inria/Train/pos\\crop001002.png\n",
      "\n",
      "0: 640x576 7 persons, 2 handbags, 1 potted plant, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 448.14, 68.55, 659.26, 708.44 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 58.0, Confidence: 0.89, Bounding Box: 589.50, 201.71, 849.78, 706.05 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 1.58, 217.64, 367.25, 976.00 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 290.50, 118.84, 465.43, 711.02 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 0.00, 95.38, 161.26, 331.77 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 181.82, 184.91, 326.86, 679.73 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 153.99, 201.97, 223.49, 345.87 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 26.0, Confidence: 0.59, Bounding Box: 344.51, 226.97, 459.17, 440.78 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 26.0, Confidence: 0.53, Bounding Box: 226.65, 367.00, 278.75, 475.79 for dataset/inria/Train/pos\\crop001003.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 736.86, 94.41, 850.59, 286.74 for dataset/inria/Train/pos\\crop001003.png\n",
      "\n",
      "0: 640x576 6 persons, 1 potted plant, 12.5ms\n",
      "Speed: 4.0ms preprocess, 12.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 336.56, 138.15, 497.80, 728.39 for dataset/inria/Train/pos\\crop001004.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 464.37, 186.27, 579.89, 746.78 for dataset/inria/Train/pos\\crop001004.png\n",
      "Class: 58.0, Confidence: 0.82, Bounding Box: 0.00, 159.27, 378.39, 772.47 for dataset/inria/Train/pos\\crop001004.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 682.67, 243.91, 835.00, 975.89 for dataset/inria/Train/pos\\crop001004.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 567.75, 190.35, 705.54, 416.19 for dataset/inria/Train/pos\\crop001004.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 564.35, 328.05, 700.61, 748.69 for dataset/inria/Train/pos\\crop001004.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 669.17, 132.73, 794.64, 303.18 for dataset/inria/Train/pos\\crop001004.png\n",
      "\n",
      "0: 640x608 13 persons, 2 ties, 25.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 40.01, 104.91, 163.18, 553.68 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 753.03, 78.60, 889.74, 590.69 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 213.42, 144.72, 321.18, 325.36 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 137.18, 140.85, 227.68, 573.48 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 333.64, 150.43, 636.57, 947.65 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 211.76, 251.51, 316.98, 595.45 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 273.74, 184.11, 455.60, 937.89 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 598.94, 102.26, 707.89, 584.70 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 652.76, 100.91, 818.58, 599.70 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 27.0, Confidence: 0.60, Bounding Box: 681.14, 202.47, 720.69, 353.63 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 556.48, 154.86, 621.38, 257.89 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 292.40, 102.34, 388.24, 319.89 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 293.47, 100.25, 387.76, 240.31 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 388.76, 114.14, 474.64, 343.23 for dataset/inria/Train/pos\\crop001005.png\n",
      "Class: 27.0, Confidence: 0.30, Bounding Box: 651.81, 193.00, 703.92, 362.13 for dataset/inria/Train/pos\\crop001005.png\n",
      "\n",
      "0: 640x640 10 persons, 1 suitcase, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 558.20, 87.49, 746.06, 691.34 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 361.54, 121.39, 488.95, 631.64 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 13.36, 216.32, 195.94, 976.00 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 420.38, 112.84, 615.50, 708.14 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 119.61, 149.75, 404.18, 976.00 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 309.28, 179.89, 381.52, 304.60 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 376.13, 113.89, 588.90, 702.40 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 0.00, 207.83, 404.89, 975.58 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 28.0, Confidence: 0.49, Bounding Box: 684.89, 542.45, 917.39, 854.65 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 2.83, 118.07, 114.23, 293.78 for dataset/inria/Train/pos\\crop001006.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 113.67, 133.15, 222.50, 391.31 for dataset/inria/Train/pos\\crop001006.png\n",
      "\n",
      "0: 416x640 12 persons, 2 skiss, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 728.71, 278.94, 911.35, 763.14 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 902.14, 288.73, 1146.14, 764.04 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 231.32, 234.77, 401.29, 723.48 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 385.97, 207.85, 552.11, 730.08 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 557.08, 224.10, 727.68, 733.72 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 76.22, 361.46, 103.87, 431.81 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 148.20, 384.25, 178.87, 446.45 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 1115.31, 202.36, 1156.61, 318.24 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 30.0, Confidence: 0.45, Bounding Box: 235.65, 695.74, 452.35, 744.51 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 1095.81, 253.51, 1126.20, 315.25 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 41.71, 304.18, 54.32, 338.93 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 1197.27, 192.20, 1255.19, 319.16 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 1154.76, 202.10, 1187.46, 312.16 for dataset/inria/Train/pos\\crop001007.png\n",
      "Class: 30.0, Confidence: 0.25, Bounding Box: 266.07, 694.53, 403.63, 742.71 for dataset/inria/Train/pos\\crop001007.png\n",
      "\n",
      "0: 640x640 7 persons, 2 skiss, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 526.54, 48.54, 712.33, 697.40 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 6.18, 288.71, 59.57, 422.53 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 357.50, 324.30, 412.46, 445.92 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 214.24, 283.76, 266.33, 416.65 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 142.19, 164.30, 172.43, 237.83 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 7.91, 153.04, 35.54, 226.59 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 55.58, 166.18, 93.63, 230.05 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 30.0, Confidence: 0.30, Bounding Box: 358.32, 434.74, 431.14, 451.34 for dataset/inria/Train/pos\\crop001008.png\n",
      "Class: 30.0, Confidence: 0.25, Bounding Box: 1.30, 406.24, 52.21, 425.70 for dataset/inria/Train/pos\\crop001008.png\n",
      "\n",
      "0: 640x512 1 person, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 198.72, 156.27, 294.88, 473.63 for dataset/inria/Train/pos\\crop001009.png\n",
      "\n",
      "0: 640x448 1 person, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 166.00, 194.57, 421.64, 717.05 for dataset/inria/Train/pos\\crop001010.png\n",
      "\n",
      "0: 480x640 4 persons, 3 cars, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 702.87, 115.66, 929.54, 825.30 for dataset/inria/Train/pos\\crop001011.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 168.10, 168.84, 436.51, 779.64 for dataset/inria/Train/pos\\crop001011.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 497.61, 162.03, 758.94, 777.79 for dataset/inria/Train/pos\\crop001011.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 421.39, 313.46, 559.40, 732.71 for dataset/inria/Train/pos\\crop001011.png\n",
      "Class: 2.0, Confidence: 0.36, Bounding Box: 75.14, 320.38, 119.94, 361.86 for dataset/inria/Train/pos\\crop001011.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 31.31, 308.26, 87.89, 361.33 for dataset/inria/Train/pos\\crop001011.png\n",
      "Class: 2.0, Confidence: 0.26, Bounding Box: 44.86, 319.38, 87.31, 361.75 for dataset/inria/Train/pos\\crop001011.png\n",
      "\n",
      "0: 640x544 1 person, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 306.51, 272.38, 516.40, 765.89 for dataset/inria/Train/pos\\crop001012.png\n",
      "\n",
      "0: 640x512 1 person, 14.2ms\n",
      "Speed: 2.6ms preprocess, 14.2ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 181.54, 159.05, 559.04, 854.65 for dataset/inria/Train/pos\\crop001013.png\n",
      "\n",
      "0: 640x576 1 person, 1 backpack, 16.9ms\n",
      "Speed: 5.5ms preprocess, 16.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 310.89, 312.54, 509.95, 702.70 for dataset/inria/Train/pos\\crop001014.png\n",
      "Class: 24.0, Confidence: 0.49, Bounding Box: 308.27, 321.20, 413.87, 445.95 for dataset/inria/Train/pos\\crop001014.png\n",
      "\n",
      "0: 640x416 1 person, 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 244.57, 334.36, 371.56, 661.73 for dataset/inria/Train/pos\\crop001015.png\n",
      "\n",
      "0: 640x480 2 persons, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 258.10, 312.47, 410.41, 701.70 for dataset/inria/Train/pos\\crop001016.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 279.78, 242.53, 414.00, 412.65 for dataset/inria/Train/pos\\crop001016.png\n",
      "\n",
      "0: 640x448 1 person, 34.0ms\n",
      "Speed: 3.0ms preprocess, 34.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 186.19, 253.64, 439.47, 966.53 for dataset/inria/Train/pos\\crop001017.png\n",
      "\n",
      "0: 640x480 1 person, 14.5ms\n",
      "Speed: 3.0ms preprocess, 14.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 183.71, 178.37, 533.41, 968.56 for dataset/inria/Train/pos\\crop001018.png\n",
      "\n",
      "0: 640x448 1 person, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 186.99, 155.45, 526.82, 956.91 for dataset/inria/Train/pos\\crop001019.png\n",
      "\n",
      "0: 640x512 1 person, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 277.15, 268.47, 473.19, 918.34 for dataset/inria/Train/pos\\crop001020.png\n",
      "\n",
      "0: 640x640 1 person, 12.0ms\n",
      "Speed: 5.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 318.32, 118.61, 731.94, 820.76 for dataset/inria/Train/pos\\crop001021.png\n",
      "\n",
      "0: 640x480 3 persons, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 278.63, 251.49, 481.43, 761.18 for dataset/inria/Train/pos\\crop001022.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 237.42, 294.67, 352.09, 723.23 for dataset/inria/Train/pos\\crop001022.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 333.51, 299.15, 364.90, 352.86 for dataset/inria/Train/pos\\crop001022.png\n",
      "\n",
      "0: 608x640 3 persons, 2 backpacks, 1 handbag, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 257.17, 222.38, 510.37, 853.24 for dataset/inria/Train/pos\\crop001023.png\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 938.77, 338.24, 1079.53, 964.84 for dataset/inria/Train/pos\\crop001023.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 584.10, 226.28, 831.48, 883.61 for dataset/inria/Train/pos\\crop001023.png\n",
      "Class: 24.0, Confidence: 0.65, Bounding Box: 749.80, 294.74, 899.96, 528.37 for dataset/inria/Train/pos\\crop001023.png\n",
      "Class: 24.0, Confidence: 0.31, Bounding Box: 709.03, 293.94, 900.24, 586.58 for dataset/inria/Train/pos\\crop001023.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 180.32, 638.51, 410.47, 856.71 for dataset/inria/Train/pos\\crop001023.png\n",
      "\n",
      "0: 640x544 1 person, 1 backpack, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 315.22, 353.06, 530.45, 970.58 for dataset/inria/Train/pos\\crop001024.png\n",
      "Class: 24.0, Confidence: 0.29, Bounding Box: 369.67, 439.53, 489.19, 572.09 for dataset/inria/Train/pos\\crop001024.png\n",
      "\n",
      "0: 640x544 1 person, 1 sheep, 10.8ms\n",
      "Speed: 2.9ms preprocess, 10.8ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 283.50, 293.61, 511.40, 700.27 for dataset/inria/Train/pos\\crop001025.png\n",
      "Class: 18.0, Confidence: 0.53, Bounding Box: 94.77, 950.78, 261.43, 975.73 for dataset/inria/Train/pos\\crop001025.png\n",
      "\n",
      "0: 640x608 4 persons, 1 backpack, 11.4ms\n",
      "Speed: 3.2ms preprocess, 11.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 297.78, 258.46, 506.58, 685.02 for dataset/inria/Train/pos\\crop001026.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 495.92, 296.94, 644.75, 655.95 for dataset/inria/Train/pos\\crop001026.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 644.55, 342.52, 689.34, 458.66 for dataset/inria/Train/pos\\crop001026.png\n",
      "Class: 24.0, Confidence: 0.40, Bounding Box: 523.42, 308.63, 606.57, 440.27 for dataset/inria/Train/pos\\crop001026.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 628.02, 329.31, 689.50, 464.95 for dataset/inria/Train/pos\\crop001026.png\n",
      "\n",
      "0: 640x576 4 persons, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 202.58, 230.07, 664.06, 808.77 for dataset/inria/Train/pos\\crop001027.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 136.39, 605.20, 302.71, 778.32 for dataset/inria/Train/pos\\crop001027.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 490.28, 625.91, 758.52, 843.20 for dataset/inria/Train/pos\\crop001027.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 518.44, 626.39, 690.88, 826.45 for dataset/inria/Train/pos\\crop001027.png\n",
      "\n",
      "0: 640x608 2 persons, 1 bottle, 11.2ms\n",
      "Speed: 4.3ms preprocess, 11.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 691.47, 191.52, 903.39, 973.63 for dataset/inria/Train/pos\\crop001028.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 290.14, 268.25, 561.55, 918.74 for dataset/inria/Train/pos\\crop001028.png\n",
      "Class: 39.0, Confidence: 0.38, Bounding Box: 132.52, 905.10, 169.54, 975.53 for dataset/inria/Train/pos\\crop001028.png\n",
      "\n",
      "0: 640x544 1 person, 1 bird, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 300.20, 301.66, 508.20, 731.85 for dataset/inria/Train/pos\\crop001029.png\n",
      "Class: 14.0, Confidence: 0.56, Bounding Box: 374.07, 8.13, 412.46, 58.15 for dataset/inria/Train/pos\\crop001029.png\n",
      "\n",
      "0: 640x640 6 persons, 3 backpacks, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 304.96, 241.47, 407.02, 676.54 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 578.18, 261.33, 688.79, 599.44 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 24.0, Confidence: 0.76, Bounding Box: 249.65, 300.53, 329.82, 451.45 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 0.00, 0.91, 110.98, 370.04 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 24.0, Confidence: 0.48, Bounding Box: 539.54, 321.81, 618.07, 457.65 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 24.0, Confidence: 0.44, Bounding Box: 250.08, 298.01, 362.56, 451.92 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 0.66, 0.38, 57.44, 171.97 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 1.05, 340.44, 110.85, 461.47 for dataset/inria/Train/pos\\crop001030.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 0.01, 132.20, 113.17, 460.39 for dataset/inria/Train/pos\\crop001030.png\n",
      "\n",
      "0: 640x576 4 persons, 2 backpacks, 1 frisbee, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 285.65, 537.32, 512.35, 865.54 for dataset/inria/Train/pos\\crop001031.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 304.11, 248.57, 452.42, 646.66 for dataset/inria/Train/pos\\crop001031.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 176.63, 616.51, 503.62, 973.24 for dataset/inria/Train/pos\\crop001031.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 724.29, 725.63, 836.19, 975.62 for dataset/inria/Train/pos\\crop001031.png\n",
      "Class: 24.0, Confidence: 0.60, Bounding Box: 668.57, 793.28, 758.33, 960.32 for dataset/inria/Train/pos\\crop001031.png\n",
      "Class: 29.0, Confidence: 0.44, Bounding Box: 614.26, 577.51, 653.27, 624.05 for dataset/inria/Train/pos\\crop001031.png\n",
      "Class: 24.0, Confidence: 0.31, Bounding Box: 669.05, 792.24, 825.98, 967.46 for dataset/inria/Train/pos\\crop001031.png\n",
      "\n",
      "0: 640x576 2 persons, 2 chairs, 1 microwave, 1 refrigerator, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 56.0, Confidence: 0.89, Bounding Box: 177.98, 524.15, 354.51, 801.79 for dataset/inria/Train/pos\\crop001032.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 320.73, 215.37, 528.23, 785.46 for dataset/inria/Train/pos\\crop001032.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 1.57, 421.94, 197.59, 896.08 for dataset/inria/Train/pos\\crop001032.png\n",
      "Class: 68.0, Confidence: 0.52, Bounding Box: 640.68, 422.64, 725.89, 491.54 for dataset/inria/Train/pos\\crop001032.png\n",
      "Class: 72.0, Confidence: 0.33, Bounding Box: 538.35, 361.50, 667.88, 709.14 for dataset/inria/Train/pos\\crop001032.png\n",
      "Class: 56.0, Confidence: 0.32, Bounding Box: 0.55, 423.38, 199.58, 895.36 for dataset/inria/Train/pos\\crop001032.png\n",
      "\n",
      "0: 640x608 1 person, 11.1ms\n",
      "Speed: 3.0ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 329.73, 239.41, 615.17, 897.62 for dataset/inria/Train/pos\\crop001033.png\n",
      "\n",
      "0: 640x544 1 person, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 301.33, 240.19, 470.49, 731.27 for dataset/inria/Train/pos\\crop001034.png\n",
      "\n",
      "0: 640x416 1 person, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 214.79, 192.47, 452.36, 971.49 for dataset/inria/Train/pos\\crop001035.png\n",
      "\n",
      "0: 640x448 1 person, 1 bottle, 2 vases, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 233.67, 283.71, 397.10, 691.63 for dataset/inria/Train/pos\\crop001036.png\n",
      "Class: 75.0, Confidence: 0.46, Bounding Box: 499.14, 647.03, 607.18, 915.78 for dataset/inria/Train/pos\\crop001036.png\n",
      "Class: 75.0, Confidence: 0.42, Bounding Box: 305.35, 623.95, 409.57, 919.79 for dataset/inria/Train/pos\\crop001036.png\n",
      "Class: 39.0, Confidence: 0.27, Bounding Box: 601.50, 577.52, 652.38, 688.94 for dataset/inria/Train/pos\\crop001036.png\n",
      "\n",
      "0: 640x480 1 person, 1 fire hydrant, 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 265.68, 193.51, 449.75, 777.71 for dataset/inria/Train/pos\\crop001037.png\n",
      "Class: 10.0, Confidence: 0.56, Bounding Box: 284.39, 644.15, 380.80, 826.82 for dataset/inria/Train/pos\\crop001037.png\n",
      "\n",
      "0: 512x640 9 persons, 1 car, 2 backpacks, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.97, Bounding Box: 846.92, 0.00, 1156.20, 898.83 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 263.80, 359.68, 388.29, 639.66 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 386.83, 421.40, 525.19, 645.67 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 676.82, 330.67, 807.23, 671.43 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 165.97, 329.15, 272.02, 651.93 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 91.53, 366.54, 138.02, 514.87 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 855.98, 458.03, 935.16, 666.34 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 131.34, 337.49, 200.73, 627.44 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 24.0, Confidence: 0.56, Bounding Box: 681.91, 383.51, 760.18, 487.27 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 24.0, Confidence: 0.50, Bounding Box: 182.86, 374.90, 254.76, 468.95 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 2.0, Confidence: 0.42, Bounding Box: 338.62, 387.10, 494.40, 480.26 for dataset/inria/Train/pos\\crop001038.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 174.08, 256.05, 298.24, 642.92 for dataset/inria/Train/pos\\crop001038.png\n",
      "\n",
      "0: 640x448 6 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 407.87, 324.45, 559.70, 790.34 for dataset/inria/Train/pos\\crop001039.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 261.26, 308.68, 408.94, 796.50 for dataset/inria/Train/pos\\crop001039.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 0.00, 54.18, 184.86, 953.85 for dataset/inria/Train/pos\\crop001039.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 58.06, 120.82, 238.85, 972.83 for dataset/inria/Train/pos\\crop001039.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 131.35, 177.27, 244.39, 527.32 for dataset/inria/Train/pos\\crop001039.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 81.02, 178.01, 242.69, 826.18 for dataset/inria/Train/pos\\crop001039.png\n",
      "\n",
      "0: 640x544 3 persons, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 206.73, 165.14, 335.60, 543.53 for dataset/inria/Train/pos\\crop001040.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 361.60, 0.16, 427.52, 106.89 for dataset/inria/Train/pos\\crop001040.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 316.09, 171.01, 438.45, 381.27 for dataset/inria/Train/pos\\crop001040.png\n",
      "\n",
      "0: 640x512 5 persons, 2 bottles, 1 chair, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 331.40, 215.54, 503.25, 875.87 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 480.09, 243.63, 683.76, 780.37 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 667.30, 160.61, 734.97, 285.93 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 39.0, Confidence: 0.62, Bounding Box: 649.30, 732.17, 683.12, 824.13 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 39.0, Confidence: 0.61, Bounding Box: 531.87, 784.10, 577.77, 892.25 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 220.71, 191.86, 307.86, 430.99 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 34.92, 199.47, 175.18, 409.58 for dataset/inria/Train/pos\\crop001042.png\n",
      "Class: 56.0, Confidence: 0.27, Bounding Box: 9.91, 402.59, 339.03, 882.13 for dataset/inria/Train/pos\\crop001042.png\n",
      "\n",
      "0: 640x384 3 persons, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 385.34, 267.16, 509.92, 456.02 for dataset/inria/Train/pos\\crop001043.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 116.78, 200.71, 312.93, 859.60 for dataset/inria/Train/pos\\crop001043.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 112.58, 238.65, 215.47, 451.08 for dataset/inria/Train/pos\\crop001043.png\n",
      "\n",
      "0: 640x608 2 persons, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 186.28, 138.80, 241.68, 324.65 for dataset/inria/Train/pos\\crop001044.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 79.26, 0.00, 110.46, 85.67 for dataset/inria/Train/pos\\crop001044.png\n",
      "\n",
      "0: 640x512 2 persons, 1 handbag, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 274.71, 358.42, 328.89, 497.82 for dataset/inria/Train/pos\\crop001045.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 164.52, 174.96, 203.08, 305.84 for dataset/inria/Train/pos\\crop001045.png\n",
      "Class: 26.0, Confidence: 0.36, Bounding Box: 176.24, 223.25, 196.75, 258.93 for dataset/inria/Train/pos\\crop001045.png\n",
      "\n",
      "0: 640x608 7 persons, 1 chair, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 162.91, 139.59, 254.09, 355.37 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 409.19, 13.02, 438.60, 100.00 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 367.52, 6.35, 400.34, 101.48 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 115.56, 110.95, 179.73, 265.93 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 98.75, 29.37, 115.83, 85.06 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 56.0, Confidence: 0.57, Bounding Box: 358.97, 94.74, 437.85, 181.28 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 325.66, 16.91, 374.07, 91.37 for dataset/inria/Train/pos\\crop001046.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 113.04, 21.37, 136.15, 84.57 for dataset/inria/Train/pos\\crop001046.png\n",
      "\n",
      "0: 640x576 10 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 214.21, 195.84, 282.48, 376.72 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 215.43, 436.24, 300.69, 546.80 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 17.14, 349.85, 97.47, 494.77 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 80.97, 296.28, 169.68, 485.68 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 384.94, 191.61, 451.96, 349.25 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 0.40, 409.02, 53.38, 545.96 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 368.25, 109.78, 384.07, 169.39 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 430.56, 220.69, 475.92, 419.18 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 381.07, 103.77, 403.57, 165.81 for dataset/inria/Train/pos\\crop001047.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 383.04, 100.79, 409.17, 166.88 for dataset/inria/Train/pos\\crop001047.png\n",
      "\n",
      "0: 608x640 7 persons, 1 motorcycle, 1 handbag, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 254.16, 0.00, 320.74, 144.67 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 471.40, 0.00, 563.93, 205.11 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 253.76, 201.84, 361.17, 519.53 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 55.50, 117.28, 135.78, 263.62 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 119.27, 64.05, 211.42, 257.43 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 423.43, 0.01, 489.58, 117.61 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 0.14, 177.04, 108.59, 442.82 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 26.0, Confidence: 0.59, Bounding Box: 511.22, 13.13, 554.53, 106.16 for dataset/inria/Train/pos\\crop001048.png\n",
      "Class: 3.0, Confidence: 0.28, Bounding Box: 0.06, 277.28, 107.74, 444.82 for dataset/inria/Train/pos\\crop001048.png\n",
      "\n",
      "0: 544x640 7 persons, 1 handbag, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 220.14, 130.57, 257.91, 218.45 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 178.60, 125.80, 210.98, 220.56 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 0.01, 257.68, 65.30, 349.65 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 136.42, 134.80, 175.80, 206.92 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 238.51, 63.77, 261.76, 113.19 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 287.08, 69.19, 322.48, 114.32 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 13.87, 293.49, 63.63, 351.43 for dataset/inria/Train/pos\\crop001049.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 301.31, 68.67, 323.20, 114.49 for dataset/inria/Train/pos\\crop001049.png\n",
      "\n",
      "0: 640x576 13 persons, 1 backpack, 5 handbags, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 423.17, 272.27, 569.39, 767.06 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.00, 152.42, 218.01, 976.00 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 215.18, 295.82, 354.21, 730.05 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 733.21, 304.01, 814.73, 417.45 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 153.26, 284.21, 207.12, 442.46 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 379.15, 321.15, 435.56, 484.70 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 73.45, 290.68, 143.97, 391.39 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 630.54, 318.82, 694.46, 448.63 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 26.0, Confidence: 0.66, Bounding Box: 445.08, 345.09, 532.06, 527.08 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 685.87, 336.52, 739.84, 409.89 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 194.71, 283.47, 247.93, 447.80 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 318.95, 290.02, 384.24, 453.10 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 443.77, 406.13, 532.65, 520.55 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 133.31, 298.55, 161.95, 401.28 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 411.67, 499.10, 467.65, 619.20 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 574.22, 324.99, 624.20, 376.71 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 316.01, 363.39, 369.08, 537.94 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 24.0, Confidence: 0.27, Bounding Box: 312.97, 362.39, 369.09, 536.58 for dataset/inria/Train/pos\\crop001050.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 446.14, 444.51, 532.29, 520.46 for dataset/inria/Train/pos\\crop001050.png\n",
      "\n",
      "0: 640x480 14 persons, 3 handbags, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 0.00, 198.75, 154.75, 975.06 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 227.44, 263.02, 360.87, 636.66 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 443.02, 133.26, 725.00, 973.60 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 564.27, 286.40, 618.31, 365.29 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 368.43, 264.82, 571.33, 934.06 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 354.72, 337.72, 430.42, 649.20 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 150.35, 267.91, 251.84, 492.80 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 355.48, 349.25, 427.58, 570.41 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 26.0, Confidence: 0.49, Bounding Box: 315.99, 312.66, 353.67, 419.03 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 409.43, 292.60, 474.32, 389.20 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 154.45, 457.43, 229.39, 633.63 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 58.48, 256.38, 155.54, 400.27 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 162.14, 240.24, 209.22, 442.57 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 176.13, 484.17, 231.96, 593.65 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 320.87, 333.29, 354.69, 421.00 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 57.92, 256.19, 163.48, 455.23 for dataset/inria/Train/pos\\crop001051.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 184.80, 268.73, 240.09, 395.42 for dataset/inria/Train/pos\\crop001051.png\n",
      "\n",
      "0: 640x544 7 persons, 1 backpack, 2 handbags, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 164.14, 170.47, 245.37, 360.72 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 155.81, 0.24, 272.29, 209.50 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 234.84, 0.36, 383.07, 377.83 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 0.00, 7.27, 168.25, 515.28 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 26.0, Confidence: 0.72, Bounding Box: 336.78, 18.44, 380.41, 134.25 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 374.81, 49.52, 419.00, 407.45 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 24.0, Confidence: 0.40, Bounding Box: 60.56, 0.00, 178.14, 72.88 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 340.78, 59.51, 379.87, 132.10 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 372.96, 106.86, 418.81, 288.28 for dataset/inria/Train/pos\\crop001052.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 370.65, 47.91, 419.00, 293.52 for dataset/inria/Train/pos\\crop001052.png\n",
      "\n",
      "0: 640x416 3 persons, 1 bicycle, 1 truck, 2 traffic lights, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 204.67, 246.84, 451.57, 933.50 for dataset/inria/Train/pos\\crop001053.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 97.16, 296.24, 186.50, 456.50 for dataset/inria/Train/pos\\crop001053.png\n",
      "Class: 9.0, Confidence: 0.74, Bounding Box: 237.27, 167.24, 263.34, 243.75 for dataset/inria/Train/pos\\crop001053.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 65.61, 314.85, 89.56, 378.71 for dataset/inria/Train/pos\\crop001053.png\n",
      "Class: 9.0, Confidence: 0.65, Bounding Box: 185.72, 160.95, 237.42, 242.92 for dataset/inria/Train/pos\\crop001053.png\n",
      "Class: 1.0, Confidence: 0.48, Bounding Box: 163.91, 358.65, 225.39, 405.57 for dataset/inria/Train/pos\\crop001053.png\n",
      "Class: 7.0, Confidence: 0.40, Bounding Box: 294.69, 133.21, 597.17, 467.84 for dataset/inria/Train/pos\\crop001053.png\n",
      "\n",
      "0: 640x608 6 persons, 2 bicycles, 1 car, 2 traffic lights, 1 tie, 11.0ms\n",
      "Speed: 4.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 271.87, 196.14, 411.41, 465.06 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 450.31, 111.99, 626.00, 672.00 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 217.38, 221.61, 253.58, 320.91 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 0.00, 234.30, 34.48, 281.74 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 9.0, Confidence: 0.75, Bounding Box: 499.44, 0.26, 542.88, 103.46 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 343.63, 189.84, 400.08, 375.95 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 174.80, 226.36, 198.35, 293.43 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 1.0, Confidence: 0.50, Bounding Box: 417.46, 282.28, 485.81, 374.58 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 9.0, Confidence: 0.45, Bounding Box: 416.78, 0.41, 488.48, 102.53 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 1.0, Confidence: 0.42, Bounding Box: 376.90, 282.84, 485.81, 374.44 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 27.0, Confidence: 0.37, Bounding Box: 578.29, 295.92, 617.30, 335.20 for dataset/inria/Train/pos\\crop001054.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 132.92, 232.36, 157.22, 279.16 for dataset/inria/Train/pos\\crop001054.png\n",
      "\n",
      "0: 640x384 4 persons, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 354.05, 244.66, 404.46, 374.71 for dataset/inria/Train/pos\\crop001055.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 62.28, 262.73, 147.11, 522.30 for dataset/inria/Train/pos\\crop001055.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 108.05, 231.23, 217.32, 539.84 for dataset/inria/Train/pos\\crop001055.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 94.81, 231.26, 213.61, 431.06 for dataset/inria/Train/pos\\crop001055.png\n",
      "\n",
      "0: 544x640 5 persons, 1 car, 1 truck, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 174.89, 175.12, 225.84, 305.57 for dataset/inria/Train/pos\\crop001056.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 258.33, 180.18, 301.43, 309.20 for dataset/inria/Train/pos\\crop001056.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 374.61, 175.10, 409.40, 280.02 for dataset/inria/Train/pos\\crop001056.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 0.00, 189.42, 33.56, 359.44 for dataset/inria/Train/pos\\crop001056.png\n",
      "Class: 2.0, Confidence: 0.70, Bounding Box: 460.04, 148.22, 513.84, 284.81 for dataset/inria/Train/pos\\crop001056.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 401.37, 186.11, 430.97, 273.38 for dataset/inria/Train/pos\\crop001056.png\n",
      "Class: 7.0, Confidence: 0.32, Bounding Box: 460.14, 148.29, 513.83, 283.40 for dataset/inria/Train/pos\\crop001056.png\n",
      "\n",
      "0: 640x576 2 persons, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 240.96, 194.00, 323.82, 440.25 for dataset/inria/Train/pos\\crop001057.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 0.00, 237.07, 41.85, 303.03 for dataset/inria/Train/pos\\crop001057.png\n",
      "\n",
      "0: 640x544 5 persons, 14.5ms\n",
      "Speed: 2.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 277.18, 243.61, 516.78, 898.79 for dataset/inria/Train/pos\\crop001058.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 750.97, 172.59, 823.00, 398.27 for dataset/inria/Train/pos\\crop001058.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 509.28, 211.46, 565.63, 274.31 for dataset/inria/Train/pos\\crop001058.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 727.68, 189.07, 771.20, 388.53 for dataset/inria/Train/pos\\crop001058.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 415.94, 177.16, 465.72, 267.11 for dataset/inria/Train/pos\\crop001058.png\n",
      "\n",
      "0: 640x448 3 persons, 1 traffic light, 1 handbag, 13.2ms\n",
      "Speed: 2.5ms preprocess, 13.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 171.77, 235.46, 396.65, 820.28 for dataset/inria/Train/pos\\crop001059.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 42.63, 195.03, 193.59, 753.49 for dataset/inria/Train/pos\\crop001059.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 166.79, 237.33, 271.55, 776.38 for dataset/inria/Train/pos\\crop001059.png\n",
      "Class: 9.0, Confidence: 0.39, Bounding Box: 470.51, 150.26, 522.54, 247.34 for dataset/inria/Train/pos\\crop001059.png\n",
      "Class: 26.0, Confidence: 0.39, Bounding Box: 156.90, 332.25, 249.69, 541.70 for dataset/inria/Train/pos\\crop001059.png\n",
      "\n",
      "0: 640x448 2 persons, 12.2ms\n",
      "Speed: 1.7ms preprocess, 12.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 169.55, 272.80, 431.88, 972.89 for dataset/inria/Train/pos\\crop001060.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 551.45, 408.48, 661.33, 931.48 for dataset/inria/Train/pos\\crop001060.png\n",
      "\n",
      "0: 640x480 3 persons, 3 boats, 26.0ms\n",
      "Speed: 3.6ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 290.57, 276.83, 442.00, 678.99 for dataset/inria/Train/pos\\crop001061.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 512.12, 361.95, 553.73, 491.85 for dataset/inria/Train/pos\\crop001061.png\n",
      "Class: 8.0, Confidence: 0.44, Bounding Box: 1.01, 458.53, 36.72, 553.34 for dataset/inria/Train/pos\\crop001061.png\n",
      "Class: 8.0, Confidence: 0.39, Bounding Box: 0.36, 592.84, 446.31, 896.30 for dataset/inria/Train/pos\\crop001061.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 1.49, 633.03, 112.52, 766.45 for dataset/inria/Train/pos\\crop001061.png\n",
      "Class: 8.0, Confidence: 0.27, Bounding Box: 0.00, 472.46, 446.25, 886.62 for dataset/inria/Train/pos\\crop001061.png\n",
      "\n",
      "0: 640x288 6 persons, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 205.81, 240.87, 337.26, 801.09 for dataset/inria/Train/pos\\crop001063.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 309.73, 311.53, 376.17, 549.87 for dataset/inria/Train/pos\\crop001063.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 0.26, 244.05, 33.04, 506.55 for dataset/inria/Train/pos\\crop001063.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 108.86, 309.54, 208.52, 540.43 for dataset/inria/Train/pos\\crop001063.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 170.01, 312.34, 221.70, 533.48 for dataset/inria/Train/pos\\crop001063.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 84.11, 298.58, 157.56, 539.26 for dataset/inria/Train/pos\\crop001063.png\n",
      "\n",
      "0: 448x640 9 persons, 1 bird, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 578.17, 127.60, 716.72, 512.60 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 435.57, 139.66, 571.31, 489.92 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 143.25, 142.76, 280.30, 473.87 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 294.98, 190.63, 409.34, 500.82 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 14.0, Confidence: 0.79, Bounding Box: 285.27, 505.35, 344.28, 549.67 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 807.35, 199.43, 868.79, 429.28 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 44.09, 167.97, 162.59, 334.52 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 774.09, 186.27, 837.34, 427.40 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 345.56, 153.87, 411.13, 366.85 for dataset/inria/Train/pos\\crop001064.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 346.74, 152.97, 407.00, 247.74 for dataset/inria/Train/pos\\crop001064.png\n",
      "\n",
      "0: 640x512 2 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 233.82, 178.11, 420.78, 643.48 for dataset/inria/Train/pos\\crop001066.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 474.94, 263.14, 633.00, 794.00 for dataset/inria/Train/pos\\crop001066.png\n",
      "\n",
      "0: 640x480 9 persons, 2 boats, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 199.28, 177.47, 406.24, 590.97 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 41.20, 110.06, 94.20, 255.92 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 111.01, 653.00, 260.48, 773.63 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 299.25, 645.86, 427.98, 773.77 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 8.0, Confidence: 0.59, Bounding Box: 0.00, 272.82, 106.07, 387.08 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 41.42, 243.76, 88.01, 321.83 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 0.00, 91.73, 31.97, 208.36 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 0.00, 253.49, 65.50, 329.74 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 8.0, Confidence: 0.36, Bounding Box: 18.99, 0.00, 557.00, 745.28 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 44.32, 586.30, 465.94, 773.25 for dataset/inria/Train/pos\\crop001068.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 190.42, 583.93, 465.65, 771.61 for dataset/inria/Train/pos\\crop001068.png\n",
      "\n",
      "0: 640x544 6 persons, 1 boat, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 159.78, 144.88, 215.55, 295.40 for dataset/inria/Train/pos\\crop001069.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 101.33, 288.74, 179.22, 361.38 for dataset/inria/Train/pos\\crop001069.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 98.45, 130.15, 152.15, 245.84 for dataset/inria/Train/pos\\crop001069.png\n",
      "Class: 8.0, Confidence: 0.43, Bounding Box: 66.99, 250.74, 226.89, 425.83 for dataset/inria/Train/pos\\crop001069.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 161.41, 278.65, 205.04, 359.10 for dataset/inria/Train/pos\\crop001069.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 161.51, 280.01, 219.06, 366.32 for dataset/inria/Train/pos\\crop001069.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 34.39, 63.80, 51.48, 110.37 for dataset/inria/Train/pos\\crop001069.png\n",
      "\n",
      "0: 640x416 8 persons, 2 cars, 1 handbag, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 225.56, 173.97, 437.92, 795.16 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 418.80, 218.02, 507.85, 498.89 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 492.29, 255.43, 573.31, 498.34 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 587.42, 236.74, 634.00, 487.43 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 550.40, 244.85, 613.95, 492.17 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 140.47, 240.39, 229.95, 437.45 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 381.86, 238.77, 431.03, 462.70 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 2.0, Confidence: 0.42, Bounding Box: 11.64, 233.30, 114.61, 302.82 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 6.69, 230.89, 115.07, 326.39 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 391.00, 239.61, 428.69, 338.46 for dataset/inria/Train/pos\\crop001070.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 551.00, 297.99, 576.76, 363.98 for dataset/inria/Train/pos\\crop001070.png\n",
      "\n",
      "0: 640x480 4 persons, 1 chair, 13.2ms\n",
      "Speed: 2.0ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 362.49, 112.09, 556.00, 793.00 for dataset/inria/Train/pos\\crop001071.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 162.90, 243.74, 364.31, 651.41 for dataset/inria/Train/pos\\crop001071.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 354.65, 243.34, 495.64, 363.53 for dataset/inria/Train/pos\\crop001071.png\n",
      "Class: 56.0, Confidence: 0.35, Bounding Box: 8.50, 258.45, 112.46, 297.86 for dataset/inria/Train/pos\\crop001071.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 446.87, 243.09, 497.91, 308.88 for dataset/inria/Train/pos\\crop001071.png\n",
      "\n",
      "0: 608x640 15 persons, 1 handbag, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.00, 151.22, 270.54, 973.02 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 249.88, 258.65, 427.18, 821.99 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 816.58, 275.54, 955.19, 771.40 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 400.40, 329.93, 563.04, 823.63 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 762.37, 283.77, 850.64, 672.43 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 994.16, 318.63, 1075.43, 672.02 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 678.94, 280.12, 801.62, 775.30 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 190.37, 296.79, 275.86, 772.82 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 593.06, 282.25, 712.68, 788.98 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 903.19, 309.24, 1025.39, 710.16 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 506.15, 298.07, 671.33, 814.25 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 26.0, Confidence: 0.48, Bounding Box: 517.91, 401.14, 569.65, 550.88 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 125.38, 336.05, 166.04, 364.97 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 163.71, 326.72, 210.66, 401.09 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 803.59, 278.68, 931.72, 708.99 for dataset/inria/Train/pos\\crop001072.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 749.99, 281.77, 821.73, 727.53 for dataset/inria/Train/pos\\crop001072.png\n",
      "\n",
      "0: 640x416 9 persons, 1 bird, 1 umbrella, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 82.39, 214.00, 274.19, 734.67 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 14.0, Confidence: 0.84, Bounding Box: 237.54, 565.76, 299.58, 614.34 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 339.06, 186.68, 429.52, 420.48 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 0.00, 195.94, 119.96, 732.33 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 414.36, 252.52, 466.99, 465.40 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 300.81, 184.72, 367.19, 409.29 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 76.42, 180.80, 136.81, 321.31 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 231.48, 182.22, 291.80, 337.44 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 25.0, Confidence: 0.45, Bounding Box: 296.55, 158.20, 385.23, 207.58 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 284.63, 184.81, 326.80, 323.93 for dataset/inria/Train/pos\\crop001073.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 45.67, 196.29, 81.54, 312.91 for dataset/inria/Train/pos\\crop001073.png\n",
      "\n",
      "0: 608x640 14 persons, 2 umbrellas, 1 bottle, 11.0ms\n",
      "Speed: 4.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.03, 228.47, 140.89, 601.96 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 205.33, 204.36, 291.71, 433.12 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 164.73, 201.27, 232.29, 423.96 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 346.10, 313.64, 391.88, 467.60 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 573.11, 260.50, 632.42, 450.11 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 433.13, 223.63, 518.96, 465.17 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 396.42, 201.10, 458.62, 456.40 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 280.41, 271.51, 336.91, 479.00 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 617.97, 193.15, 654.69, 586.51 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 617.70, 196.28, 654.82, 459.18 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 97.59, 198.51, 160.05, 352.42 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 150.31, 202.36, 192.00, 340.16 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 518.08, 192.74, 569.40, 360.83 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 290.77, 218.59, 373.34, 424.03 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 25.0, Confidence: 0.42, Bounding Box: 163.08, 172.89, 250.43, 213.03 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 39.0, Confidence: 0.34, Bounding Box: 561.89, 80.15, 598.74, 183.84 for dataset/inria/Train/pos\\crop001074.png\n",
      "Class: 25.0, Confidence: 0.27, Bounding Box: 162.93, 174.10, 249.17, 201.59 for dataset/inria/Train/pos\\crop001074.png\n",
      "\n",
      "0: 640x416 7 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 365.87, 211.11, 469.02, 458.85 for dataset/inria/Train/pos\\crop001075.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 213.29, 210.54, 319.95, 605.89 for dataset/inria/Train/pos\\crop001075.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 22.67, 262.91, 112.00, 489.27 for dataset/inria/Train/pos\\crop001075.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 170.21, 286.10, 230.95, 478.03 for dataset/inria/Train/pos\\crop001075.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 1.01, 230.79, 52.43, 474.82 for dataset/inria/Train/pos\\crop001075.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 337.91, 224.21, 388.43, 410.54 for dataset/inria/Train/pos\\crop001075.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 296.31, 240.52, 347.24, 410.53 for dataset/inria/Train/pos\\crop001075.png\n",
      "\n",
      "0: 640x640 11 persons, 2 birds, 1 skateboard, 13.0ms\n",
      "Speed: 3.1ms preprocess, 13.0ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 171.40, 119.08, 273.78, 362.22 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 271.12, 136.63, 350.52, 360.53 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 18.06, 114.31, 117.72, 501.12 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 0.09, 186.98, 34.93, 374.41 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 143.32, 127.49, 195.79, 316.86 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 14.0, Confidence: 0.66, Bounding Box: 333.54, 423.58, 389.02, 463.89 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 14.0, Confidence: 0.65, Bounding Box: 413.07, 354.09, 432.31, 383.64 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 370.81, 122.28, 407.84, 223.61 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 431.54, 119.19, 480.00, 493.96 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 104.48, 137.70, 159.02, 316.79 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 424.03, 121.85, 448.46, 196.04 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 95.77, 121.34, 122.43, 165.01 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 182.64, 116.31, 207.83, 186.75 for dataset/inria/Train/pos\\crop001078.png\n",
      "Class: 36.0, Confidence: 0.27, Bounding Box: 101.24, 324.70, 132.75, 353.04 for dataset/inria/Train/pos\\crop001078.png\n",
      "\n",
      "0: 640x448 10 persons, 2 birds, 2 handbags, 16.6ms\n",
      "Speed: 1.6ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 14.0, Confidence: 0.84, Bounding Box: 68.76, 510.99, 124.95, 554.79 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 3.05, 224.81, 84.85, 450.67 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 164.55, 179.82, 316.04, 607.03 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 362.82, 348.95, 435.03, 568.87 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 354.44, 203.54, 412.08, 362.62 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 14.0, Confidence: 0.57, Bounding Box: 147.88, 443.89, 169.43, 475.95 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 26.0, Confidence: 0.47, Bounding Box: 417.02, 427.10, 444.13, 512.07 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 436.97, 210.65, 467.03, 312.02 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 289.14, 217.81, 333.70, 363.61 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 316.12, 207.94, 350.80, 299.41 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 105.68, 208.72, 145.65, 327.98 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 404.63, 214.96, 428.80, 301.65 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 415.58, 398.68, 444.95, 510.82 for dataset/inria/Train/pos\\crop001079.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 314.59, 205.91, 350.09, 345.64 for dataset/inria/Train/pos\\crop001079.png\n",
      "\n",
      "0: 640x480 11 persons, 1 bird, 2 baseball gloves, 36.8ms\n",
      "Speed: 2.0ms preprocess, 36.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 158.66, 139.72, 225.69, 355.27 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 1.00, 199.63, 67.72, 421.37 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 239.13, 117.24, 277.14, 196.62 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 0.42, 55.37, 37.93, 206.30 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 14.0, Confidence: 0.77, Bounding Box: 204.00, 357.80, 262.88, 402.02 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 315.77, 111.70, 359.56, 362.19 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 202.67, 55.41, 237.53, 153.20 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 46.46, 70.23, 70.09, 157.78 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 148.65, 55.00, 175.09, 160.98 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 64.64, 62.39, 92.70, 160.39 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 35.0, Confidence: 0.35, Bounding Box: 280.20, 166.15, 313.96, 184.82 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 291.71, 67.44, 320.10, 137.63 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 35.0, Confidence: 0.30, Bounding Box: 234.78, 222.74, 253.76, 240.10 for dataset/inria/Train/pos\\crop001081.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 323.10, 50.59, 359.16, 359.24 for dataset/inria/Train/pos\\crop001081.png\n",
      "\n",
      "0: 640x480 8 persons, 8 birds, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 258.65, 222.16, 384.17, 669.94 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 141.57, 238.03, 229.33, 408.12 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 62.57, 239.69, 117.46, 433.67 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.73, Bounding Box: 251.91, 699.32, 290.40, 763.95 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.73, Bounding Box: 464.76, 574.67, 504.76, 617.47 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 220.07, 228.31, 306.57, 464.78 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.62, Bounding Box: 528.41, 594.98, 553.72, 630.16 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 132.65, 239.80, 177.13, 382.17 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.51, Bounding Box: 482.02, 495.96, 512.32, 536.46 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 0.00, 223.76, 36.15, 460.81 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 14.78, 227.18, 60.98, 412.64 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.35, Bounding Box: 74.78, 464.40, 123.29, 501.29 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 104.40, 232.22, 147.79, 377.41 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.31, Bounding Box: 534.93, 652.45, 593.26, 698.99 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.31, Bounding Box: 364.10, 549.55, 405.65, 588.34 for dataset/inria/Train/pos\\crop001083.png\n",
      "Class: 14.0, Confidence: 0.26, Bounding Box: 466.87, 406.55, 488.86, 427.66 for dataset/inria/Train/pos\\crop001083.png\n",
      "\n",
      "0: 640x512 15 persons, 1 bird, 1 backpack, 1 clock, 22.0ms\n",
      "Speed: 3.5ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 0.10, 286.63, 166.25, 974.74 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 196.99, 298.69, 389.99, 976.00 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 337.94, 241.77, 520.52, 960.76 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 14.0, Confidence: 0.65, Bounding Box: 716.75, 618.57, 744.55, 668.40 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 570.98, 262.01, 611.60, 412.22 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 439.95, 233.47, 519.16, 417.06 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 162.41, 200.44, 292.04, 605.71 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 631.04, 260.51, 696.36, 395.39 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 74.0, Confidence: 0.52, Bounding Box: 242.76, 0.00, 349.30, 68.51 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 76.07, 225.36, 207.67, 645.48 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 24.0, Confidence: 0.42, Bounding Box: 121.78, 304.70, 213.33, 466.94 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 76.89, 225.07, 171.56, 427.22 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 38.33, 249.75, 118.67, 359.77 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 0.82, 201.58, 48.35, 342.90 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 502.40, 237.95, 559.08, 432.10 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 623.79, 279.86, 655.85, 369.24 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 163.89, 194.36, 226.53, 286.48 for dataset/inria/Train/pos\\crop001084.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 693.37, 251.66, 748.21, 439.23 for dataset/inria/Train/pos\\crop001084.png\n",
      "\n",
      "0: 640x288 5 persons, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 0.00, 225.42, 208.54, 792.90 for dataset/inria/Train/pos\\crop001085.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 223.67, 132.67, 408.61, 975.27 for dataset/inria/Train/pos\\crop001085.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 183.07, 210.82, 261.78, 510.99 for dataset/inria/Train/pos\\crop001085.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 9.32, 188.08, 103.71, 325.66 for dataset/inria/Train/pos\\crop001085.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 130.55, 211.59, 175.08, 309.89 for dataset/inria/Train/pos\\crop001085.png\n",
      "\n",
      "0: 640x416 11 persons, 2 backpacks, 1 handbag, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 216.62, 263.53, 400.03, 789.83 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 375.55, 239.00, 523.55, 831.55 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 123.21, 275.30, 221.33, 649.54 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 410.15, 338.92, 599.91, 973.37 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 24.0, Confidence: 0.50, Bounding Box: 456.76, 353.91, 599.20, 703.18 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 202.33, 264.05, 283.38, 663.72 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 6.16, 254.91, 167.19, 768.90 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 383.19, 254.21, 444.24, 367.61 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 0.00, 280.75, 57.77, 804.49 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 347.80, 248.74, 405.50, 383.08 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 79.60, 358.14, 153.09, 484.35 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 24.0, Confidence: 0.28, Bounding Box: 446.07, 350.75, 599.32, 595.66 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 41.88, 216.27, 129.84, 714.98 for dataset/inria/Train/pos\\crop001086.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 207.14, 263.27, 283.36, 404.75 for dataset/inria/Train/pos\\crop001086.png\n",
      "\n",
      "0: 640x256 5 persons, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 168.37, 299.38, 343.50, 766.88 for dataset/inria/Train/pos\\crop001087.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 1.66, 342.27, 162.02, 747.51 for dataset/inria/Train/pos\\crop001087.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 108.63, 250.45, 225.18, 577.05 for dataset/inria/Train/pos\\crop001087.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 0.15, 256.52, 56.60, 489.67 for dataset/inria/Train/pos\\crop001087.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 26.80, 245.31, 77.37, 365.42 for dataset/inria/Train/pos\\crop001087.png\n",
      "\n",
      "0: 640x352 7 persons, 3 birds, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 70.90, 247.25, 288.22, 754.81 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 14.0, Confidence: 0.83, Bounding Box: 255.31, 792.25, 300.86, 863.98 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 436.45, 304.48, 494.77, 503.74 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 227.12, 175.39, 422.97, 925.04 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 36.90, 283.16, 135.00, 590.37 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 14.0, Confidence: 0.64, Bounding Box: 0.00, 770.78, 33.53, 867.33 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 27.23, 290.34, 75.20, 374.51 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 14.0, Confidence: 0.28, Bounding Box: 443.27, 519.21, 482.25, 563.72 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 23.30, 289.97, 76.19, 431.22 for dataset/inria/Train/pos\\crop001088.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 22.69, 289.83, 75.72, 542.48 for dataset/inria/Train/pos\\crop001088.png\n",
      "\n",
      "0: 640x512 8 persons, 3 birds, 1 dog, 1 handbag, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 58.43, 230.38, 194.91, 628.37 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 14.0, Confidence: 0.85, Bounding Box: 0.00, 840.80, 99.15, 917.08 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 252.97, 198.73, 520.63, 823.63 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 210.25, 250.78, 329.71, 625.72 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 437.60, 118.13, 679.05, 973.41 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 14.0, Confidence: 0.70, Bounding Box: 145.04, 846.22, 204.91, 962.33 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 696.03, 276.02, 733.71, 520.80 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 0.44, 193.99, 120.82, 688.69 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 16.0, Confidence: 0.62, Bounding Box: 474.42, 870.43, 531.04, 958.27 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 186.23, 256.68, 260.17, 557.02 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 14.0, Confidence: 0.45, Bounding Box: 145.80, 878.52, 201.86, 962.59 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 442.94, 565.49, 541.50, 701.98 for dataset/inria/Train/pos\\crop001089.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 492.79, 381.50, 533.99, 522.83 for dataset/inria/Train/pos\\crop001089.png\n",
      "\n",
      "0: 640x640 10 persons, 3 handbags, 19.0ms\n",
      "Speed: 4.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 579.07, 204.93, 929.05, 976.00 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 332.78, 259.85, 508.02, 774.35 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 517.06, 283.91, 682.28, 775.46 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 119.60, 210.77, 408.75, 972.03 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 0.31, 138.49, 153.11, 973.51 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 26.0, Confidence: 0.65, Bounding Box: 146.26, 471.96, 240.88, 690.36 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 817.40, 126.29, 979.88, 965.72 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 500.44, 292.26, 584.54, 697.38 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 929.23, 160.53, 979.76, 974.98 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 504.77, 294.46, 586.54, 524.38 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 26.0, Confidence: 0.36, Bounding Box: 836.28, 694.63, 958.85, 867.49 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 0.00, 322.71, 80.60, 445.86 for dataset/inria/Train/pos\\crop001090.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 892.81, 480.83, 942.72, 640.81 for dataset/inria/Train/pos\\crop001090.png\n",
      "\n",
      "0: 640x448 8 persons, 4 birds, 1 handbag, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 440.17, 225.54, 573.12, 614.78 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 283.31, 191.04, 510.00, 800.10 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 4.54, 145.73, 314.47, 976.00 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 26.0, Confidence: 0.80, Bounding Box: 301.83, 383.68, 373.49, 549.32 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 14.0, Confidence: 0.77, Bounding Box: 362.03, 816.37, 481.36, 888.63 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 0.00, 198.12, 73.86, 740.61 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 14.0, Confidence: 0.59, Bounding Box: 512.20, 821.16, 581.96, 933.89 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 14.0, Confidence: 0.58, Bounding Box: 304.22, 931.84, 360.08, 975.58 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 584.67, 248.01, 668.00, 613.08 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 51.94, 239.99, 133.05, 385.15 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 564.47, 252.15, 631.51, 545.11 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 14.0, Confidence: 0.35, Bounding Box: 253.12, 806.64, 288.97, 874.44 for dataset/inria/Train/pos\\crop001091.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 120.38, 217.81, 156.34, 273.61 for dataset/inria/Train/pos\\crop001091.png\n",
      "\n",
      "0: 640x480 11 persons, 5 birds, 2 handbags, 18.0ms\n",
      "Speed: 6.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 584.46, 245.69, 689.99, 585.55 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 453.47, 223.08, 643.94, 743.93 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 155.13, 176.20, 468.01, 907.31 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 76.00, 202.32, 258.80, 781.75 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 14.0, Confidence: 0.72, Bounding Box: 515.61, 760.45, 616.88, 827.75 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 0.00, 295.43, 84.56, 867.31 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 14.0, Confidence: 0.61, Bounding Box: 465.42, 861.72, 512.65, 915.13 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 21.25, 184.62, 101.93, 338.86 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 32.54, 227.50, 148.49, 716.80 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 26.0, Confidence: 0.56, Bounding Box: 457.11, 344.93, 531.10, 529.02 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 133.85, 204.14, 270.94, 703.03 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 14.0, Confidence: 0.51, Bounding Box: 419.54, 756.95, 451.94, 815.85 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 14.0, Confidence: 0.41, Bounding Box: 657.00, 783.34, 689.17, 865.73 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 14.0, Confidence: 0.39, Bounding Box: 210.10, 905.65, 332.60, 975.69 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 26.0, Confidence: 0.36, Bounding Box: 463.32, 380.29, 525.88, 529.24 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 0.48, 310.08, 70.36, 772.38 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 33.94, 290.09, 116.75, 707.45 for dataset/inria/Train/pos\\crop001092.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 247.46, 258.98, 313.92, 373.42 for dataset/inria/Train/pos\\crop001092.png\n",
      "\n",
      "0: 640x608 13 persons, 3 handbags, 20.0ms\n",
      "Speed: 4.0ms preprocess, 20.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 554.01, 152.40, 902.40, 942.42 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 1.56, 189.14, 128.96, 730.47 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 161.09, 226.92, 332.52, 795.63 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 299.85, 293.06, 470.30, 874.93 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 479.19, 189.01, 663.94, 811.48 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 431.95, 204.82, 564.23, 754.43 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 416.97, 166.49, 504.65, 339.56 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 540.80, 187.10, 677.98, 723.38 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 661.08, 245.16, 734.94, 383.70 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 138.30, 254.02, 234.43, 725.04 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 144.04, 256.79, 235.69, 603.06 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 10.93, 388.41, 105.30, 526.82 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 616.22, 228.52, 672.07, 390.40 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 26.0, Confidence: 0.36, Bounding Box: 8.52, 277.73, 115.53, 525.84 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 299.40, 245.33, 365.20, 401.04 for dataset/inria/Train/pos\\crop001093.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 720.34, 273.98, 849.90, 439.89 for dataset/inria/Train/pos\\crop001093.png\n",
      "\n",
      "0: 640x576 10 persons, 4 birds, 1 handbag, 19.0ms\n",
      "Speed: 4.5ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 401.59, 254.03, 550.31, 677.05 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 251.36, 251.95, 377.28, 675.42 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 518.25, 256.93, 616.79, 496.82 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 675.80, 280.92, 758.33, 494.02 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 14.0, Confidence: 0.82, Bounding Box: 523.37, 769.75, 630.71, 839.59 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 14.0, Confidence: 0.81, Bounding Box: 131.15, 762.38, 212.82, 827.91 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 593.88, 265.86, 665.67, 491.90 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 58.76, 244.05, 131.73, 488.62 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 136.26, 273.62, 205.88, 473.78 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 213.36, 277.44, 270.16, 460.73 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 14.0, Confidence: 0.61, Bounding Box: 428.27, 833.99, 543.39, 878.62 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 668.40, 288.60, 703.19, 388.11 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 2.23, 266.30, 40.99, 474.84 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 135.62, 304.24, 168.06, 374.77 for dataset/inria/Train/pos\\crop001094.png\n",
      "Class: 14.0, Confidence: 0.25, Bounding Box: 0.53, 552.93, 39.09, 648.22 for dataset/inria/Train/pos\\crop001094.png\n",
      "\n",
      "0: 640x512 13 persons, 6 birds, 1 handbag, 2 chairs, 46.5ms\n",
      "Speed: 6.0ms preprocess, 46.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 367.17, 723.16, 627.50, 975.55 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 384.32, 222.36, 559.89, 728.46 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 637.43, 212.40, 727.93, 461.72 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 14.0, Confidence: 0.82, Bounding Box: 682.49, 655.16, 762.53, 710.93 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 189.36, 142.37, 459.08, 856.63 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 0.45, 28.84, 187.83, 951.16 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 132.63, 244.82, 208.32, 514.23 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 318.93, 223.59, 393.44, 503.00 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 78.90, 354.72, 149.43, 541.89 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 14.0, Confidence: 0.54, Bounding Box: 646.33, 843.16, 715.40, 906.17 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 715.75, 245.63, 759.37, 329.88 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 371.14, 236.44, 416.11, 352.66 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 56.0, Confidence: 0.46, Bounding Box: 566.02, 343.65, 657.72, 447.95 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 14.0, Confidence: 0.45, Bounding Box: 252.97, 832.44, 311.64, 913.57 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 575.84, 252.18, 636.48, 333.41 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 56.0, Confidence: 0.38, Bounding Box: 694.76, 337.06, 767.63, 426.79 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 14.0, Confidence: 0.34, Bounding Box: 704.12, 473.80, 769.67, 573.83 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 512.73, 244.91, 546.83, 305.49 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 2.79, 533.62, 113.05, 665.54 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 1.39, 534.22, 114.81, 940.46 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 14.0, Confidence: 0.29, Bounding Box: 609.56, 887.82, 683.72, 975.27 for dataset/inria/Train/pos\\crop001095.png\n",
      "Class: 14.0, Confidence: 0.27, Bounding Box: 300.11, 830.13, 384.69, 890.09 for dataset/inria/Train/pos\\crop001095.png\n",
      "\n",
      "0: 640x448 8 persons, 1 bird, 4 chairs, 1 dining table, 37.5ms\n",
      "Speed: 7.0ms preprocess, 37.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 393.49, 247.31, 532.43, 787.00 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 0.05, 335.29, 67.65, 786.56 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 96.91, 272.45, 182.64, 391.75 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 187.92, 212.17, 328.29, 593.51 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 56.0, Confidence: 0.71, Bounding Box: 81.06, 413.92, 201.66, 568.12 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 290.50, 610.23, 461.72, 757.66 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 56.0, Confidence: 0.66, Bounding Box: 172.28, 424.68, 238.73, 554.87 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 311.37, 260.43, 371.83, 383.07 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 1.57, 260.54, 48.43, 350.33 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 56.0, Confidence: 0.53, Bounding Box: 280.54, 406.58, 382.14, 541.66 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 327.14, 276.71, 425.05, 405.19 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 14.0, Confidence: 0.47, Bounding Box: 138.72, 594.62, 199.67, 634.08 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 56.0, Confidence: 0.38, Bounding Box: 360.99, 409.91, 426.17, 535.31 for dataset/inria/Train/pos\\crop001096.png\n",
      "Class: 60.0, Confidence: 0.27, Bounding Box: 45.61, 373.39, 220.22, 563.07 for dataset/inria/Train/pos\\crop001096.png\n",
      "\n",
      "0: 448x640 10 persons, 1 handbag, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 512.15, 144.92, 638.33, 491.07 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 43.57, 145.15, 182.71, 472.38 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 146.74, 123.47, 236.77, 399.31 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 660.36, 154.10, 722.82, 494.48 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 394.10, 165.19, 465.61, 366.59 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 473.67, 169.67, 528.46, 354.00 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 316.44, 136.69, 391.90, 383.81 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 227.73, 145.99, 283.34, 372.39 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 256.81, 159.41, 319.37, 370.33 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 373.37, 257.66, 408.35, 310.73 for dataset/inria/Train/pos\\crop001097.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 233.73, 148.46, 298.46, 371.53 for dataset/inria/Train/pos\\crop001097.png\n",
      "\n",
      "0: 640x640 8 persons, 25.0ms\n",
      "Speed: 4.0ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 34.40, 150.00, 180.45, 530.96 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 306.64, 175.32, 390.81, 389.29 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 154.49, 149.80, 246.17, 390.05 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 448.88, 195.44, 548.84, 531.48 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 213.66, 158.75, 299.39, 386.65 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 472.50, 129.65, 548.93, 330.09 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 302.31, 182.51, 333.91, 282.67 for dataset/inria/Train/pos\\crop001098.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 362.43, 166.60, 401.54, 283.44 for dataset/inria/Train/pos\\crop001098.png\n",
      "\n",
      "0: 640x448 6 persons, 1 bicycle, 1 handbag, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 168.12, 217.73, 416.20, 810.17 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 61.71, 417.91, 194.94, 824.59 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 545.59, 269.32, 655.46, 562.34 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 0.94, 259.20, 98.49, 721.40 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 89.71, 240.45, 208.17, 500.13 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 162.42, 312.57, 281.18, 589.73 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 537.99, 268.56, 571.29, 405.29 for dataset/inria/Train/pos\\crop001099.png\n",
      "Class: 1.0, Confidence: 0.28, Bounding Box: 559.51, 429.97, 636.32, 573.75 for dataset/inria/Train/pos\\crop001099.png\n",
      "\n",
      "0: 480x640 13 persons, 2 handbags, 1 skateboard, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 619.82, 30.90, 898.39, 633.94 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 341.64, 179.83, 438.80, 422.24 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 99.50, 195.70, 238.61, 570.51 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 432.45, 192.53, 523.07, 424.28 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 216.83, 183.36, 280.72, 382.83 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 523.08, 183.87, 585.18, 403.05 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 577.84, 180.92, 645.83, 438.66 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 679.71, 181.68, 787.63, 348.68 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 629.94, 158.38, 691.86, 434.31 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 762.36, 183.34, 829.55, 298.80 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 268.91, 193.16, 310.72, 365.91 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 291.12, 195.48, 352.91, 400.10 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 36.0, Confidence: 0.37, Bounding Box: 119.28, 389.88, 209.24, 572.74 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 88.01, 192.19, 132.62, 386.62 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 305.26, 227.37, 347.17, 296.59 for dataset/inria/Train/pos\\crop001102.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 306.18, 248.51, 347.55, 296.11 for dataset/inria/Train/pos\\crop001102.png\n",
      "\n",
      "0: 640x576 10 persons, 3 cars, 1 baseball glove, 19.0ms\n",
      "Speed: 5.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 222.60, 194.51, 317.96, 397.26 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 14.65, 288.64, 116.66, 398.73 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 119.06, 321.46, 213.25, 401.21 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 118.35, 148.21, 363.54, 274.30 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 2.0, Confidence: 0.61, Bounding Box: 293.87, 175.20, 419.69, 294.51 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 2.0, Confidence: 0.59, Bounding Box: 284.87, 176.36, 541.00, 293.83 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 371.56, 146.49, 388.26, 175.49 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 58.36, 149.80, 77.07, 175.39 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 22.31, 149.81, 45.25, 178.27 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 75.52, 145.95, 91.83, 175.52 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 325.75, 146.83, 340.43, 165.61 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 519.54, 151.68, 536.20, 185.89 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 120.06, 141.68, 135.11, 174.28 for dataset/inria/Train/pos\\crop001103.png\n",
      "Class: 35.0, Confidence: 0.27, Bounding Box: 118.90, 321.73, 142.05, 347.94 for dataset/inria/Train/pos\\crop001103.png\n",
      "\n",
      "0: 640x480 15 persons, 1 car, 1 horse, 1 backpack, 17.3ms\n",
      "Speed: 2.7ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 201.87, 270.39, 357.42, 687.62 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 84.98, 242.32, 253.07, 650.47 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 2.0, Confidence: 0.85, Bounding Box: 312.41, 296.52, 580.75, 424.27 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 363.40, 421.95, 430.02, 670.85 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 24.0, Confidence: 0.72, Bounding Box: 243.30, 343.15, 338.05, 482.24 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 571.15, 279.30, 623.84, 495.56 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 424.51, 270.02, 450.48, 297.04 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 493.26, 263.25, 515.92, 297.87 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 473.23, 279.22, 488.58, 299.97 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 590.48, 272.10, 624.26, 495.96 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 450.44, 278.31, 465.54, 297.51 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 538.76, 273.10, 559.78, 299.02 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 17.0, Confidence: 0.32, Bounding Box: 355.33, 0.10, 546.07, 117.34 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 515.63, 268.76, 539.30, 304.66 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 368.96, 269.28, 397.56, 302.17 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 41.39, 266.24, 58.90, 295.45 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 514.83, 269.29, 539.63, 317.49 for dataset/inria/Train/pos\\crop001104.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 0.16, 597.28, 94.63, 759.08 for dataset/inria/Train/pos\\crop001104.png\n",
      "\n",
      "0: 640x448 9 persons, 1 car, 17.6ms\n",
      "Speed: 3.0ms preprocess, 17.6ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 296.00, 187.03, 517.00, 776.26 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 0.00, 319.41, 225.11, 443.93 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 4.66, 440.50, 69.51, 692.02 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 202.27, 304.80, 274.41, 517.33 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 339.41, 293.66, 412.83, 520.22 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 63.74, 289.44, 88.31, 320.24 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 132.47, 283.01, 156.50, 317.60 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 7.23, 289.96, 37.10, 323.12 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 419.83, 288.45, 440.33, 318.02 for dataset/inria/Train/pos\\crop001105.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 399.28, 290.81, 418.81, 316.09 for dataset/inria/Train/pos\\crop001105.png\n",
      "\n",
      "0: 640x480 1 person, 1 car, 15.6ms\n",
      "Speed: 3.0ms preprocess, 15.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 217.90, 214.91, 415.86, 671.44 for dataset/inria/Train/pos\\crop001106.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 49.20, 313.77, 224.93, 445.84 for dataset/inria/Train/pos\\crop001106.png\n",
      "\n",
      "0: 640x608 12 persons, 3 backpacks, 1 handbag, 19.1ms\n",
      "Speed: 4.0ms preprocess, 19.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 508.43, 328.01, 678.66, 933.05 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 247.75, 206.96, 488.44, 875.42 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 767.53, 279.21, 920.00, 976.00 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 24.0, Confidence: 0.84, Bounding Box: 672.93, 345.70, 799.83, 533.95 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 659.86, 252.64, 842.91, 844.85 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 1.37, 200.58, 132.34, 685.81 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 433.35, 236.89, 551.58, 721.47 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 0.00, 296.44, 120.35, 933.15 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 132.81, 274.82, 274.68, 818.91 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 669.00, 250.80, 770.91, 440.91 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 24.0, Confidence: 0.44, Bounding Box: 162.94, 315.22, 272.48, 511.08 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 530.80, 243.28, 672.47, 451.66 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 24.0, Confidence: 0.37, Bounding Box: 162.70, 285.88, 323.49, 511.66 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 218.86, 500.01, 264.26, 568.60 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 832.69, 315.92, 867.15, 384.16 for dataset/inria/Train/pos\\crop001107.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 139.40, 484.07, 264.62, 818.17 for dataset/inria/Train/pos\\crop001107.png\n",
      "\n",
      "0: 640x608 10 persons, 1 car, 1 truck, 1 handbag, 21.0ms\n",
      "Speed: 4.0ms preprocess, 21.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 325.09, 129.01, 442.22, 364.44 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 166.73, 145.77, 290.67, 448.95 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 26.0, Confidence: 0.79, Bounding Box: 188.74, 199.31, 240.84, 342.04 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 7.0, Confidence: 0.71, Bounding Box: 10.37, 142.08, 106.29, 216.52 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 2.0, Confidence: 0.66, Bounding Box: 10.66, 142.15, 105.99, 216.34 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 285.11, 151.97, 301.77, 175.43 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 105.54, 23.26, 227.67, 223.51 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 221.08, 17.79, 239.93, 66.02 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 198.57, 11.09, 215.80, 50.21 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 490.44, 80.59, 546.45, 252.58 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 314.90, 155.95, 336.49, 185.49 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 439.03, 153.38, 460.12, 182.05 for dataset/inria/Train/pos\\crop001108.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 462.58, 150.84, 482.14, 177.11 for dataset/inria/Train/pos\\crop001108.png\n",
      "\n",
      "0: 640x576 5 persons, 22.5ms\n",
      "Speed: 3.0ms preprocess, 22.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 223.43, 131.45, 319.52, 408.56 for dataset/inria/Train/pos\\crop001109.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 43.92, 15.04, 474.00, 554.05 for dataset/inria/Train/pos\\crop001109.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 226.14, 1.88, 474.00, 551.73 for dataset/inria/Train/pos\\crop001109.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 319.84, 245.51, 363.91, 375.51 for dataset/inria/Train/pos\\crop001109.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 0.00, 0.27, 239.37, 146.97 for dataset/inria/Train/pos\\crop001109.png\n",
      "\n",
      "0: 640x544 4 persons, 2 cars, 1 bus, 23.0ms\n",
      "Speed: 4.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 251.42, 285.25, 330.50, 478.02 for dataset/inria/Train/pos\\crop001110.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 41.17, 295.62, 75.44, 397.34 for dataset/inria/Train/pos\\crop001110.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.79, 291.80, 33.40, 400.20 for dataset/inria/Train/pos\\crop001110.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 213.05, 282.39, 263.74, 473.74 for dataset/inria/Train/pos\\crop001110.png\n",
      "Class: 5.0, Confidence: 0.78, Bounding Box: 119.95, 241.17, 208.70, 308.45 for dataset/inria/Train/pos\\crop001110.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 81.03, 272.12, 113.91, 299.90 for dataset/inria/Train/pos\\crop001110.png\n",
      "Class: 2.0, Confidence: 0.32, Bounding Box: 0.07, 265.03, 68.76, 297.94 for dataset/inria/Train/pos\\crop001110.png\n",
      "\n",
      "0: 640x480 2 persons, 33.5ms\n",
      "Speed: 3.0ms preprocess, 33.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 212.46, 254.82, 282.13, 475.36 for dataset/inria/Train/pos\\crop001111.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 1.63, 145.59, 366.54, 714.05 for dataset/inria/Train/pos\\crop001111.png\n",
      "\n",
      "0: 608x640 4 persons, 9 cars, 1 bus, 37.0ms\n",
      "Speed: 4.0ms preprocess, 37.0ms inference, 3.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 233.03, 206.16, 277.17, 312.47 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 284.84, 209.09, 319.40, 310.48 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 457.77, 193.71, 505.34, 386.89 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.78, Bounding Box: 165.27, 189.40, 206.46, 220.39 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 496.22, 206.76, 521.86, 390.15 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 328.11, 184.32, 357.29, 211.48 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 5.0, Confidence: 0.58, Bounding Box: 363.53, 153.83, 450.84, 219.48 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.57, Bounding Box: 5.25, 213.41, 85.93, 256.93 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.53, Bounding Box: 234.54, 178.39, 287.11, 207.03 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 43.80, 213.53, 86.24, 257.02 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 190.37, 174.60, 242.64, 205.13 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.48, Bounding Box: 277.93, 182.27, 312.95, 210.15 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 96.84, 197.37, 147.02, 238.86 for dataset/inria/Train/pos\\crop001112.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 448.09, 169.69, 464.60, 204.34 for dataset/inria/Train/pos\\crop001112.png\n",
      "\n",
      "0: 640x448 11 persons, 32.5ms\n",
      "Speed: 11.0ms preprocess, 32.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 223.04, 263.06, 378.19, 747.65 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 1.21, 331.89, 107.20, 926.77 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 329.16, 252.51, 445.55, 653.83 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 489.46, 188.48, 593.31, 524.19 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 437.54, 215.39, 506.89, 542.44 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 536.75, 184.62, 610.00, 632.14 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 152.28, 39.35, 210.10, 196.09 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 305.21, 101.57, 338.27, 202.20 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 430.08, 67.09, 479.29, 213.13 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 385.38, 61.58, 437.85, 196.39 for dataset/inria/Train/pos\\crop001113.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 354.88, 203.03, 464.17, 453.38 for dataset/inria/Train/pos\\crop001113.png\n",
      "\n",
      "0: 640x640 17 persons, 1 handbag, 21.0ms\n",
      "Speed: 4.0ms preprocess, 21.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 653.31, 248.53, 836.24, 772.60 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 788.19, 262.21, 951.07, 737.04 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 0.00, 319.89, 121.48, 802.66 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 284.67, 293.71, 448.39, 704.02 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 232.92, 245.64, 334.09, 571.51 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 437.26, 234.38, 604.13, 780.91 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 182.28, 274.56, 251.32, 602.68 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 77.97, 307.24, 186.42, 710.91 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 794.35, 241.54, 856.82, 345.53 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 175.09, 125.09, 223.93, 268.55 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 49.90, 158.85, 83.48, 259.62 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 405.53, 242.19, 481.32, 669.10 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 531.21, 255.47, 628.70, 718.14 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 551.25, 323.87, 609.68, 482.18 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 130.95, 119.46, 180.86, 247.06 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 353.52, 164.23, 387.12, 242.66 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 306.92, 235.26, 413.47, 369.05 for dataset/inria/Train/pos\\crop001114.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 130.61, 218.70, 179.98, 306.73 for dataset/inria/Train/pos\\crop001114.png\n",
      "\n",
      "0: 544x640 7 persons, 15.9ms\n",
      "Speed: 3.6ms preprocess, 15.9ms inference, 2.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 231.65, 120.40, 282.02, 276.30 for dataset/inria/Train/pos\\crop001115.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 453.09, 131.99, 475.89, 272.64 for dataset/inria/Train/pos\\crop001115.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 183.89, 113.96, 239.74, 276.00 for dataset/inria/Train/pos\\crop001115.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 174.97, 161.65, 211.19, 279.31 for dataset/inria/Train/pos\\crop001115.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 174.21, 120.67, 206.93, 192.72 for dataset/inria/Train/pos\\crop001115.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 174.45, 120.97, 206.16, 170.54 for dataset/inria/Train/pos\\crop001115.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 431.04, 190.42, 455.51, 256.84 for dataset/inria/Train/pos\\crop001115.png\n",
      "\n",
      "0: 512x640 12 persons, 1 elephant, 2 handbags, 17.8ms\n",
      "Speed: 1.6ms preprocess, 17.8ms inference, 2.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 244.16, 178.97, 296.23, 316.88 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 376.51, 161.83, 429.94, 315.21 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 302.01, 207.19, 345.29, 312.64 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 438.21, 187.56, 485.47, 311.37 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 357.61, 217.05, 386.20, 302.29 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 183.80, 178.04, 221.46, 314.14 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 462.55, 226.31, 534.41, 359.85 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 26.0, Confidence: 0.44, Bounding Box: 209.19, 225.23, 230.11, 255.40 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 489.14, 320.17, 543.81, 437.49 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 556.71, 258.37, 581.87, 437.11 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 209.21, 202.64, 230.76, 255.35 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 20.0, Confidence: 0.30, Bounding Box: 0.03, 166.95, 13.46, 344.11 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 208.58, 202.09, 234.87, 296.91 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 488.81, 320.15, 567.89, 436.99 for dataset/inria/Train/pos\\crop001116.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 557.38, 160.49, 581.87, 286.41 for dataset/inria/Train/pos\\crop001116.png\n",
      "\n",
      "0: 480x640 19 persons, 1 backpack, 19.6ms\n",
      "Speed: 2.6ms preprocess, 19.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 438.18, 249.62, 504.86, 378.00 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 266.73, 255.52, 367.95, 377.18 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 127.70, 270.33, 234.68, 378.00 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 81.84, 167.54, 113.90, 266.37 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 216.49, 290.57, 276.40, 376.42 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 161.79, 126.32, 213.51, 260.46 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 48.92, 325.80, 110.44, 378.00 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 385.49, 172.19, 417.75, 250.27 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 206.91, 132.48, 254.79, 275.88 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 336.39, 243.26, 410.87, 376.48 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 351.52, 175.39, 378.04, 255.37 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 24.0, Confidence: 0.66, Bounding Box: 175.35, 150.53, 209.28, 201.26 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 223.76, 213.47, 294.23, 305.37 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 349.63, 300.89, 448.13, 378.00 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 157.23, 225.27, 210.07, 316.39 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 180.90, 283.29, 232.32, 352.06 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 103.87, 312.31, 158.21, 378.00 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 90.61, 274.15, 134.74, 340.05 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 337.56, 243.29, 448.23, 375.71 for dataset/inria/Train/pos\\crop001119.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 71.36, 274.25, 101.64, 332.20 for dataset/inria/Train/pos\\crop001119.png\n",
      "\n",
      "0: 544x640 6 persons, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.78, 11.96, 335.24, 495.53 for dataset/inria/Train/pos\\crop001120.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 581.06, 207.17, 611.85, 348.57 for dataset/inria/Train/pos\\crop001120.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 414.62, 177.32, 465.02, 337.80 for dataset/inria/Train/pos\\crop001120.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 228.65, 152.04, 275.51, 328.57 for dataset/inria/Train/pos\\crop001120.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 264.57, 156.82, 323.42, 332.46 for dataset/inria/Train/pos\\crop001120.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 358.16, 158.51, 418.63, 334.65 for dataset/inria/Train/pos\\crop001120.png\n",
      "\n",
      "0: 544x640 5 persons, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 196.31, 210.12, 243.53, 351.26 for dataset/inria/Train/pos\\crop001121.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 32.04, 179.13, 79.45, 338.37 for dataset/inria/Train/pos\\crop001121.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 248.78, 165.82, 324.59, 361.21 for dataset/inria/Train/pos\\crop001121.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 347.25, 162.70, 417.69, 355.91 for dataset/inria/Train/pos\\crop001121.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 0.00, 162.18, 34.75, 337.01 for dataset/inria/Train/pos\\crop001121.png\n",
      "\n",
      "0: 640x576 3 persons, 1 backpack, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 200.46, 195.05, 273.61, 401.51 for dataset/inria/Train/pos\\crop001122.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 263.01, 191.43, 340.01, 406.71 for dataset/inria/Train/pos\\crop001122.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 0.00, 186.61, 32.78, 374.35 for dataset/inria/Train/pos\\crop001122.png\n",
      "Class: 24.0, Confidence: 0.43, Bounding Box: 288.18, 220.86, 337.33, 285.80 for dataset/inria/Train/pos\\crop001122.png\n",
      "\n",
      "0: 640x576 8 persons, 2 traffic lights, 1 stop sign, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 371.48, 152.24, 540.33, 518.65 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 191.18, 144.98, 346.37, 478.42 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 268.11, 136.94, 363.20, 482.13 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 38.72, 175.32, 79.52, 218.01 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 9.0, Confidence: 0.62, Bounding Box: 373.51, 40.40, 406.34, 109.21 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 328.11, 154.40, 381.75, 405.51 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 367.59, 165.39, 449.31, 300.01 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 363.84, 163.12, 448.46, 414.76 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 11.0, Confidence: 0.44, Bounding Box: 365.32, 116.83, 415.76, 164.19 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 9.0, Confidence: 0.42, Bounding Box: 267.09, 113.43, 278.89, 139.69 for dataset/inria/Train/pos\\crop001123.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 485.25, 179.62, 538.51, 277.22 for dataset/inria/Train/pos\\crop001123.png\n",
      "\n",
      "0: 640x448 9 persons, 1 traffic light, 1 stop sign, 2 handbags, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 211.77, 182.86, 370.92, 549.57 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 363.63, 218.52, 481.13, 551.24 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 21.56, 175.95, 179.06, 510.10 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 98.90, 174.16, 188.47, 513.03 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 11.0, Confidence: 0.67, Bounding Box: 193.40, 148.74, 247.02, 196.46 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 153.83, 187.57, 214.15, 434.60 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 9.0, Confidence: 0.53, Bounding Box: 203.96, 72.49, 237.68, 145.84 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 26.0, Confidence: 0.45, Bounding Box: 449.06, 296.77, 495.41, 393.44 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 198.69, 198.95, 274.26, 442.60 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 449.70, 325.38, 495.31, 393.69 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 156.13, 188.01, 213.81, 335.39 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 200.61, 196.25, 280.39, 331.88 for dataset/inria/Train/pos\\crop001124.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 198.94, 217.65, 241.45, 331.30 for dataset/inria/Train/pos\\crop001124.png\n",
      "\n",
      "0: 640x480 8 persons, 1 backpack, 1 handbag, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 247.96, 220.54, 352.74, 548.94 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 404.60, 228.93, 497.56, 501.05 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 122.21, 241.69, 226.78, 554.60 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 364.40, 260.81, 439.37, 497.50 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 24.0, Confidence: 0.56, Bounding Box: 461.73, 266.56, 506.00, 366.79 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 222.15, 303.78, 277.64, 487.62 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 0.00, 231.65, 72.80, 689.00 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 0.00, 234.50, 42.58, 686.76 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 319.23, 265.71, 370.24, 436.68 for dataset/inria/Train/pos\\crop001125.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 462.56, 262.40, 506.00, 370.13 for dataset/inria/Train/pos\\crop001125.png\n",
      "\n",
      "0: 608x640 13 persons, 1 backpack, 7 handbags, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 202.10, 187.35, 294.51, 458.07 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 633.19, 210.55, 683.82, 440.60 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 46.22, 178.28, 150.31, 506.62 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 552.10, 207.93, 626.37, 436.97 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 372.88, 213.51, 435.28, 444.08 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 301.47, 209.12, 381.23, 446.26 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 423.13, 215.24, 496.07, 447.02 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 160.36, 220.33, 241.19, 450.97 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 476.44, 197.57, 558.38, 437.41 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.53, Bounding Box: 296.64, 244.85, 373.15, 345.68 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.53, Bounding Box: 607.73, 276.77, 654.55, 354.46 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 18.79, 252.89, 76.87, 442.58 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 0.00, 204.18, 24.07, 521.14 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 609.85, 299.79, 652.82, 355.21 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 0.00, 201.95, 24.12, 404.01 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 550.10, 257.52, 595.29, 353.93 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 117.51, 223.86, 166.06, 404.88 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 369.27, 250.97, 389.06, 325.43 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 67.34, 279.06, 131.11, 350.10 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 260.60, 221.58, 312.27, 327.09 for dataset/inria/Train/pos\\crop001126.png\n",
      "Class: 24.0, Confidence: 0.25, Bounding Box: 259.52, 225.11, 311.39, 323.52 for dataset/inria/Train/pos\\crop001126.png\n",
      "\n",
      "0: 544x640 9 persons, 1 backpack, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 362.16, 178.39, 463.23, 424.23 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 24.0, Confidence: 0.86, Bounding Box: 379.30, 216.72, 437.86, 295.60 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 440.62, 2.73, 689.18, 585.53 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 282.54, 190.02, 346.81, 378.83 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 194.04, 154.46, 276.75, 475.03 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 339.33, 207.39, 385.78, 375.31 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 115.93, 247.76, 147.75, 295.31 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 82.79, 256.96, 113.49, 293.09 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 33.48, 256.52, 54.81, 294.82 for dataset/inria/Train/pos\\crop001127.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 0.00, 256.73, 22.74, 297.38 for dataset/inria/Train/pos\\crop001127.png\n",
      "\n",
      "0: 640x512 10 persons, 1 backpack, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 485.34, 359.57, 631.00, 654.85 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 163.79, 185.69, 458.25, 785.24 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 0.20, 133.84, 149.87, 779.77 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 135.51, 323.39, 253.82, 610.36 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 372.15, 333.50, 595.17, 633.78 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 564.69, 262.61, 629.35, 372.92 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 385.27, 226.81, 454.98, 347.52 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 459.47, 228.81, 527.45, 399.08 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 390.81, 334.92, 491.51, 473.45 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 24.0, Confidence: 0.42, Bounding Box: 314.06, 326.08, 434.09, 510.00 for dataset/inria/Train/pos\\crop001128.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 362.03, 334.82, 494.19, 631.15 for dataset/inria/Train/pos\\crop001128.png\n",
      "\n",
      "0: 640x384 6 persons, 1 car, 1 fire hydrant, 3 handbags, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 252.83, 151.67, 359.37, 441.68 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 69.53, 152.49, 245.46, 558.82 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 327.85, 198.47, 403.29, 706.78 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 186.88, 137.12, 255.56, 419.85 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 0.00, 352.08, 44.64, 387.71 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 343.24, 159.72, 404.37, 284.71 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 188.06, 136.22, 255.09, 296.86 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 54.61, 296.78, 118.91, 380.21 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 55.71, 217.33, 136.93, 385.82 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 10.0, Confidence: 0.29, Bounding Box: 111.61, 553.79, 155.43, 707.58 for dataset/inria/Train/pos\\crop001129.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 288.14, 196.51, 358.14, 305.10 for dataset/inria/Train/pos\\crop001129.png\n",
      "\n",
      "0: 640x416 7 persons, 3 cars, 2 handbags, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 178.95, 200.27, 415.97, 872.28 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 460.89, 191.15, 611.45, 748.75 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 20.31, 217.94, 121.10, 481.83 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 85.88, 273.25, 238.73, 778.37 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 423.38, 207.34, 508.40, 528.34 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 2.0, Confidence: 0.62, Bounding Box: 138.52, 252.77, 249.18, 346.17 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 2.0, Confidence: 0.55, Bounding Box: 351.77, 255.21, 499.71, 386.22 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 105.08, 226.10, 161.61, 343.61 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 26.0, Confidence: 0.46, Bounding Box: 51.71, 260.60, 121.48, 358.11 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 2.0, Confidence: 0.36, Bounding Box: 315.47, 239.86, 426.92, 288.64 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 421.43, 207.97, 504.09, 443.80 for dataset/inria/Train/pos\\crop001130.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 67.97, 283.12, 119.99, 355.48 for dataset/inria/Train/pos\\crop001130.png\n",
      "\n",
      "0: 640x416 8 persons, 4 cars, 34.0ms\n",
      "Speed: 10.1ms preprocess, 34.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.00, 243.62, 147.13, 925.18 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 192.58, 241.47, 424.35, 805.03 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 489.93, 303.01, 600.00, 816.41 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 346.23, 247.84, 437.53, 455.09 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 154.70, 258.41, 242.01, 516.33 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 527.19, 265.95, 571.80, 376.98 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 471.81, 274.52, 543.69, 382.65 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 2.0, Confidence: 0.43, Bounding Box: 51.75, 289.21, 160.17, 337.57 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 89.30, 307.96, 202.36, 439.70 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 2.0, Confidence: 0.32, Bounding Box: 89.51, 305.08, 249.14, 439.68 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 472.12, 273.51, 544.50, 545.23 for dataset/inria/Train/pos\\crop001131.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 46.94, 303.94, 207.47, 439.71 for dataset/inria/Train/pos\\crop001131.png\n",
      "\n",
      "0: 640x480 10 persons, 1 horse, 1 backpack, 1 handbag, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 205.13, 242.62, 463.28, 946.05 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 76.81, 243.81, 254.95, 756.77 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 501.85, 338.00, 690.95, 974.69 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 531.59, 190.80, 607.97, 370.13 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 373.75, 195.97, 434.47, 372.43 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 461.23, 233.13, 527.47, 429.92 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 599.61, 205.01, 665.61, 412.88 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 428.50, 356.16, 619.82, 734.68 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 206.79, 191.54, 274.52, 350.35 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 427.14, 356.60, 626.94, 968.22 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 175.99, 323.89, 228.85, 501.56 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 24.0, Confidence: 0.26, Bounding Box: 377.10, 230.23, 420.85, 295.35 for dataset/inria/Train/pos\\crop001132.png\n",
      "Class: 17.0, Confidence: 0.25, Bounding Box: 149.11, 82.96, 255.01, 212.50 for dataset/inria/Train/pos\\crop001132.png\n",
      "\n",
      "0: 544x640 14 persons, 1 backpack, 3 handbags, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 244.41, 323.83, 390.07, 473.23 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 0.22, 190.08, 205.38, 471.84 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 217.06, 188.06, 293.16, 401.80 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 433.80, 159.87, 515.17, 300.93 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 373.21, 158.57, 442.12, 388.16 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 290.68, 141.58, 380.14, 339.48 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 24.0, Confidence: 0.72, Bounding Box: 128.08, 183.24, 177.36, 253.29 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 125.25, 148.42, 191.57, 339.64 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 70.36, 178.83, 125.57, 297.10 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 393.66, 16.71, 591.79, 383.68 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 332.91, 29.12, 591.78, 467.20 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 328.08, 285.67, 591.71, 470.20 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 328.71, 171.91, 591.65, 470.18 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 26.0, Confidence: 0.35, Bounding Box: 281.70, 274.76, 313.88, 343.62 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 208.22, 149.49, 250.28, 232.16 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 207.86, 229.77, 247.14, 298.84 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 208.20, 254.63, 243.75, 299.02 for dataset/inria/Train/pos\\crop001133.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 219.04, 152.33, 266.45, 245.81 for dataset/inria/Train/pos\\crop001133.png\n",
      "\n",
      "0: 640x480 8 persons, 1 bicycle, 2 cars, 3 handbags, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 349.44, 167.23, 451.00, 609.22 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 177.56, 175.60, 285.56, 468.49 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 8.47, 180.17, 173.42, 593.75 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 250.34, 236.08, 381.22, 599.24 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 26.0, Confidence: 0.63, Bounding Box: 0.24, 244.46, 58.61, 411.76 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 109.27, 161.40, 179.87, 452.28 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 267.47, 186.10, 334.67, 298.98 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 2.0, Confidence: 0.50, Bounding Box: 310.94, 212.81, 429.15, 294.74 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 370.28, 180.29, 399.04, 214.12 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 110.38, 161.86, 179.22, 331.87 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 269.96, 211.55, 427.08, 308.96 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 0.07, 288.42, 52.85, 411.66 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 208.88, 217.27, 277.34, 325.86 for dataset/inria/Train/pos\\crop001134.png\n",
      "Class: 1.0, Confidence: 0.31, Bounding Box: 154.49, 291.77, 205.93, 428.65 for dataset/inria/Train/pos\\crop001134.png\n",
      "\n",
      "0: 640x544 2 persons, 4 cars, 1 backpack, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 2.0, Confidence: 0.87, Bounding Box: 36.23, 158.51, 130.14, 206.13 for dataset/inria/Train/pos\\crop001135.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 156.40, 141.00, 216.60, 338.55 for dataset/inria/Train/pos\\crop001135.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.47, 237.21, 79.74, 323.23 for dataset/inria/Train/pos\\crop001135.png\n",
      "Class: 2.0, Confidence: 0.60, Bounding Box: 0.80, 163.60, 41.73, 206.97 for dataset/inria/Train/pos\\crop001135.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 125.80, 158.29, 164.83, 199.05 for dataset/inria/Train/pos\\crop001135.png\n",
      "Class: 2.0, Confidence: 0.42, Bounding Box: 125.08, 155.01, 226.46, 201.09 for dataset/inria/Train/pos\\crop001135.png\n",
      "Class: 24.0, Confidence: 0.29, Bounding Box: 240.60, 156.18, 276.06, 195.12 for dataset/inria/Train/pos\\crop001135.png\n",
      "\n",
      "0: 544x640 6 persons, 3 cars, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 310.90, 113.49, 554.87, 459.76 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 40.08, 166.64, 121.14, 217.92 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 214.30, 157.95, 247.48, 273.24 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 154.00, 166.58, 223.22, 215.79 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 240.75, 165.56, 278.21, 272.70 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 310.97, 170.06, 336.39, 270.49 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 292.04, 167.27, 318.24, 271.31 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 2.0, Confidence: 0.68, Bounding Box: 0.03, 177.28, 13.22, 218.61 for dataset/inria/Train/pos\\crop001136.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 412.85, 149.21, 439.56, 210.17 for dataset/inria/Train/pos\\crop001136.png\n",
      "\n",
      "0: 640x480 8 persons, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 218.35, 201.48, 333.57, 593.88 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 394.48, 230.49, 513.00, 654.24 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 0.53, 201.18, 150.33, 726.23 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 140.66, 251.39, 246.74, 553.90 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 362.78, 220.40, 440.88, 570.98 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 115.66, 201.54, 162.94, 274.12 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 364.28, 271.34, 424.51, 570.66 for dataset/inria/Train/pos\\crop001137.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 0.00, 202.41, 297.39, 725.92 for dataset/inria/Train/pos\\crop001137.png\n",
      "\n",
      "0: 640x544 9 persons, 1 bench, 1 handbag, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 316.05, 236.48, 471.89, 694.32 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 419.84, 428.45, 610.88, 735.36 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 52.52, 207.69, 165.76, 602.35 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 223.69, 235.38, 365.65, 678.74 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 194.80, 226.16, 271.26, 574.16 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 503.43, 153.35, 609.61, 467.64 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 442.02, 261.30, 537.20, 549.76 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 0.67, 269.94, 75.72, 567.01 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 321.79, 242.58, 374.65, 308.38 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 135.98, 335.76, 198.56, 459.22 for dataset/inria/Train/pos\\crop001138.png\n",
      "Class: 13.0, Confidence: 0.26, Bounding Box: 0.24, 541.03, 130.79, 730.95 for dataset/inria/Train/pos\\crop001138.png\n",
      "\n",
      "0: 640x480 8 persons, 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 177.33, 224.52, 294.44, 624.32 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 0.47, 245.86, 236.21, 707.29 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 50.97, 122.24, 192.34, 349.80 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 415.58, 179.77, 503.55, 706.22 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 1.50, 184.91, 72.94, 279.70 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 283.84, 297.52, 434.39, 615.72 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 228.88, 191.31, 328.14, 363.00 for dataset/inria/Train/pos\\crop001140.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 318.55, 222.28, 395.21, 302.87 for dataset/inria/Train/pos\\crop001140.png\n",
      "\n",
      "0: 640x512 10 persons, 1 handbag, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 576.10, 198.60, 720.68, 576.28 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 176.75, 222.89, 371.88, 742.59 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 423.36, 205.97, 547.58, 747.55 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 325.54, 231.58, 418.47, 714.98 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 120.92, 234.92, 192.57, 417.72 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 42.18, 220.29, 117.40, 415.57 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 571.98, 191.95, 637.51, 426.82 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 394.92, 228.91, 464.79, 428.28 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 185.13, 234.07, 239.38, 363.06 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 26.0, Confidence: 0.39, Bounding Box: 402.93, 364.61, 524.04, 504.36 for dataset/inria/Train/pos\\crop001141.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 575.77, 193.77, 636.39, 315.76 for dataset/inria/Train/pos\\crop001141.png\n",
      "\n",
      "0: 640x480 4 persons, 1 chair, 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 201.50, 188.28, 317.43, 476.85 for dataset/inria/Train/pos\\crop001142.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 271.12, 205.96, 562.00, 756.00 for dataset/inria/Train/pos\\crop001142.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 431.42, 191.96, 560.25, 350.13 for dataset/inria/Train/pos\\crop001142.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 496.45, 158.71, 561.12, 270.98 for dataset/inria/Train/pos\\crop001142.png\n",
      "Class: 56.0, Confidence: 0.29, Bounding Box: 117.38, 258.30, 212.26, 383.89 for dataset/inria/Train/pos\\crop001142.png\n",
      "\n",
      "0: 640x608 18 persons, 1 backpack, 2 handbags, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 149.00, 435.17, 301.44, 697.40 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 417.13, 155.45, 512.39, 477.04 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 57.60, 79.24, 148.29, 320.25 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 275.77, 164.63, 413.69, 518.27 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 483.34, 141.76, 570.57, 422.49 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 193.76, 120.93, 293.93, 476.87 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 137.79, 117.19, 215.63, 418.91 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 440.84, 220.75, 647.94, 696.97 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 418.20, 34.41, 484.95, 152.96 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 551.58, 155.56, 644.38, 398.08 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 417.35, 111.40, 499.32, 191.78 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 575.57, 51.42, 629.38, 166.42 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 1.10, 251.99, 192.77, 696.55 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 100.00, 180.53, 159.35, 258.38 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 1.45, 251.51, 122.88, 688.71 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 24.0, Confidence: 0.31, Bounding Box: 165.01, 176.17, 219.98, 277.60 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 400.02, 290.02, 433.96, 336.24 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 292.74, 127.68, 365.52, 275.20 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 360.33, 165.24, 442.98, 523.93 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 155.79, 12.02, 205.18, 118.28 for dataset/inria/Train/pos\\crop001143.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 528.33, 98.18, 598.27, 231.97 for dataset/inria/Train/pos\\crop001143.png\n",
      "\n",
      "0: 544x640 22 persons, 1 boat, 1 handbag, 16.5ms\n",
      "Speed: 3.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.21, 243.84, 152.36, 504.90 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 411.90, 155.56, 489.29, 364.36 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 449.35, 305.03, 549.68, 431.06 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 180.74, 354.78, 300.86, 510.00 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 345.90, 163.71, 420.38, 373.71 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 86.69, 214.88, 178.95, 418.35 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 318.06, 139.80, 363.06, 366.36 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 160.53, 156.45, 216.94, 347.16 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 233.62, 201.98, 311.79, 367.18 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 199.87, 192.08, 248.76, 351.50 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 498.53, 268.28, 566.35, 409.18 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 491.99, 186.87, 577.77, 350.30 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 268.62, 155.74, 321.03, 358.93 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 65.86, 204.36, 107.30, 269.75 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 271.35, 405.83, 390.47, 510.00 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 124.28, 156.62, 173.89, 339.32 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 8.0, Confidence: 0.38, Bounding Box: 522.24, 151.65, 558.14, 175.58 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 459.05, 438.78, 524.12, 508.18 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 566.06, 186.92, 600.93, 283.68 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 402.13, 404.58, 586.01, 509.59 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 178.30, 403.33, 240.21, 510.00 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 573.87, 201.70, 603.83, 472.66 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 582.63, 223.05, 603.80, 474.07 for dataset/inria/Train/pos\\crop001144.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 458.18, 405.32, 586.66, 509.82 for dataset/inria/Train/pos\\crop001144.png\n",
      "\n",
      "0: 576x640 18 persons, 1 backpack, 2 handbags, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.53, 104.60, 74.63, 314.64 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 66.47, 97.81, 144.39, 306.94 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 103.20, 246.38, 204.31, 371.86 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 239.24, 179.30, 330.04, 421.88 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 456.08, 264.66, 597.24, 509.06 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 381.39, 222.58, 475.86, 509.71 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 320.35, 216.02, 399.27, 509.86 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 33.93, 376.73, 265.01, 510.24 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 152.55, 209.72, 220.01, 350.38 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 145.49, 127.79, 233.28, 289.39 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 0.13, 78.83, 16.23, 288.59 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 24.0, Confidence: 0.47, Bounding Box: 348.41, 277.83, 402.19, 377.95 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 475.27, 227.73, 549.79, 372.91 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 282.85, 263.04, 344.78, 357.91 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 339.44, 112.44, 387.90, 217.34 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 26.0, Confidence: 0.35, Bounding Box: 547.93, 442.19, 599.13, 512.36 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 0.04, 80.06, 17.51, 190.75 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 485.52, 115.54, 571.44, 218.94 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 0.06, 356.62, 42.85, 435.61 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 380.20, 104.35, 429.53, 210.31 for dataset/inria/Train/pos\\crop001145.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 549.49, 141.11, 599.76, 233.90 for dataset/inria/Train/pos\\crop001145.png\n",
      "\n",
      "0: 640x544 11 persons, 1 backpack, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 257.58, 211.38, 413.28, 720.00 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 386.08, 0.00, 511.22, 295.86 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 279.24, 0.12, 405.47, 251.20 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 227.37, 0.00, 325.34, 193.88 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 0.62, 24.12, 232.45, 720.00 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 175.05, 0.15, 241.72, 98.84 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 468.39, 0.12, 572.18, 308.01 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 543.37, 323.64, 582.87, 615.84 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 24.0, Confidence: 0.40, Bounding Box: 167.78, 263.79, 303.53, 477.26 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 91.28, 198.51, 304.63, 716.85 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 530.62, 0.03, 583.36, 229.43 for dataset/inria/Train/pos\\crop001146.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 197.50, 0.38, 242.14, 98.58 for dataset/inria/Train/pos\\crop001146.png\n",
      "\n",
      "0: 640x480 11 persons, 1 car, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 152.59, 137.55, 306.86, 451.45 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 2.0, Confidence: 0.80, Bounding Box: 0.00, 211.84, 42.27, 367.25 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 1.03, 128.70, 51.03, 253.85 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 252.74, 142.47, 297.42, 259.69 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 71.77, 144.72, 103.97, 258.02 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 359.15, 131.42, 398.79, 255.90 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 402.87, 192.96, 439.73, 360.14 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 133.59, 124.36, 180.34, 253.33 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 236.10, 137.79, 265.55, 251.85 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 291.40, 130.06, 335.91, 261.37 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 387.12, 118.51, 430.16, 269.14 for dataset/inria/Train/pos\\crop001147.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 288.54, 127.58, 324.45, 261.60 for dataset/inria/Train/pos\\crop001147.png\n",
      "\n",
      "0: 576x640 2 persons, 1 traffic light, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 5.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 165.35, 78.96, 217.73, 202.37 for dataset/inria/Train/pos\\crop001148.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 187.47, 81.58, 217.78, 192.84 for dataset/inria/Train/pos\\crop001148.png\n",
      "Class: 9.0, Confidence: 0.37, Bounding Box: 72.47, 37.17, 92.08, 67.98 for dataset/inria/Train/pos\\crop001148.png\n",
      "\n",
      "0: 640x512 13 persons, 1 car, 1 bottle, 23.1ms\n",
      "Speed: 2.0ms preprocess, 23.1ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.20, 156.68, 181.97, 580.78 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 230.23, 192.53, 354.86, 581.37 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 123.44, 171.66, 225.48, 548.11 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 389.25, 164.98, 444.98, 391.01 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 230.55, 146.53, 578.00, 722.03 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 247.99, 141.13, 311.76, 272.60 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 39.0, Confidence: 0.74, Bounding Box: 4.82, 331.91, 31.20, 407.34 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 339.18, 142.19, 404.49, 409.25 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 2.0, Confidence: 0.63, Bounding Box: 433.50, 198.39, 562.45, 301.76 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 517.98, 191.40, 578.00, 366.72 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 309.23, 137.12, 371.89, 293.15 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 309.71, 138.71, 379.63, 428.42 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 316.98, 143.38, 395.30, 412.55 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 519.21, 189.68, 578.00, 522.50 for dataset/inria/Train/pos\\crop001149.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 231.46, 448.37, 578.00, 723.39 for dataset/inria/Train/pos\\crop001149.png\n",
      "\n",
      "0: 640x640 7 persons, 1 car, 19.2ms\n",
      "Speed: 3.3ms preprocess, 19.2ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 0.16, 116.49, 142.15, 337.70 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 230.48, 88.23, 277.30, 228.94 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 94.99, 108.99, 150.15, 227.44 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 255.09, 109.91, 353.51, 335.90 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 57.52, 95.49, 97.90, 184.59 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 172.96, 119.36, 202.95, 225.83 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 338.77, 104.37, 353.92, 234.43 for dataset/inria/Train/pos\\crop001150.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 25.11, 93.98, 55.18, 138.07 for dataset/inria/Train/pos\\crop001150.png\n",
      "\n",
      "0: 640x512 2 persons, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 276.96, 242.39, 475.71, 813.18 for dataset/inria/Train/pos\\crop001151.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 0.00, 259.64, 51.42, 611.87 for dataset/inria/Train/pos\\crop001151.png\n",
      "\n",
      "0: 640x512 3 persons, 10.9ms\n",
      "Speed: 1.4ms preprocess, 10.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 272.89, 192.18, 526.85, 934.63 for dataset/inria/Train/pos\\crop001152.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 275.83, 132.77, 321.60, 235.28 for dataset/inria/Train/pos\\crop001152.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 730.27, 412.48, 768.00, 566.80 for dataset/inria/Train/pos\\crop001152.png\n",
      "\n",
      "0: 640x576 2 persons, 1 backpack, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 289.94, 342.08, 514.91, 593.57 for dataset/inria/Train/pos\\crop001153.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 250.11, 272.64, 320.33, 404.42 for dataset/inria/Train/pos\\crop001153.png\n",
      "Class: 24.0, Confidence: 0.35, Bounding Box: 289.24, 280.99, 321.14, 335.28 for dataset/inria/Train/pos\\crop001153.png\n",
      "\n",
      "0: 640x608 1 person, 28.4ms\n",
      "Speed: 5.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 286.37, 166.86, 619.17, 960.75 for dataset/inria/Train/pos\\crop001154.png\n",
      "\n",
      "0: 640x480 1 person, 16.0ms\n",
      "Speed: 6.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 237.46, 171.96, 427.45, 953.80 for dataset/inria/Train/pos\\crop001155.png\n",
      "\n",
      "0: 640x384 1 person, 20.5ms\n",
      "Speed: 3.0ms preprocess, 20.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 180.22, 195.07, 358.22, 781.28 for dataset/inria/Train/pos\\crop001156.png\n",
      "\n",
      "0: 640x416 8 persons, 2 bicycles, 3 handbags, 1 suitcase, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 53.02, 92.13, 130.91, 338.79 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 25.72, 93.69, 72.59, 249.10 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 154.31, 88.92, 203.28, 226.25 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 205.59, 98.36, 235.25, 172.55 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 1.0, Confidence: 0.63, Bounding Box: 211.10, 135.28, 261.80, 207.19 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 136.67, 96.15, 154.49, 149.75 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 28.0, Confidence: 0.53, Bounding Box: 48.64, 228.30, 74.38, 303.42 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 235.64, 104.48, 262.00, 153.68 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 1.0, Confidence: 0.48, Bounding Box: 236.59, 148.21, 261.89, 227.61 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 26.0, Confidence: 0.45, Bounding Box: 48.56, 227.18, 74.43, 303.25 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 130.71, 98.74, 139.43, 127.34 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 139.50, 118.43, 163.85, 175.68 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 143.90, 108.11, 165.05, 155.16 for dataset/inria/Train/pos\\crop001158.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 189.11, 100.16, 209.87, 124.45 for dataset/inria/Train/pos\\crop001158.png\n",
      "\n",
      "0: 640x640 3 persons, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 102.71, 73.54, 157.01, 174.58 for dataset/inria/Train/pos\\crop001160.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 227.31, 117.69, 254.86, 257.45 for dataset/inria/Train/pos\\crop001160.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 172.21, 72.46, 195.59, 139.88 for dataset/inria/Train/pos\\crop001160.png\n",
      "\n",
      "0: 640x544 5 persons, 1 handbag, 31.0ms\n",
      "Speed: 5.0ms preprocess, 31.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 186.62, 95.60, 255.97, 345.86 for dataset/inria/Train/pos\\crop001162.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 258.18, 109.61, 323.27, 287.14 for dataset/inria/Train/pos\\crop001162.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 118.14, 91.64, 188.71, 343.60 for dataset/inria/Train/pos\\crop001162.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 307.50, 78.82, 349.00, 257.93 for dataset/inria/Train/pos\\crop001162.png\n",
      "Class: 26.0, Confidence: 0.58, Bounding Box: 121.49, 133.62, 181.30, 204.19 for dataset/inria/Train/pos\\crop001162.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 59.21, 93.24, 86.13, 259.13 for dataset/inria/Train/pos\\crop001162.png\n",
      "\n",
      "0: 640x352 6 persons, 1 handbag, 1 vase, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 16.36, 94.89, 115.61, 366.99 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 0.30, 106.39, 31.08, 350.13 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 97.40, 104.04, 145.32, 299.59 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 179.96, 109.01, 212.26, 169.52 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 26.0, Confidence: 0.43, Bounding Box: 99.94, 204.87, 134.60, 259.20 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 75.0, Confidence: 0.35, Bounding Box: 152.29, 213.21, 196.94, 297.41 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 98.45, 104.55, 144.34, 237.58 for dataset/inria/Train/pos\\crop001163.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 180.39, 108.35, 212.58, 199.49 for dataset/inria/Train/pos\\crop001163.png\n",
      "\n",
      "0: 480x640 9 persons, 1 backpack, 1 handbag, 1 skis, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.00, 210.69, 108.90, 622.07 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 598.67, 225.86, 776.05, 609.49 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 396.81, 253.24, 540.38, 642.97 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 770.51, 201.72, 926.93, 589.07 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 156.90, 220.64, 246.21, 483.34 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 525.10, 246.12, 592.30, 449.31 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 230.80, 249.37, 289.29, 415.67 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 24.0, Confidence: 0.66, Bounding Box: 0.00, 287.78, 63.78, 432.73 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 112.65, 255.77, 150.17, 358.26 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 329.23, 250.99, 386.78, 464.06 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 30.0, Confidence: 0.31, Bounding Box: 665.69, 546.97, 768.06, 621.67 for dataset/inria/Train/pos\\crop001164.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 337.91, 282.74, 373.44, 348.42 for dataset/inria/Train/pos\\crop001164.png\n",
      "\n",
      "0: 640x448 1 person, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 238.57, 189.49, 437.83, 814.10 for dataset/inria/Train/pos\\crop001200.png\n",
      "\n",
      "0: 640x416 3 persons, 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 226.99, 277.28, 375.32, 673.17 for dataset/inria/Train/pos\\crop001201.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 415.19, 247.03, 626.00, 822.15 for dataset/inria/Train/pos\\crop001201.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 397.58, 269.21, 517.67, 613.13 for dataset/inria/Train/pos\\crop001201.png\n",
      "\n",
      "0: 640x416 4 persons, 15.7ms\n",
      "Speed: 2.1ms preprocess, 15.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.00, 249.87, 133.53, 657.53 for dataset/inria/Train/pos\\crop001202.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 171.20, 215.33, 412.02, 817.23 for dataset/inria/Train/pos\\crop001202.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 157.10, 240.37, 278.49, 595.96 for dataset/inria/Train/pos\\crop001202.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 375.35, 307.29, 456.42, 513.12 for dataset/inria/Train/pos\\crop001202.png\n",
      "\n",
      "0: 576x640 2 persons, 1 sports ball, 20.0ms\n",
      "Speed: 5.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 239.32, 156.68, 344.89, 415.26 for dataset/inria/Train/pos\\crop001205.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 314.19, 180.78, 443.01, 387.79 for dataset/inria/Train/pos\\crop001205.png\n",
      "Class: 32.0, Confidence: 0.54, Bounding Box: 243.09, 180.21, 270.92, 208.09 for dataset/inria/Train/pos\\crop001205.png\n",
      "\n",
      "0: 640x416 2 persons, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 359.86, 186.10, 442.00, 385.05 for dataset/inria/Train/pos\\crop001206.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 166.34, 160.38, 280.27, 539.73 for dataset/inria/Train/pos\\crop001206.png\n",
      "\n",
      "0: 640x544 2 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 157.86, 110.22, 243.49, 309.23 for dataset/inria/Train/pos\\crop001207.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.00, 83.76, 78.57, 454.00 for dataset/inria/Train/pos\\crop001207.png\n",
      "\n",
      "0: 640x576 5 persons, 16.0ms\n",
      "Speed: 5.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 181.26, 150.49, 285.91, 408.45 for dataset/inria/Train/pos\\crop001208.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 365.62, 166.71, 512.14, 618.68 for dataset/inria/Train/pos\\crop001208.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 291.10, 131.37, 352.86, 252.63 for dataset/inria/Train/pos\\crop001208.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 542.88, 145.58, 585.76, 250.02 for dataset/inria/Train/pos\\crop001208.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 360.30, 128.43, 396.31, 225.15 for dataset/inria/Train/pos\\crop001208.png\n",
      "\n",
      "0: 640x320 1 person, 32.1ms\n",
      "Speed: 2.0ms preprocess, 32.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 157.99, 155.52, 299.61, 536.84 for dataset/inria/Train/pos\\crop001210.png\n",
      "\n",
      "0: 640x480 2 persons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 222.22, 220.39, 479.77, 814.92 for dataset/inria/Train/pos\\crop001211.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 411.42, 159.03, 504.21, 453.49 for dataset/inria/Train/pos\\crop001211.png\n",
      "\n",
      "0: 640x416 3 persons, 1 frisbee, 1 sports ball, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.00, 200.59, 141.83, 808.84 for dataset/inria/Train/pos\\crop001212.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 179.34, 190.60, 393.17, 652.76 for dataset/inria/Train/pos\\crop001212.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 291.60, 187.87, 383.87, 396.57 for dataset/inria/Train/pos\\crop001212.png\n",
      "Class: 29.0, Confidence: 0.39, Bounding Box: 196.22, 158.02, 256.95, 217.05 for dataset/inria/Train/pos\\crop001212.png\n",
      "Class: 32.0, Confidence: 0.32, Bounding Box: 196.64, 158.67, 257.03, 216.66 for dataset/inria/Train/pos\\crop001212.png\n",
      "\n",
      "0: 640x416 3 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 153.44, 236.63, 423.56, 835.36 for dataset/inria/Train/pos\\crop001213.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.16, 229.34, 92.05, 471.88 for dataset/inria/Train/pos\\crop001213.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 458.65, 231.58, 599.89, 620.90 for dataset/inria/Train/pos\\crop001213.png\n",
      "\n",
      "0: 640x480 3 persons, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 219.43, 214.51, 323.36, 478.50 for dataset/inria/Train/pos\\crop001214.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 388.21, 329.18, 504.00, 683.68 for dataset/inria/Train/pos\\crop001214.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 0.00, 158.18, 73.51, 679.71 for dataset/inria/Train/pos\\crop001214.png\n",
      "\n",
      "0: 640x512 2 persons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 187.91, 105.60, 347.00, 567.52 for dataset/inria/Train/pos\\crop001216.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 459.55, 454.57, 547.74, 528.54 for dataset/inria/Train/pos\\crop001216.png\n",
      "\n",
      "0: 640x480 6 persons, 15.3ms\n",
      "Speed: 3.0ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 362.37, 151.77, 467.67, 662.37 for dataset/inria/Train/pos\\crop001217.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 47.32, 125.11, 106.05, 256.65 for dataset/inria/Train/pos\\crop001217.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 116.92, 129.87, 160.51, 269.84 for dataset/inria/Train/pos\\crop001217.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 156.41, 157.18, 316.24, 559.57 for dataset/inria/Train/pos\\crop001217.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 0.00, 120.62, 39.86, 278.38 for dataset/inria/Train/pos\\crop001217.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 264.10, 115.67, 321.86, 236.39 for dataset/inria/Train/pos\\crop001217.png\n",
      "\n",
      "0: 640x448 1 person, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 103.79, 214.72, 274.39, 584.86 for dataset/inria/Train/pos\\crop001218.png\n",
      "\n",
      "0: 640x480 2 persons, 1 sports ball, 28.0ms\n",
      "Speed: 3.0ms preprocess, 28.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 135.07, 146.52, 275.51, 453.21 for dataset/inria/Train/pos\\crop001219.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 311.64, 120.65, 429.59, 594.92 for dataset/inria/Train/pos\\crop001219.png\n",
      "Class: 32.0, Confidence: 0.48, Bounding Box: 171.30, 157.34, 218.28, 201.98 for dataset/inria/Train/pos\\crop001219.png\n",
      "\n",
      "0: 640x416 2 persons, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 231.30, 192.54, 351.55, 526.47 for dataset/inria/Train/pos\\crop001220.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.71, 265.83, 180.08, 690.15 for dataset/inria/Train/pos\\crop001220.png\n",
      "\n",
      "0: 640x544 6 persons, 19.0ms\n",
      "Speed: 5.5ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 148.21, 131.15, 247.50, 323.77 for dataset/inria/Train/pos\\crop001222.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 236.70, 161.91, 421.34, 742.40 for dataset/inria/Train/pos\\crop001222.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 403.70, 213.38, 640.94, 764.09 for dataset/inria/Train/pos\\crop001222.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 374.61, 113.95, 444.50, 253.22 for dataset/inria/Train/pos\\crop001222.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 362.42, 124.64, 395.74, 203.23 for dataset/inria/Train/pos\\crop001222.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 362.32, 125.45, 394.94, 238.68 for dataset/inria/Train/pos\\crop001222.png\n",
      "\n",
      "0: 640x512 3 persons, 1 sports ball, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 431.68, 436.26, 635.97, 758.58 for dataset/inria/Train/pos\\crop001230.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 423.73, 229.78, 533.72, 521.19 for dataset/inria/Train/pos\\crop001230.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 214.09, 186.62, 376.22, 625.86 for dataset/inria/Train/pos\\crop001230.png\n",
      "Class: 32.0, Confidence: 0.89, Bounding Box: 352.63, 110.71, 422.60, 179.82 for dataset/inria/Train/pos\\crop001230.png\n",
      "\n",
      "0: 640x512 2 persons, 1 sports ball, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 190.30, 170.61, 354.35, 458.89 for dataset/inria/Train/pos\\crop001231.png\n",
      "Class: 32.0, Confidence: 0.89, Bounding Box: 341.09, 180.79, 401.87, 238.41 for dataset/inria/Train/pos\\crop001231.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 496.06, 281.59, 519.32, 396.21 for dataset/inria/Train/pos\\crop001231.png\n",
      "\n",
      "0: 640x480 4 persons, 1 sports ball, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 32.0, Confidence: 0.96, Bounding Box: 51.07, 306.78, 112.06, 365.59 for dataset/inria/Train/pos\\crop001232.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 209.47, 170.47, 509.64, 843.31 for dataset/inria/Train/pos\\crop001232.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 508.65, 128.83, 667.71, 930.00 for dataset/inria/Train/pos\\crop001232.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.92, 298.28, 62.33, 543.01 for dataset/inria/Train/pos\\crop001232.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 207.85, 164.77, 323.78, 517.98 for dataset/inria/Train/pos\\crop001232.png\n",
      "\n",
      "0: 640x448 1 person, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 182.54, 171.90, 332.24, 533.18 for dataset/inria/Train/pos\\crop001234.png\n",
      "\n",
      "0: 640x512 2 persons, 17.0ms\n",
      "Speed: 10.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 161.95, 189.18, 452.89, 726.80 for dataset/inria/Train/pos\\crop001235.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 157.26, 169.23, 287.48, 652.69 for dataset/inria/Train/pos\\crop001235.png\n",
      "\n",
      "0: 544x640 7 persons, 2 sports balls, 18.5ms\n",
      "Speed: 3.0ms preprocess, 18.5ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 328.50, 161.94, 476.02, 531.82 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 474.87, 169.31, 602.94, 473.17 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 136.51, 164.41, 234.76, 470.41 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 630.36, 183.36, 747.71, 624.39 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 32.0, Confidence: 0.85, Bounding Box: 452.15, 191.34, 496.36, 233.68 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 209.82, 169.45, 247.95, 296.51 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 338.73, 174.04, 373.36, 287.87 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 32.0, Confidence: 0.50, Bounding Box: 323.73, 274.07, 338.36, 287.19 for dataset/inria/Train/pos\\crop001236.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 358.71, 183.79, 402.06, 237.95 for dataset/inria/Train/pos\\crop001236.png\n",
      "\n",
      "0: 640x512 3 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 195.51, 137.98, 297.06, 446.28 for dataset/inria/Train/pos\\crop001237.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.35, 79.85, 108.11, 610.00 for dataset/inria/Train/pos\\crop001237.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 268.13, 168.76, 304.39, 232.23 for dataset/inria/Train/pos\\crop001237.png\n",
      "\n",
      "0: 640x448 4 persons, 1 bench, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.00, 137.83, 132.58, 607.71 for dataset/inria/Train/pos\\crop001238.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 168.07, 139.52, 272.40, 475.14 for dataset/inria/Train/pos\\crop001238.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 326.68, 137.02, 404.55, 538.84 for dataset/inria/Train/pos\\crop001238.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 353.25, 133.41, 405.00, 541.42 for dataset/inria/Train/pos\\crop001238.png\n",
      "Class: 13.0, Confidence: 0.43, Bounding Box: 300.82, 255.10, 374.96, 297.59 for dataset/inria/Train/pos\\crop001238.png\n",
      "\n",
      "0: 640x544 2 persons, 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 222.40, 158.19, 332.74, 483.08 for dataset/inria/Train/pos\\crop001241.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.20, 265.22, 149.15, 577.81 for dataset/inria/Train/pos\\crop001241.png\n",
      "\n",
      "0: 640x448 3 persons, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 441.79, 227.81, 535.54, 532.93 for dataset/inria/Train/pos\\crop001242.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.56, 210.04, 86.92, 536.80 for dataset/inria/Train/pos\\crop001242.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 223.71, 167.53, 416.66, 796.49 for dataset/inria/Train/pos\\crop001242.png\n",
      "\n",
      "0: 640x288 3 persons, 14.5ms\n",
      "Speed: 2.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 148.84, 175.09, 280.46, 528.25 for dataset/inria/Train/pos\\crop001246.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 230.01, 146.98, 314.00, 537.45 for dataset/inria/Train/pos\\crop001246.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 264.92, 162.01, 314.00, 544.92 for dataset/inria/Train/pos\\crop001246.png\n",
      "\n",
      "0: 640x448 3 persons, 1 frisbee, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.26, 196.72, 193.21, 621.75 for dataset/inria/Train/pos\\crop001247.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 189.50, 164.67, 390.39, 714.44 for dataset/inria/Train/pos\\crop001247.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 186.43, 204.40, 286.85, 518.07 for dataset/inria/Train/pos\\crop001247.png\n",
      "Class: 29.0, Confidence: 0.29, Bounding Box: 87.23, 254.61, 124.65, 287.55 for dataset/inria/Train/pos\\crop001247.png\n",
      "\n",
      "0: 640x480 2 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 209.43, 187.44, 498.60, 736.48 for dataset/inria/Train/pos\\crop001248.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 531.36, 308.33, 651.23, 906.61 for dataset/inria/Train/pos\\crop001248.png\n",
      "\n",
      "0: 640x576 2 persons, 17.0ms\n",
      "Speed: 5.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 282.16, 114.92, 546.60, 808.02 for dataset/inria/Train/pos\\crop001250.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 0.00, 186.93, 50.05, 545.32 for dataset/inria/Train/pos\\crop001250.png\n",
      "\n",
      "0: 640x448 3 persons, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 432.55, 123.17, 528.74, 463.80 for dataset/inria/Train/pos\\crop001251.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 134.33, 150.24, 253.39, 479.53 for dataset/inria/Train/pos\\crop001251.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 203.05, 80.92, 506.64, 853.86 for dataset/inria/Train/pos\\crop001251.png\n",
      "\n",
      "0: 640x544 3 persons, 104.5ms\n",
      "Speed: 4.0ms preprocess, 104.5ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 159.03, 128.29, 335.83, 490.24 for dataset/inria/Train/pos\\crop001252.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 430.83, 74.01, 510.41, 604.87 for dataset/inria/Train/pos\\crop001252.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 285.01, 247.91, 350.81, 317.88 for dataset/inria/Train/pos\\crop001252.png\n",
      "\n",
      "0: 640x416 4 persons, 22.5ms\n",
      "Speed: 13.4ms preprocess, 22.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 224.13, 164.25, 400.47, 847.58 for dataset/inria/Train/pos\\crop001253.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 149.62, 197.33, 238.45, 516.12 for dataset/inria/Train/pos\\crop001253.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.00, 129.54, 57.42, 767.31 for dataset/inria/Train/pos\\crop001253.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 571.34, 252.38, 634.76, 556.94 for dataset/inria/Train/pos\\crop001253.png\n",
      "\n",
      "0: 640x288 5 persons, 23.4ms\n",
      "Speed: 1.0ms preprocess, 23.4ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 44.31, 134.55, 193.20, 775.87 for dataset/inria/Train/pos\\crop001254.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 284.18, 205.80, 373.30, 520.69 for dataset/inria/Train/pos\\crop001254.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 1.47, 211.76, 58.49, 577.55 for dataset/inria/Train/pos\\crop001254.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 342.87, 193.33, 417.00, 574.73 for dataset/inria/Train/pos\\crop001254.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 341.89, 190.69, 416.51, 762.30 for dataset/inria/Train/pos\\crop001254.png\n",
      "\n",
      "0: 640x384 2 persons, 21.1ms\n",
      "Speed: 2.0ms preprocess, 21.1ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 213.87, 171.82, 353.50, 551.50 for dataset/inria/Train/pos\\crop001255.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.94, 122.92, 233.51, 749.26 for dataset/inria/Train/pos\\crop001255.png\n",
      "\n",
      "0: 640x544 4 persons, 24.0ms\n",
      "Speed: 9.0ms preprocess, 24.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 208.23, 142.14, 331.20, 481.14 for dataset/inria/Train/pos\\crop001256.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 380.65, 85.53, 505.93, 625.69 for dataset/inria/Train/pos\\crop001256.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 325.03, 96.14, 443.47, 627.00 for dataset/inria/Train/pos\\crop001256.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 136.36, 238.86, 211.16, 335.38 for dataset/inria/Train/pos\\crop001256.png\n",
      "\n",
      "0: 640x576 2 persons, 21.0ms\n",
      "Speed: 4.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 187.48, 142.97, 307.09, 458.42 for dataset/inria/Train/pos\\crop001257.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 322.26, 116.28, 533.25, 618.00 for dataset/inria/Train/pos\\crop001257.png\n",
      "\n",
      "0: 640x512 3 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 5.63, 89.46, 224.97, 651.19 for dataset/inria/Train/pos\\crop001258.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 235.75, 140.76, 340.14, 508.35 for dataset/inria/Train/pos\\crop001258.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 113.95, 3.61, 274.59, 643.48 for dataset/inria/Train/pos\\crop001258.png\n",
      "\n",
      "0: 640x416 1 person, 1 sports ball, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 166.23, 134.60, 376.50, 644.78 for dataset/inria/Train/pos\\crop001259.png\n",
      "Class: 32.0, Confidence: 0.90, Bounding Box: 353.25, 490.08, 416.31, 550.93 for dataset/inria/Train/pos\\crop001259.png\n",
      "\n",
      "0: 640x512 6 persons, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 45.61, 186.77, 114.57, 385.29 for dataset/inria/Train/pos\\crop001260.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 187.29, 186.93, 365.46, 633.35 for dataset/inria/Train/pos\\crop001260.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 271.78, 188.34, 405.37, 599.57 for dataset/inria/Train/pos\\crop001260.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 105.36, 205.71, 158.10, 369.62 for dataset/inria/Train/pos\\crop001260.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 120.71, 211.25, 162.24, 370.36 for dataset/inria/Train/pos\\crop001260.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 130.30, 212.82, 162.75, 352.00 for dataset/inria/Train/pos\\crop001260.png\n",
      "\n",
      "0: 640x480 8 persons, 1 frisbee, 21.5ms\n",
      "Speed: 3.0ms preprocess, 21.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 130.57, 143.20, 302.89, 644.08 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 116.75, 120.22, 175.80, 269.72 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 198.59, 126.44, 241.56, 214.67 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 256.15, 124.96, 335.23, 340.52 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 481.41, 128.42, 565.45, 382.05 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 0.00, 96.81, 87.50, 338.28 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 468.43, 134.30, 516.32, 293.75 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 476.32, 134.68, 526.65, 295.87 for dataset/inria/Train/pos\\crop001263.png\n",
      "Class: 29.0, Confidence: 0.34, Bounding Box: 90.52, 201.45, 113.54, 221.72 for dataset/inria/Train/pos\\crop001263.png\n",
      "\n",
      "0: 640x480 3 persons, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 123.53, 161.93, 315.03, 697.30 for dataset/inria/Train/pos\\crop001264.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 286.78, 125.56, 486.63, 796.18 for dataset/inria/Train/pos\\crop001264.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 651.92, 255.01, 715.00, 601.93 for dataset/inria/Train/pos\\crop001264.png\n",
      "\n",
      "0: 480x640 8 persons, 13.3ms\n",
      "Speed: 2.6ms preprocess, 13.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 0.00, 236.56, 180.25, 417.73 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 381.20, 113.84, 441.63, 294.92 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 245.63, 115.23, 328.89, 344.67 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 172.37, 123.15, 225.39, 262.16 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 224.75, 101.61, 275.03, 287.65 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 520.61, 114.65, 571.82, 294.91 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 502.54, 127.10, 538.95, 249.66 for dataset/inria/Train/pos\\crop001265.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 549.52, 117.63, 571.87, 296.78 for dataset/inria/Train/pos\\crop001265.png\n",
      "\n",
      "0: 480x640 7 persons, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 183.28, 140.21, 234.76, 338.10 for dataset/inria/Train/pos\\crop001267.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 345.58, 149.11, 406.02, 340.11 for dataset/inria/Train/pos\\crop001267.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 498.10, 169.67, 570.89, 338.05 for dataset/inria/Train/pos\\crop001267.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 113.56, 167.47, 160.50, 319.75 for dataset/inria/Train/pos\\crop001267.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 230.17, 158.34, 282.94, 293.51 for dataset/inria/Train/pos\\crop001267.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 457.75, 156.24, 524.31, 288.13 for dataset/inria/Train/pos\\crop001267.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 274.12, 204.52, 314.18, 252.96 for dataset/inria/Train/pos\\crop001267.png\n",
      "\n",
      "0: 448x640 8 persons, 1 car, 12.7ms\n",
      "Speed: 1.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 395.00, 126.86, 471.43, 307.72 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 329.73, 125.66, 392.30, 271.25 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.20, 147.03, 71.42, 314.44 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 128.94, 121.48, 179.43, 264.93 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 207.02, 126.23, 289.91, 315.63 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 491.63, 141.10, 526.21, 287.96 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 450.13, 127.11, 501.57, 279.04 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 198.50, 126.36, 233.50, 285.33 for dataset/inria/Train/pos\\crop001268.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 293.94, 193.75, 332.54, 211.57 for dataset/inria/Train/pos\\crop001268.png\n",
      "\n",
      "0: 640x640 5 persons, 1 sports ball, 12.2ms\n",
      "Speed: 3.1ms preprocess, 12.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 32.0, Confidence: 0.96, Bounding Box: 437.44, 376.10, 477.67, 416.48 for dataset/inria/Train/pos\\crop001270.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 290.37, 215.87, 460.59, 517.54 for dataset/inria/Train/pos\\crop001270.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 201.95, 159.83, 330.24, 401.17 for dataset/inria/Train/pos\\crop001270.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 0.00, 205.95, 35.81, 456.04 for dataset/inria/Train/pos\\crop001270.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 291.03, 157.46, 361.62, 370.52 for dataset/inria/Train/pos\\crop001270.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 297.57, 157.61, 361.20, 271.00 for dataset/inria/Train/pos\\crop001270.png\n",
      "\n",
      "0: 640x608 4 persons, 11.4ms\n",
      "Speed: 3.0ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 185.03, 86.49, 271.08, 288.70 for dataset/inria/Train/pos\\crop001271.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 137.73, 107.67, 182.34, 259.17 for dataset/inria/Train/pos\\crop001271.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 346.73, 98.69, 379.00, 296.58 for dataset/inria/Train/pos\\crop001271.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 274.41, 154.34, 313.64, 205.27 for dataset/inria/Train/pos\\crop001271.png\n",
      "\n",
      "0: 640x640 1 person, 16.5ms\n",
      "Speed: 3.0ms preprocess, 16.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 152.72, 101.74, 261.59, 344.24 for dataset/inria/Train/pos\\crop001272.png\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 19.3ms\n",
      "Speed: 2.0ms preprocess, 19.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 581.78, 128.46, 704.36, 478.24 for dataset/inria/Train/pos\\crop001273.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 226.04, 115.40, 353.40, 423.46 for dataset/inria/Train/pos\\crop001273.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 559.38, 117.30, 630.12, 345.87 for dataset/inria/Train/pos\\crop001273.png\n",
      "Class: 56.0, Confidence: 0.26, Bounding Box: 403.38, 293.26, 576.31, 356.07 for dataset/inria/Train/pos\\crop001273.png\n",
      "\n",
      "0: 640x416 2 persons, 13.5ms\n",
      "Speed: 1.0ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 374.42, 228.69, 465.71, 575.96 for dataset/inria/Train/pos\\crop001274.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 181.18, 120.74, 326.18, 616.71 for dataset/inria/Train/pos\\crop001274.png\n",
      "\n",
      "0: 640x416 2 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 172.61, 184.76, 331.04, 529.82 for dataset/inria/Train/pos\\crop001275.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 0.00, 73.86, 121.85, 580.14 for dataset/inria/Train/pos\\crop001275.png\n",
      "\n",
      "0: 640x416 5 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 197.85, 166.77, 437.13, 864.91 for dataset/inria/Train/pos\\crop001276.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 530.49, 273.72, 586.00, 461.85 for dataset/inria/Train/pos\\crop001276.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 392.13, 226.71, 465.98, 536.88 for dataset/inria/Train/pos\\crop001276.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 120.54, 243.72, 260.94, 620.23 for dataset/inria/Train/pos\\crop001276.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 118.78, 282.64, 192.58, 547.80 for dataset/inria/Train/pos\\crop001276.png\n",
      "\n",
      "0: 640x416 3 persons, 9.5ms\n",
      "Speed: 1.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 40.11, 120.50, 100.21, 253.85 for dataset/inria/Train/pos\\crop001278.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 152.62, 111.77, 229.36, 370.65 for dataset/inria/Train/pos\\crop001278.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 189.50, 116.53, 229.25, 371.20 for dataset/inria/Train/pos\\crop001278.png\n",
      "\n",
      "0: 640x480 1 person, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 108.12, 110.20, 149.16, 231.18 for dataset/inria/Train/pos\\crop001500.png\n",
      "\n",
      "0: 544x640 1 person, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 187.20, 138.40, 268.08, 264.09 for dataset/inria/Train/pos\\crop001503.png\n",
      "\n",
      "0: 640x640 13 persons, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 770.05, 328.36, 859.11, 611.58 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.40, 320.80, 132.20, 714.97 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.23, 681.59, 102.19, 952.67 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 566.66, 260.16, 690.52, 812.11 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 471.08, 340.69, 598.49, 803.20 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 273.25, 346.77, 428.08, 745.70 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 357.79, 342.23, 488.38, 735.27 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 151.09, 336.56, 226.61, 545.62 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 684.32, 311.52, 731.79, 464.95 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 150.46, 338.32, 227.18, 645.95 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 229.51, 322.81, 354.17, 647.92 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 424.20, 322.22, 474.69, 444.62 for dataset/inria/Train/pos\\crop001505.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 104.08, 313.35, 154.14, 607.31 for dataset/inria/Train/pos\\crop001505.png\n",
      "\n",
      "0: 640x448 11 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 139.00, 279.97, 232.89, 575.15 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 45.17, 585.93, 340.92, 835.59 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 212.05, 188.21, 366.76, 628.90 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 391.24, 203.16, 474.57, 445.77 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 72.70, 189.18, 178.50, 517.85 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 1.94, 513.03, 199.71, 834.48 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 482.01, 197.15, 577.00, 551.37 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 313.07, 189.32, 387.43, 627.75 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 339.11, 180.11, 391.30, 486.05 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 458.82, 401.72, 508.72, 557.08 for dataset/inria/Train/pos\\crop001506.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 409.93, 399.16, 510.38, 557.90 for dataset/inria/Train/pos\\crop001506.png\n",
      "\n",
      "0: 640x480 6 persons, 1 umbrella, 1 skateboard, 13.0ms\n",
      "Speed: 4.5ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 394.23, 300.02, 559.94, 771.69 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 4.83, 282.76, 107.67, 623.67 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 69.05, 277.44, 159.01, 584.92 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 189.88, 288.22, 394.47, 750.53 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 354.01, 254.26, 480.73, 693.88 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 36.0, Confidence: 0.57, Bounding Box: 392.95, 721.72, 530.83, 777.47 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 25.0, Confidence: 0.38, Bounding Box: 132.68, 16.64, 455.17, 314.26 for dataset/inria/Train/pos\\crop001507.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 184.55, 285.26, 275.03, 533.98 for dataset/inria/Train/pos\\crop001507.png\n",
      "\n",
      "0: 640x608 11 persons, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 390.99, 262.33, 552.65, 713.36 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 498.98, 251.59, 614.83, 657.38 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 655.62, 269.32, 834.92, 881.40 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 2.81, 275.73, 138.36, 878.04 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 294.19, 262.70, 429.74, 683.21 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 108.68, 408.52, 242.80, 658.49 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 152.85, 286.96, 237.45, 418.61 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 640.16, 263.88, 761.60, 605.12 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 238.35, 255.11, 330.71, 573.83 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 52.06, 269.63, 146.90, 454.00 for dataset/inria/Train/pos\\crop001508.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 115.50, 334.37, 207.79, 471.31 for dataset/inria/Train/pos\\crop001508.png\n",
      "\n",
      "0: 640x576 7 persons, 2 handbags, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 382.14, 277.67, 528.65, 746.84 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 264.87, 259.97, 387.01, 725.59 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 477.06, 418.25, 658.25, 755.53 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 745.60, 244.44, 842.70, 802.68 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 15.97, 269.24, 144.28, 419.17 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 196.22, 312.04, 283.39, 649.85 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 1.44, 428.48, 208.61, 958.44 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 363.42, 488.53, 399.93, 547.00 for dataset/inria/Train/pos\\crop001509.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 363.57, 459.36, 403.69, 547.21 for dataset/inria/Train/pos\\crop001509.png\n",
      "\n",
      "0: 640x448 5 persons, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 201.96, 214.36, 406.42, 779.34 for dataset/inria/Train/pos\\crop001510.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 0.46, 380.34, 110.16, 702.05 for dataset/inria/Train/pos\\crop001510.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 506.67, 339.20, 642.06, 843.49 for dataset/inria/Train/pos\\crop001510.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 429.37, 371.45, 598.64, 797.77 for dataset/inria/Train/pos\\crop001510.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 449.68, 351.10, 643.09, 844.32 for dataset/inria/Train/pos\\crop001510.png\n",
      "\n",
      "0: 512x640 6 persons, 2 cars, 1 bus, 2 trucks, 1 stop sign, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 252.73, 214.73, 398.16, 568.53 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 636.85, 210.66, 772.79, 527.63 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 0.00, 346.10, 164.32, 526.27 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 200.37, 175.50, 295.33, 354.29 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 749.72, 210.70, 791.10, 330.88 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 718.38, 208.62, 762.99, 286.47 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 2.0, Confidence: 0.59, Bounding Box: 0.92, 276.30, 260.15, 515.64 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 11.0, Confidence: 0.55, Bounding Box: 28.60, 21.58, 61.02, 96.84 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 7.0, Confidence: 0.44, Bounding Box: 331.71, 12.76, 651.25, 458.18 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 159.89, 105.42, 226.88, 200.04 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 7.0, Confidence: 0.32, Bounding Box: 127.33, 12.84, 650.22, 470.04 for dataset/inria/Train/pos\\crop001513.png\n",
      "Class: 5.0, Confidence: 0.27, Bounding Box: 67.87, 12.32, 651.79, 460.62 for dataset/inria/Train/pos\\crop001513.png\n",
      "\n",
      "0: 640x576 7 persons, 4 cars, 1 handbag, 22.0ms\n",
      "Speed: 4.0ms preprocess, 22.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 1.16, 179.92, 164.34, 658.23 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 107.50, 206.73, 196.37, 382.81 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 247.85, 225.95, 384.32, 501.85 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 397.40, 221.26, 459.01, 302.26 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 268.60, 207.10, 322.07, 280.86 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 198.79, 206.71, 271.21, 407.70 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 341.26, 209.34, 387.69, 268.62 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 2.0, Confidence: 0.43, Bounding Box: 195.21, 201.49, 227.64, 223.30 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 2.0, Confidence: 0.41, Bounding Box: 52.16, 200.02, 115.57, 260.24 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 124.31, 234.54, 177.71, 335.86 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 374.98, 222.64, 563.40, 517.69 for dataset/inria/Train/pos\\crop001515.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 172.55, 211.10, 212.85, 246.28 for dataset/inria/Train/pos\\crop001515.png\n",
      "\n",
      "0: 640x512 11 persons, 1 truck, 1 umbrella, 24.5ms\n",
      "Speed: 2.0ms preprocess, 24.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 348.59, 213.10, 529.59, 800.36 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 75.94, 254.71, 477.83, 959.53 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 486.54, 234.92, 556.60, 372.43 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 617.26, 240.58, 708.12, 467.33 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 197.76, 218.16, 215.84, 278.18 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 0.00, 240.42, 21.47, 476.24 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 556.14, 229.19, 613.34, 410.77 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 277.14, 224.78, 297.32, 260.86 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 596.13, 240.33, 661.48, 457.14 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 612.01, 235.73, 654.98, 290.74 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 25.0, Confidence: 0.33, Bounding Box: 0.00, 136.21, 153.60, 181.29 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 707.07, 243.87, 761.52, 310.92 for dataset/inria/Train/pos\\crop001516.png\n",
      "Class: 7.0, Confidence: 0.25, Bounding Box: 457.30, 208.89, 524.98, 273.42 for dataset/inria/Train/pos\\crop001516.png\n",
      "\n",
      "0: 640x448 10 persons, 1 handbag, 14.0ms\n",
      "Speed: 5.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 187.05, 154.33, 441.67, 812.67 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 515.81, 282.60, 625.41, 689.93 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 458.81, 271.37, 573.10, 657.69 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 0.00, 240.17, 56.48, 463.39 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 76.53, 223.59, 156.74, 475.66 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 55.33, 239.89, 107.27, 467.76 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 148.41, 212.69, 249.27, 609.66 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 26.0, Confidence: 0.43, Bounding Box: 126.98, 458.31, 199.91, 567.57 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 25.56, 210.92, 78.18, 463.25 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 450.69, 258.16, 529.94, 634.74 for dataset/inria/Train/pos\\crop001517.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 455.84, 249.60, 532.85, 394.18 for dataset/inria/Train/pos\\crop001517.png\n",
      "\n",
      "0: 640x512 10 persons, 1 parking meter, 1 tie, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 446.94, 157.56, 585.00, 742.00 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 307.88, 223.93, 407.48, 554.62 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 148.26, 262.23, 222.98, 545.08 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 203.36, 241.90, 274.08, 536.86 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 149.54, 136.06, 206.27, 282.39 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 273.86, 237.25, 344.84, 552.65 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 395.59, 202.17, 498.72, 552.88 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 27.0, Confidence: 0.47, Bounding Box: 270.97, 300.26, 296.12, 433.03 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 394.97, 201.15, 502.35, 727.06 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 12.0, Confidence: 0.42, Bounding Box: 7.82, 45.03, 173.81, 663.79 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 478.24, 242.33, 537.72, 356.64 for dataset/inria/Train/pos\\crop001518.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 242.01, 205.43, 306.55, 316.65 for dataset/inria/Train/pos\\crop001518.png\n",
      "\n",
      "0: 640x480 6 persons, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 214.83, 221.14, 453.13, 769.92 for dataset/inria/Train/pos\\crop001519.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 151.22, 208.60, 297.43, 723.43 for dataset/inria/Train/pos\\crop001519.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 1.29, 162.72, 122.96, 735.65 for dataset/inria/Train/pos\\crop001519.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 24.47, 370.71, 128.36, 717.33 for dataset/inria/Train/pos\\crop001519.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 133.57, 179.58, 238.92, 390.86 for dataset/inria/Train/pos\\crop001519.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 93.96, 194.17, 214.26, 681.17 for dataset/inria/Train/pos\\crop001519.png\n",
      "\n",
      "0: 544x640 3 persons, 2 cars, 2 trucks, 1 traffic light, 11.3ms\n",
      "Speed: 2.0ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 0.16, 155.55, 107.37, 426.21 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 99.54, 140.33, 155.35, 293.87 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 2.0, Confidence: 0.58, Bounding Box: 278.34, 171.05, 398.98, 219.34 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 7.0, Confidence: 0.41, Bounding Box: 46.27, 127.05, 126.66, 180.79 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 379.05, 158.49, 521.48, 221.24 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 9.0, Confidence: 0.35, Bounding Box: 477.70, 104.26, 491.42, 134.36 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 7.0, Confidence: 0.32, Bounding Box: 377.91, 158.85, 521.61, 221.36 for dataset/inria/Train/pos\\crop001523.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 105.06, 150.63, 116.33, 176.80 for dataset/inria/Train/pos\\crop001523.png\n",
      "\n",
      "0: 608x640 3 persons, 2 cars, 1 bench, 1 handbag, 1 sports ball, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 565.12, 201.42, 752.52, 758.28 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 744.63, 190.47, 797.55, 346.64 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 13.0, Confidence: 0.80, Bounding Box: 192.65, 243.85, 340.66, 304.70 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 2.0, Confidence: 0.70, Bounding Box: 0.00, 188.29, 61.42, 216.91 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 2.0, Confidence: 0.66, Bounding Box: 898.90, 220.01, 1002.42, 278.43 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 595.97, 189.74, 615.23, 242.92 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 680.16, 516.00, 752.32, 598.09 for dataset/inria/Train/pos\\crop001524.png\n",
      "Class: 32.0, Confidence: 0.28, Bounding Box: 302.15, 0.45, 352.44, 36.15 for dataset/inria/Train/pos\\crop001524.png\n",
      "\n",
      "0: 640x640 4 persons, 1 truck, 1 backpack, 2 handbags, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 445.44, 223.09, 515.81, 407.14 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 298.17, 216.79, 412.72, 571.35 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 216.77, 222.97, 305.96, 489.78 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 636.77, 240.98, 765.00, 770.38 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 26.0, Confidence: 0.57, Bounding Box: 631.39, 452.39, 688.59, 542.10 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 7.0, Confidence: 0.56, Bounding Box: 148.91, 175.36, 329.38, 329.11 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 240.94, 357.94, 303.20, 451.95 for dataset/inria/Train/pos\\crop001525.png\n",
      "Class: 24.0, Confidence: 0.29, Bounding Box: 443.63, 255.81, 474.08, 320.99 for dataset/inria/Train/pos\\crop001525.png\n",
      "\n",
      "0: 608x640 7 persons, 1 truck, 2 backpacks, 3 handbags, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 279.92, 224.98, 478.80, 770.68 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 467.07, 258.79, 660.82, 770.06 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 96.15, 208.61, 162.14, 386.46 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 0.00, 199.26, 59.64, 546.05 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 454.25, 207.13, 535.30, 389.02 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 7.0, Confidence: 0.45, Bounding Box: 394.23, 17.07, 1048.00, 690.17 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 326.19, 202.42, 378.71, 301.52 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 24.0, Confidence: 0.36, Bounding Box: 93.12, 243.16, 125.74, 300.33 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 26.0, Confidence: 0.36, Bounding Box: 453.85, 436.68, 541.10, 533.91 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 24.0, Confidence: 0.33, Bounding Box: 1.79, 299.77, 63.53, 398.36 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 93.20, 241.47, 124.49, 299.03 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 327.57, 182.68, 390.73, 302.76 for dataset/inria/Train/pos\\crop001526.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 273.30, 426.35, 330.77, 519.08 for dataset/inria/Train/pos\\crop001526.png\n",
      "\n",
      "0: 608x640 5 persons, 1 car, 1 handbag, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 730.80, 300.80, 818.72, 546.15 for dataset/inria/Train/pos\\crop001527.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 346.77, 292.32, 417.50, 485.58 for dataset/inria/Train/pos\\crop001527.png\n",
      "Class: 2.0, Confidence: 0.85, Bounding Box: 0.42, 334.29, 127.78, 596.61 for dataset/inria/Train/pos\\crop001527.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 466.11, 316.84, 580.97, 616.44 for dataset/inria/Train/pos\\crop001527.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 222.85, 278.99, 345.65, 612.39 for dataset/inria/Train/pos\\crop001527.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 844.01, 395.33, 894.85, 746.10 for dataset/inria/Train/pos\\crop001527.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 317.22, 370.75, 357.98, 421.54 for dataset/inria/Train/pos\\crop001527.png\n",
      "\n",
      "0: 640x480 2 persons, 1 bus, 9.5ms\n",
      "Speed: 2.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 241.40, 248.78, 406.85, 760.69 for dataset/inria/Train/pos\\crop001528.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 142.27, 250.78, 219.12, 459.99 for dataset/inria/Train/pos\\crop001528.png\n",
      "Class: 5.0, Confidence: 0.59, Bounding Box: 408.81, 26.26, 690.00, 772.93 for dataset/inria/Train/pos\\crop001528.png\n",
      "\n",
      "0: 640x640 9 persons, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 351.41, 191.55, 533.33, 634.26 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 488.96, 184.75, 579.03, 444.60 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 574.77, 180.02, 647.10, 374.97 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 697.65, 192.23, 778.16, 387.18 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 662.36, 194.48, 711.94, 377.03 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 793.93, 214.01, 843.56, 336.70 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 774.52, 187.56, 818.69, 350.80 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 638.08, 191.15, 676.30, 373.86 for dataset/inria/Train/pos\\crop001529.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 497.54, 153.94, 540.97, 220.29 for dataset/inria/Train/pos\\crop001529.png\n",
      "\n",
      "0: 480x640 14 persons, 1 bus, 2 traffic lights, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 31.51, 168.98, 214.10, 525.34 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 167.55, 162.06, 258.18, 422.44 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 5.0, Confidence: 0.84, Bounding Box: 494.27, 70.73, 687.52, 277.31 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 473.39, 193.06, 541.55, 300.14 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 454.70, 164.19, 508.20, 326.11 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 271.81, 161.35, 328.25, 353.09 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 255.79, 156.28, 304.16, 347.11 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 381.69, 168.67, 443.95, 359.05 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 556.92, 142.41, 589.56, 202.88 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 340.51, 169.53, 391.11, 357.24 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 233.72, 158.54, 274.46, 344.72 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 9.0, Confidence: 0.32, Bounding Box: 192.05, 83.16, 220.60, 113.09 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 177.25, 131.53, 221.71, 198.46 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 9.0, Confidence: 0.29, Bounding Box: 191.89, 81.50, 211.51, 112.94 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 296.85, 169.68, 331.21, 341.18 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 286.76, 166.21, 326.31, 351.58 for dataset/inria/Train/pos\\crop001530.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 355.78, 173.49, 395.52, 359.77 for dataset/inria/Train/pos\\crop001530.png\n",
      "\n",
      "0: 640x640 2 persons, 19.0ms\n",
      "Speed: 4.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 490.20, 284.68, 688.54, 795.76 for dataset/inria/Train/pos\\crop001532.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 297.65, 305.33, 454.36, 730.94 for dataset/inria/Train/pos\\crop001532.png\n",
      "\n",
      "0: 608x640 12 persons, 2 handbags, 1 suitcase, 19.4ms\n",
      "Speed: 3.5ms preprocess, 19.4ms inference, 2.8ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 846.38, 274.89, 930.42, 462.11 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 269.59, 278.89, 495.86, 762.74 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 440.22, 263.06, 527.40, 457.37 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 639.90, 257.37, 710.61, 476.81 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 172.56, 295.32, 291.23, 462.09 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 28.0, Confidence: 0.63, Bounding Box: 687.06, 385.92, 727.87, 476.07 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 238.17, 262.55, 377.41, 668.48 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 273.03, 263.92, 378.86, 435.17 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 611.70, 269.25, 661.59, 347.79 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 167.44, 292.39, 305.44, 658.36 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 568.05, 285.91, 614.93, 347.02 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 565.59, 285.68, 616.14, 405.50 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 909.17, 382.07, 936.80, 424.98 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 31.85, 321.94, 82.89, 358.90 for dataset/inria/Train/pos\\crop001534.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 422.01, 371.23, 492.89, 492.49 for dataset/inria/Train/pos\\crop001534.png\n",
      "\n",
      "0: 608x640 6 persons, 1 backpack, 1 handbag, 17.6ms\n",
      "Speed: 4.0ms preprocess, 17.6ms inference, 3.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 127.96, 317.74, 233.26, 569.87 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 239.66, 316.70, 319.73, 515.73 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 376.17, 320.72, 438.05, 465.68 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 0.07, 306.24, 163.83, 789.80 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 588.33, 322.27, 634.40, 456.37 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 683.41, 325.83, 719.18, 420.77 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 26.0, Confidence: 0.52, Bounding Box: 419.35, 402.71, 451.39, 445.70 for dataset/inria/Train/pos\\crop001536.png\n",
      "Class: 24.0, Confidence: 0.30, Bounding Box: 401.43, 349.20, 437.79, 412.97 for dataset/inria/Train/pos\\crop001536.png\n",
      "\n",
      "0: 512x640 5 persons, 1 backpack, 1 handbag, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 76.46, 221.91, 155.21, 419.42 for dataset/inria/Train/pos\\crop001537.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 212.60, 222.87, 275.96, 369.07 for dataset/inria/Train/pos\\crop001537.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.00, 224.52, 67.63, 471.49 for dataset/inria/Train/pos\\crop001537.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 515.50, 229.85, 554.87, 326.72 for dataset/inria/Train/pos\\crop001537.png\n",
      "Class: 26.0, Confidence: 0.56, Bounding Box: 255.72, 307.93, 287.43, 347.67 for dataset/inria/Train/pos\\crop001537.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 424.22, 225.44, 470.69, 359.15 for dataset/inria/Train/pos\\crop001537.png\n",
      "Class: 24.0, Confidence: 0.46, Bounding Box: 436.78, 234.03, 465.77, 280.77 for dataset/inria/Train/pos\\crop001537.png\n",
      "\n",
      "0: 640x608 6 persons, 1 tv, 19.0ms\n",
      "Speed: 5.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 470.21, 175.89, 566.41, 386.20 for dataset/inria/Train/pos\\crop001538.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 18.85, 237.96, 70.72, 282.90 for dataset/inria/Train/pos\\crop001538.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 611.57, 190.85, 712.97, 750.21 for dataset/inria/Train/pos\\crop001538.png\n",
      "Class: 62.0, Confidence: 0.54, Bounding Box: 607.37, 112.95, 652.82, 150.94 for dataset/inria/Train/pos\\crop001538.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 179.75, 206.93, 357.82, 627.36 for dataset/inria/Train/pos\\crop001538.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 257.08, 286.17, 359.75, 628.02 for dataset/inria/Train/pos\\crop001538.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 62.74, 227.90, 93.47, 273.05 for dataset/inria/Train/pos\\crop001538.png\n",
      "\n",
      "0: 576x640 5 persons, 1 suitcase, 4 tvs, 20.0ms\n",
      "Speed: 4.0ms preprocess, 20.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 242.27, 157.90, 314.60, 342.16 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 28.0, Confidence: 0.70, Bounding Box: 131.72, 249.39, 181.13, 339.73 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 314.04, 182.80, 352.00, 217.39 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 239.95, 158.64, 282.43, 341.53 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 62.0, Confidence: 0.46, Bounding Box: 300.85, 84.68, 348.63, 138.70 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 62.0, Confidence: 0.42, Bounding Box: 271.24, 83.07, 302.41, 135.93 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 62.0, Confidence: 0.39, Bounding Box: 487.33, 73.71, 541.76, 137.60 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 62.0, Confidence: 0.35, Bounding Box: 384.65, 76.40, 443.84, 137.15 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 235.21, 158.71, 278.96, 308.94 for dataset/inria/Train/pos\\crop001539.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 358.58, 183.17, 439.29, 284.26 for dataset/inria/Train/pos\\crop001539.png\n",
      "\n",
      "0: 640x640 7 persons, 1 chair, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 434.64, 258.67, 546.45, 539.02 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 221.67, 245.85, 318.50, 473.85 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 562.65, 262.55, 613.04, 399.29 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 592.48, 246.85, 625.67, 338.16 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 692.80, 269.26, 723.82, 350.40 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 560.67, 262.88, 599.25, 399.68 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 73.64, 524.74, 246.45, 700.93 for dataset/inria/Train/pos\\crop001540.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 600.34, 259.25, 624.95, 337.81 for dataset/inria/Train/pos\\crop001540.png\n",
      "\n",
      "0: 640x640 12 persons, 1 handbag, 17.0ms\n",
      "Speed: 5.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 448.66, 317.07, 524.60, 569.72 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 1.57, 300.67, 80.70, 572.89 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 287.25, 304.43, 459.52, 769.88 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 517.27, 308.95, 578.53, 496.64 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 245.85, 325.28, 309.13, 534.94 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 93.04, 304.44, 144.40, 434.28 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 26.0, Confidence: 0.51, Bounding Box: 567.58, 419.13, 604.17, 473.63 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 834.61, 311.68, 880.09, 434.91 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 223.34, 309.92, 264.88, 385.11 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 719.72, 311.69, 752.34, 394.38 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 200.41, 303.58, 228.40, 380.42 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 309.69, 300.13, 339.66, 367.33 for dataset/inria/Train/pos\\crop001541.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 123.63, 293.59, 156.62, 433.60 for dataset/inria/Train/pos\\crop001541.png\n",
      "\n",
      "0: 640x640 7 persons, 2 suitcases, 1 microwave, 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 238.44, 194.18, 326.02, 471.38 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 472.32, 232.30, 555.78, 457.35 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 176.59, 201.09, 241.87, 375.45 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 19.76, 208.91, 97.30, 377.60 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 413.52, 234.32, 488.59, 452.01 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 128.87, 202.95, 200.81, 445.98 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 94.59, 239.22, 148.24, 408.49 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 28.0, Confidence: 0.49, Bounding Box: 303.43, 373.53, 405.39, 463.76 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 68.0, Confidence: 0.48, Bounding Box: 591.38, 73.69, 680.07, 191.97 for dataset/inria/Train/pos\\crop001542.png\n",
      "Class: 28.0, Confidence: 0.27, Bounding Box: 166.95, 369.39, 251.04, 453.82 for dataset/inria/Train/pos\\crop001542.png\n",
      "\n",
      "0: 640x640 2 persons, 3 cars, 1 traffic light, 1 chair, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 91.61, 158.18, 256.83, 213.80 for dataset/inria/Train/pos\\crop001543.png\n",
      "Class: 2.0, Confidence: 0.86, Bounding Box: 0.14, 159.71, 92.77, 211.20 for dataset/inria/Train/pos\\crop001543.png\n",
      "Class: 56.0, Confidence: 0.84, Bounding Box: 63.32, 257.11, 135.11, 346.16 for dataset/inria/Train/pos\\crop001543.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 294.29, 129.91, 317.19, 192.07 for dataset/inria/Train/pos\\crop001543.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 150.58, 140.75, 180.95, 226.94 for dataset/inria/Train/pos\\crop001543.png\n",
      "Class: 9.0, Confidence: 0.32, Bounding Box: 255.62, 106.68, 275.88, 126.99 for dataset/inria/Train/pos\\crop001543.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 175.39, 157.13, 255.88, 213.67 for dataset/inria/Train/pos\\crop001543.png\n",
      "\n",
      "0: 480x640 10 persons, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 99.53, 103.43, 121.00, 148.59 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 40.91, 128.93, 62.46, 177.06 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 173.22, 152.97, 192.18, 201.83 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 198.82, 105.95, 218.34, 144.44 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 203.68, 160.44, 222.09, 205.81 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 235.23, 154.80, 251.33, 197.09 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 41.90, 243.73, 58.73, 300.00 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 263.89, 189.84, 278.55, 226.75 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 342.18, 126.86, 366.50, 176.73 for dataset/inria/Train/pos\\crop001547.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 34.02, 245.59, 53.25, 300.00 for dataset/inria/Train/pos\\crop001547.png\n",
      "\n",
      "0: 576x640 4 persons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 143.30, 35.62, 163.79, 76.17 for dataset/inria/Train/pos\\crop001548.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 106.67, 105.49, 151.37, 175.84 for dataset/inria/Train/pos\\crop001548.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 106.76, 105.55, 134.18, 176.38 for dataset/inria/Train/pos\\crop001548.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 18.37, 65.21, 41.01, 104.32 for dataset/inria/Train/pos\\crop001548.png\n",
      "\n",
      "0: 608x640 4 persons, 17.4ms\n",
      "Speed: 5.0ms preprocess, 17.4ms inference, 3.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 828.01, 63.06, 954.53, 517.52 for dataset/inria/Train/pos\\crop001550.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 511.32, 181.90, 748.34, 591.54 for dataset/inria/Train/pos\\crop001550.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 318.02, 248.42, 552.57, 686.81 for dataset/inria/Train/pos\\crop001550.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 872.29, 502.18, 954.59, 666.44 for dataset/inria/Train/pos\\crop001550.png\n",
      "\n",
      "0: 640x640 5 persons, 1 toilet, 17.9ms\n",
      "Speed: 5.0ms preprocess, 17.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 123.66, 214.15, 359.18, 667.10 for dataset/inria/Train/pos\\crop001551.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 657.97, 0.25, 796.10, 354.40 for dataset/inria/Train/pos\\crop001551.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 410.20, 196.47, 614.00, 636.77 for dataset/inria/Train/pos\\crop001551.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 169.05, 512.86, 418.09, 825.00 for dataset/inria/Train/pos\\crop001551.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 515.82, 0.01, 723.25, 278.48 for dataset/inria/Train/pos\\crop001551.png\n",
      "Class: 61.0, Confidence: 0.67, Bounding Box: 559.74, 544.01, 796.64, 826.94 for dataset/inria/Train/pos\\crop001551.png\n",
      "\n",
      "0: 608x640 9 persons, 1 skateboard, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.06, 490.17, 131.16, 843.30 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 357.02, 255.38, 533.07, 648.26 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 174.54, 268.38, 330.57, 647.25 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 30.74, 216.74, 240.47, 573.60 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 501.28, 259.23, 689.68, 643.28 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 618.98, 219.95, 794.13, 652.50 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 754.35, 65.94, 872.99, 244.18 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 733.78, 214.52, 880.24, 410.80 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 735.18, 212.84, 879.70, 650.46 for dataset/inria/Train/pos\\crop001552.png\n",
      "Class: 36.0, Confidence: 0.29, Bounding Box: 533.36, 568.03, 558.62, 647.71 for dataset/inria/Train/pos\\crop001552.png\n",
      "\n",
      "0: 640x512 5 persons, 1 skateboard, 18.0ms\n",
      "Speed: 5.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 207.31, 175.14, 381.61, 573.08 for dataset/inria/Train/pos\\crop001553.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 19.72, 239.04, 138.07, 415.53 for dataset/inria/Train/pos\\crop001553.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 2.58, 388.64, 145.63, 587.66 for dataset/inria/Train/pos\\crop001553.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 1.76, 391.42, 79.49, 725.75 for dataset/inria/Train/pos\\crop001553.png\n",
      "Class: 36.0, Confidence: 0.37, Bounding Box: 204.81, 546.49, 281.60, 581.74 for dataset/inria/Train/pos\\crop001553.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 493.50, 593.27, 560.91, 725.62 for dataset/inria/Train/pos\\crop001553.png\n",
      "\n",
      "0: 640x512 1 person, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 12.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 194.81, 183.29, 304.87, 501.92 for dataset/inria/Train/pos\\crop001554.png\n",
      "\n",
      "0: 448x640 5 persons, 1 suitcase, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 180.31, 137.60, 263.92, 326.50 for dataset/inria/Train/pos\\crop001556.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 12.62, 192.24, 127.01, 310.60 for dataset/inria/Train/pos\\crop001556.png\n",
      "Class: 28.0, Confidence: 0.49, Bounding Box: 484.80, 187.82, 567.50, 241.18 for dataset/inria/Train/pos\\crop001556.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 514.23, 108.68, 574.46, 195.98 for dataset/inria/Train/pos\\crop001556.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 92.86, 0.00, 136.17, 46.67 for dataset/inria/Train/pos\\crop001556.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 158.63, 0.00, 195.66, 65.44 for dataset/inria/Train/pos\\crop001556.png\n",
      "\n",
      "0: 544x640 8 persons, 56.0ms\n",
      "Speed: 4.0ms preprocess, 56.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 133.71, 257.34, 204.99, 411.40 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 7.39, 313.00, 69.10, 405.40 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 92.26, 255.95, 142.23, 409.43 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 258.37, 167.36, 317.22, 317.44 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 61.29, 257.32, 105.11, 405.12 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 187.72, 186.56, 250.16, 288.97 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 192.62, 266.01, 256.32, 411.21 for dataset/inria/Train/pos\\crop001557.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 250.72, 129.61, 323.56, 222.46 for dataset/inria/Train/pos\\crop001557.png\n",
      "\n",
      "0: 448x640 3 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 152.14, 465.65, 209.79, 515.00 for dataset/inria/Train/pos\\crop001558.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 305.82, 276.42, 360.47, 408.98 for dataset/inria/Train/pos\\crop001558.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 375.63, 297.28, 441.32, 431.87 for dataset/inria/Train/pos\\crop001558.png\n",
      "\n",
      "0: 448x640 8 persons, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 631.97, 155.92, 781.41, 546.05 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 378.08, 202.98, 578.72, 556.45 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 301.13, 187.17, 422.31, 539.80 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 508.74, 185.46, 628.61, 548.48 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 162.00, 161.54, 304.79, 451.85 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 183.61, 369.08, 350.44, 601.26 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 119.76, 146.35, 193.30, 378.69 for dataset/inria/Train/pos\\crop001559.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 0.09, 158.24, 71.16, 306.97 for dataset/inria/Train/pos\\crop001559.png\n",
      "\n",
      "0: 544x640 10 persons, 19.5ms\n",
      "Speed: 4.0ms preprocess, 19.5ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 723.92, 350.79, 815.28, 546.33 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 0.08, 298.43, 75.07, 516.43 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 491.44, 238.62, 637.10, 559.41 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 290.11, 301.75, 387.16, 534.67 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 193.29, 299.21, 284.69, 527.31 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 238.56, 311.81, 316.65, 526.03 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 438.92, 323.40, 526.34, 560.06 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 400.21, 307.56, 462.90, 528.94 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 860.19, 207.55, 904.79, 532.15 for dataset/inria/Train/pos\\crop001560.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 258.22, 313.05, 310.50, 522.90 for dataset/inria/Train/pos\\crop001560.png\n",
      "\n",
      "0: 576x640 10 persons, 1 elephant, 17.0ms\n",
      "Speed: 6.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 327.37, 353.74, 418.15, 550.74 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 474.05, 260.11, 606.10, 580.58 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 94.09, 243.10, 238.83, 564.91 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 42.59, 327.60, 129.18, 566.46 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 3.83, 311.10, 68.31, 531.56 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 717.67, 340.47, 787.64, 569.63 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 581.73, 352.14, 670.18, 541.73 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 761.51, 356.85, 819.77, 700.12 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 20.0, Confidence: 0.42, Bounding Box: 596.77, 11.58, 819.36, 351.91 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 463.75, 210.85, 542.87, 469.21 for dataset/inria/Train/pos\\crop001561.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 463.36, 210.59, 542.71, 383.82 for dataset/inria/Train/pos\\crop001561.png\n",
      "\n",
      "0: 512x640 3 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 198.95, 89.14, 224.76, 160.34 for dataset/inria/Train/pos\\crop001562.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 165.27, 81.04, 202.44, 158.70 for dataset/inria/Train/pos\\crop001562.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 0.02, 63.75, 19.98, 152.59 for dataset/inria/Train/pos\\crop001562.png\n",
      "\n",
      "0: 512x640 8 persons, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 271.31, 162.39, 389.28, 423.00 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 157.78, 166.71, 279.40, 419.43 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 429.22, 157.85, 567.06, 428.42 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 603.65, 2.24, 701.80, 207.29 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 568.77, 150.25, 676.18, 426.11 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 649.83, 159.77, 740.12, 424.65 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 511.81, 149.83, 581.20, 383.10 for dataset/inria/Train/pos\\crop001563.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 702.61, 384.64, 755.83, 524.21 for dataset/inria/Train/pos\\crop001563.png\n",
      "\n",
      "0: 544x640 9 persons, 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 166.47, 139.18, 265.47, 402.81 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 290.08, 313.91, 413.19, 457.39 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 319.55, 503.69, 420.00, 584.80 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 133.96, 342.36, 241.54, 581.76 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 450.30, 203.71, 552.11, 457.20 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 7.96, 344.24, 134.82, 582.66 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 211.76, 353.06, 303.59, 581.84 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 309.21, 209.46, 484.74, 316.25 for dataset/inria/Train/pos\\crop001564.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 75.07, 342.46, 141.43, 574.14 for dataset/inria/Train/pos\\crop001564.png\n",
      "\n",
      "0: 640x608 9 persons, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 270.13, 296.28, 455.58, 510.82 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 455.21, 0.00, 546.00, 249.87 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 136.83, 133.78, 244.64, 414.27 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 169.86, 0.00, 267.05, 194.27 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 218.22, 147.24, 306.83, 410.60 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 292.98, 106.32, 416.76, 249.67 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 11.83, 140.50, 137.00, 415.41 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 80.72, 134.65, 146.44, 369.42 for dataset/inria/Train/pos\\crop001565.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 314.44, 1.30, 482.59, 110.43 for dataset/inria/Train/pos\\crop001565.png\n",
      "\n",
      "0: 640x544 1 person, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 95.77, 98.12, 158.48, 247.70 for dataset/inria/Train/pos\\crop001567.png\n",
      "\n",
      "0: 640x416 3 persons, 18.5ms\n",
      "Speed: 3.0ms preprocess, 18.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 153.89, 198.22, 339.17, 708.74 for dataset/inria/Train/pos\\crop001568.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 1.12, 28.92, 57.21, 698.80 for dataset/inria/Train/pos\\crop001568.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 1.24, 47.91, 58.20, 317.63 for dataset/inria/Train/pos\\crop001568.png\n",
      "\n",
      "0: 416x640 7 persons, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 151.16, 156.80, 304.27, 599.15 for dataset/inria/Train/pos\\crop001569.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 831.04, 219.44, 1058.46, 769.97 for dataset/inria/Train/pos\\crop001569.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 232.99, 181.06, 466.08, 632.93 for dataset/inria/Train/pos\\crop001569.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 639.25, 166.74, 846.58, 693.21 for dataset/inria/Train/pos\\crop001569.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 982.91, 1.38, 1121.60, 54.61 for dataset/inria/Train/pos\\crop001569.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 697.42, 166.36, 882.89, 693.65 for dataset/inria/Train/pos\\crop001569.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 920.76, 172.70, 1079.59, 640.44 for dataset/inria/Train/pos\\crop001569.png\n",
      "\n",
      "0: 640x576 8 persons, 1 skateboard, 22.0ms\n",
      "Speed: 5.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 255.65, 175.90, 370.13, 438.61 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 0.67, 156.14, 63.92, 521.91 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 30.74, 168.10, 81.82, 256.82 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 100.21, 166.88, 142.38, 283.35 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 122.28, 184.81, 195.58, 364.79 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 36.0, Confidence: 0.62, Bounding Box: 137.21, 264.86, 184.11, 372.01 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 249.40, 177.07, 299.49, 376.08 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 122.00, 184.35, 196.68, 300.93 for dataset/inria/Train/pos\\crop001570.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 73.81, 171.77, 104.10, 229.90 for dataset/inria/Train/pos\\crop001570.png\n",
      "\n",
      "0: 576x640 14 persons, 1 truck, 1 skateboard, 18.5ms\n",
      "Speed: 4.0ms preprocess, 18.5ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 353.31, 204.90, 464.02, 450.64 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 220.07, 200.83, 301.75, 412.51 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 428.06, 204.81, 549.46, 562.26 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 87.66, 191.03, 118.98, 277.27 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 46.28, 191.49, 93.29, 316.44 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 741.69, 224.96, 800.79, 488.23 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 338.16, 201.94, 392.04, 369.66 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 516.41, 217.14, 569.44, 305.83 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 7.0, Confidence: 0.63, Bounding Box: 161.86, 159.69, 306.14, 322.18 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 608.35, 232.20, 679.20, 418.71 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 587.45, 216.14, 626.47, 330.59 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 36.0, Confidence: 0.48, Bounding Box: 432.05, 525.61, 523.81, 569.71 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 734.65, 226.31, 785.22, 428.43 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 321.90, 197.01, 338.42, 241.85 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 305.87, 200.93, 320.94, 241.23 for dataset/inria/Train/pos\\crop001571.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 617.38, 311.31, 674.41, 418.36 for dataset/inria/Train/pos\\crop001571.png\n",
      "\n",
      "0: 576x640 9 persons, 1 motorcycle, 1 truck, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 556.09, 196.10, 659.65, 444.84 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 421.70, 194.14, 500.42, 405.90 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 289.62, 183.46, 321.59, 268.65 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 248.46, 184.98, 296.81, 309.55 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 0.00, 165.53, 85.51, 401.94 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 65.83, 181.50, 107.94, 299.35 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 7.0, Confidence: 0.63, Bounding Box: 363.55, 150.68, 507.04, 314.43 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 3.0, Confidence: 0.61, Bounding Box: 0.00, 291.04, 53.74, 477.89 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 523.26, 190.50, 540.21, 235.58 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 90.45, 182.42, 125.85, 297.59 for dataset/inria/Train/pos\\crop001572.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 507.52, 193.92, 521.92, 234.73 for dataset/inria/Train/pos\\crop001572.png\n",
      "\n",
      "0: 640x416 2 persons, 1 car, 1 chair, 15.0ms\n",
      "Speed: 4.5ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 150.85, 36.21, 413.52, 843.91 for dataset/inria/Train/pos\\crop001577.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 4.31, 239.02, 139.75, 468.79 for dataset/inria/Train/pos\\crop001577.png\n",
      "Class: 2.0, Confidence: 0.50, Bounding Box: 70.87, 245.23, 603.85, 887.58 for dataset/inria/Train/pos\\crop001577.png\n",
      "Class: 56.0, Confidence: 0.47, Bounding Box: 396.29, 210.41, 444.74, 288.54 for dataset/inria/Train/pos\\crop001577.png\n",
      "\n",
      "0: 640x544 5 persons, 1 backpack, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 92.61, 94.29, 127.60, 189.36 for dataset/inria/Train/pos\\crop001578.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 19.38, 150.67, 24.11, 161.75 for dataset/inria/Train/pos\\crop001578.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 31.47, 150.41, 35.86, 162.36 for dataset/inria/Train/pos\\crop001578.png\n",
      "Class: 24.0, Confidence: 0.33, Bounding Box: 97.87, 106.38, 124.46, 136.69 for dataset/inria/Train/pos\\crop001578.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 71.20, 135.72, 75.24, 145.83 for dataset/inria/Train/pos\\crop001578.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 4.93, 142.46, 9.28, 154.33 for dataset/inria/Train/pos\\crop001578.png\n",
      "\n",
      "0: 640x576 6 persons, 1 snowboard, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 96.16, 87.23, 182.70, 299.54 for dataset/inria/Train/pos\\crop001579.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 79.31, 126.12, 95.91, 166.85 for dataset/inria/Train/pos\\crop001579.png\n",
      "Class: 31.0, Confidence: 0.68, Bounding Box: 45.62, 283.08, 220.86, 305.47 for dataset/inria/Train/pos\\crop001579.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 54.22, 160.62, 58.10, 167.13 for dataset/inria/Train/pos\\crop001579.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 128.42, 21.28, 141.29, 38.20 for dataset/inria/Train/pos\\crop001579.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 59.57, 142.09, 63.18, 148.76 for dataset/inria/Train/pos\\crop001579.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 98.77, 107.38, 106.49, 121.16 for dataset/inria/Train/pos\\crop001579.png\n",
      "\n",
      "0: 640x448 5 persons, 1 car, 1 motorcycle, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.29, 302.86, 236.07, 759.20 for dataset/inria/Train/pos\\crop001580.png\n",
      "Class: 3.0, Confidence: 0.81, Bounding Box: 0.00, 495.93, 346.07, 959.13 for dataset/inria/Train/pos\\crop001580.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 432.58, 186.84, 634.65, 960.00 for dataset/inria/Train/pos\\crop001580.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 254.87, 240.45, 382.56, 778.43 for dataset/inria/Train/pos\\crop001580.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 359.87, 291.54, 521.31, 959.68 for dataset/inria/Train/pos\\crop001580.png\n",
      "Class: 2.0, Confidence: 0.52, Bounding Box: 0.00, 507.36, 346.97, 959.26 for dataset/inria/Train/pos\\crop001580.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 190.95, 201.42, 315.80, 504.17 for dataset/inria/Train/pos\\crop001580.png\n",
      "\n",
      "0: 640x448 6 persons, 1 car, 1 motorcycle, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 1.51, 259.08, 126.55, 560.37 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 256.67, 180.71, 473.56, 877.14 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 136.23, 214.20, 217.41, 572.21 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 406.07, 205.78, 486.88, 362.10 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 195.74, 255.54, 311.46, 728.94 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 2.0, Confidence: 0.68, Bounding Box: 0.00, 422.40, 201.54, 904.86 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 3.0, Confidence: 0.57, Bounding Box: 1.17, 417.88, 201.87, 902.84 for dataset/inria/Train/pos\\crop001581.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 102.11, 200.87, 175.66, 401.33 for dataset/inria/Train/pos\\crop001581.png\n",
      "\n",
      "0: 640x480 7 persons, 1 car, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 207.47, 38.91, 448.11, 770.79 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 451.05, 97.10, 602.09, 396.85 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 301.94, 214.04, 680.47, 960.00 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 72.94, 0.02, 233.99, 195.41 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 550.13, 67.87, 647.65, 232.84 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 211.87, 62.77, 305.03, 264.28 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 195.65, 62.25, 305.03, 313.95 for dataset/inria/Train/pos\\crop001582.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 367.92, 114.46, 428.14, 189.55 for dataset/inria/Train/pos\\crop001582.png\n",
      "\n",
      "0: 640x320 3 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 97.77, 221.09, 270.56, 833.00 for dataset/inria/Train/pos\\crop001583.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 248.29, 259.56, 418.54, 812.35 for dataset/inria/Train/pos\\crop001583.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.71, 243.63, 125.84, 897.87 for dataset/inria/Train/pos\\crop001583.png\n",
      "\n",
      "0: 640x576 7 persons, 2 trucks, 16.0ms\n",
      "Speed: 5.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 258.64, 92.10, 521.98, 838.61 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.00, 145.57, 105.59, 583.50 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 59.30, 76.00, 253.66, 841.77 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 234.79, 124.97, 362.32, 769.66 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 7.0, Confidence: 0.42, Bounding Box: 230.66, 40.45, 822.80, 790.66 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 7.0, Confidence: 0.42, Bounding Box: 54.02, 67.93, 821.88, 852.37 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 214.21, 143.45, 273.23, 410.69 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 746.65, 156.29, 823.05, 237.76 for dataset/inria/Train/pos\\crop001584.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 15.07, 147.02, 78.79, 236.68 for dataset/inria/Train/pos\\crop001584.png\n",
      "\n",
      "0: 640x352 10 persons, 1 handbag, 33.5ms\n",
      "Speed: 4.0ms preprocess, 33.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 44.57, 221.05, 295.98, 816.92 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 298.35, 307.62, 493.40, 887.49 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 38.14, 196.54, 123.76, 387.46 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 345.94, 220.81, 428.52, 494.49 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 187.12, 255.38, 327.79, 702.28 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 423.33, 241.03, 493.20, 342.81 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 2.25, 244.41, 55.21, 610.39 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 190.34, 256.06, 329.98, 418.49 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 219.22, 347.08, 319.94, 496.70 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 29.60, 197.37, 122.93, 635.11 for dataset/inria/Train/pos\\crop001589.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 10.63, 252.23, 115.97, 618.00 for dataset/inria/Train/pos\\crop001589.png\n",
      "\n",
      "0: 640x544 3 persons, 14.8ms\n",
      "Speed: 3.2ms preprocess, 14.8ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 26.18, 189.19, 175.89, 575.08 for dataset/inria/Train/pos\\crop001591.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 183.20, 126.98, 435.47, 946.06 for dataset/inria/Train/pos\\crop001591.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 1.25, 776.53, 81.11, 925.65 for dataset/inria/Train/pos\\crop001591.png\n",
      "\n",
      "0: 480x640 8 persons, 5 cars, 2 trucks, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 0.00, 191.20, 53.82, 407.61 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 2.0, Confidence: 0.86, Bounding Box: 574.50, 240.25, 653.92, 348.55 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 7.0, Confidence: 0.83, Bounding Box: 340.72, 140.65, 586.58, 370.09 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 619.71, 223.32, 715.32, 326.57 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 414.92, 207.92, 500.20, 396.29 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 385.72, 207.21, 440.58, 384.70 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 284.00, 198.23, 378.27, 393.64 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 695.99, 272.60, 715.75, 366.45 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 105.54, 196.88, 228.82, 396.74 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 104.51, 192.34, 191.36, 393.82 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 226.43, 195.47, 311.85, 387.46 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 7.0, Confidence: 0.43, Bounding Box: 31.36, 196.26, 244.30, 355.57 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 140.82, 211.20, 231.55, 395.98 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 35.58, 201.99, 244.24, 356.85 for dataset/inria/Train/pos\\crop001592.png\n",
      "Class: 2.0, Confidence: 0.26, Bounding Box: 36.78, 203.75, 146.29, 358.87 for dataset/inria/Train/pos\\crop001592.png\n",
      "\n",
      "0: 576x640 13 persons, 1 handbag, 31.5ms\n",
      "Speed: 4.5ms preprocess, 31.5ms inference, 2.9ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 412.14, 191.02, 558.55, 525.94 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 312.80, 226.84, 400.03, 501.55 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 599.98, 252.60, 717.65, 463.16 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 189.60, 224.29, 333.37, 497.80 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 92.19, 188.71, 150.96, 356.88 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 11.46, 196.16, 75.83, 350.92 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 571.03, 207.08, 627.00, 447.00 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 59.89, 199.06, 90.84, 299.86 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 0.00, 194.44, 26.19, 310.33 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 302.92, 190.31, 369.01, 280.04 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 572.67, 205.21, 656.92, 448.34 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 627.20, 204.52, 677.05, 300.71 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 530.12, 369.09, 585.10, 460.63 for dataset/inria/Train/pos\\crop001594.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 298.83, 192.19, 348.47, 259.02 for dataset/inria/Train/pos\\crop001594.png\n",
      "\n",
      "0: 576x640 15 persons, 2 backpacks, 13.8ms\n",
      "Speed: 4.0ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 316.55, 143.58, 380.98, 315.61 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 0.23, 188.48, 139.20, 435.14 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 417.12, 182.87, 510.77, 436.72 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 244.14, 150.90, 304.37, 306.54 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 157.77, 158.03, 186.71, 244.58 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 39.58, 161.24, 71.29, 249.18 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 183.68, 156.04, 207.10, 243.77 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 91.37, 149.74, 118.75, 236.02 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 219.50, 150.93, 253.35, 265.46 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 118.29, 145.29, 162.74, 243.03 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 287.68, 157.02, 319.69, 257.72 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 408.69, 139.58, 510.82, 264.70 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 24.0, Confidence: 0.37, Bounding Box: 99.00, 161.79, 115.63, 191.56 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 375.85, 161.31, 406.37, 265.86 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 24.0, Confidence: 0.31, Bounding Box: 245.40, 174.98, 286.81, 245.52 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 201.18, 153.97, 233.50, 248.68 for dataset/inria/Train/pos\\crop001595.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 0.00, 150.95, 40.75, 286.90 for dataset/inria/Train/pos\\crop001595.png\n",
      "\n",
      "0: 544x640 7 persons, 16.0ms\n",
      "Speed: 3.1ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 640.27, 242.60, 844.71, 700.76 for dataset/inria/Train/pos\\crop001596.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 168.38, 240.72, 353.06, 797.49 for dataset/inria/Train/pos\\crop001596.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 316.36, 234.08, 480.02, 804.69 for dataset/inria/Train/pos\\crop001596.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 105.95, 178.97, 259.03, 498.46 for dataset/inria/Train/pos\\crop001596.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 493.14, 220.04, 579.85, 367.23 for dataset/inria/Train/pos\\crop001596.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 0.00, 219.34, 91.65, 425.76 for dataset/inria/Train/pos\\crop001596.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 0.13, 165.30, 58.16, 428.95 for dataset/inria/Train/pos\\crop001596.png\n",
      "\n",
      "0: 576x640 4 persons, 2 handbags, 1 tie, 3 chairs, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 289.80, 231.94, 460.67, 685.75 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.56, 132.71, 286.90, 784.21 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 86.41, 103.63, 303.20, 509.66 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 56.0, Confidence: 0.84, Bounding Box: 450.41, 321.20, 529.69, 450.15 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 437.78, 254.93, 513.23, 387.52 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 56.0, Confidence: 0.68, Bounding Box: 598.50, 237.25, 861.44, 550.09 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 27.0, Confidence: 0.59, Bounding Box: 122.73, 252.43, 174.38, 402.97 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 296.24, 402.05, 393.38, 510.59 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 56.0, Confidence: 0.35, Bounding Box: 761.10, 339.30, 874.85, 607.30 for dataset/inria/Train/pos\\crop001597.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 422.53, 397.66, 461.21, 464.07 for dataset/inria/Train/pos\\crop001597.png\n",
      "\n",
      "0: 640x512 2 persons, 1 backpack, 1 chair, 17.5ms\n",
      "Speed: 3.0ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 521.44, 228.36, 695.00, 880.81 for dataset/inria/Train/pos\\crop001598.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 231.39, 301.66, 446.66, 693.47 for dataset/inria/Train/pos\\crop001598.png\n",
      "Class: 56.0, Confidence: 0.60, Bounding Box: 1.76, 349.53, 270.69, 663.38 for dataset/inria/Train/pos\\crop001598.png\n",
      "Class: 24.0, Confidence: 0.34, Bounding Box: 394.54, 537.10, 507.30, 699.47 for dataset/inria/Train/pos\\crop001598.png\n",
      "\n",
      "0: 640x512 11 persons, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 1.80, 258.73, 243.31, 845.10 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 213.41, 235.47, 425.85, 823.92 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 424.32, 355.89, 656.23, 845.07 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 430.59, 302.38, 477.60, 454.82 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 571.38, 269.47, 656.33, 399.02 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 167.85, 296.87, 225.86, 409.18 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 150.61, 320.56, 172.60, 371.26 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 360.30, 296.09, 431.22, 553.06 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 14.50, 324.47, 43.63, 373.91 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 0.00, 324.99, 28.17, 399.98 for dataset/inria/Train/pos\\crop001599.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 377.50, 317.04, 432.56, 554.04 for dataset/inria/Train/pos\\crop001599.png\n",
      "\n",
      "0: 640x448 2 persons, 4 cars, 1 truck, 1 handbag, 16.5ms\n",
      "Speed: 3.0ms preprocess, 16.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 161.27, 172.99, 255.39, 418.22 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 314.32, 190.11, 391.00, 317.07 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 2.0, Confidence: 0.72, Bounding Box: 264.02, 183.39, 381.84, 271.28 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 7.0, Confidence: 0.67, Bounding Box: 0.00, 135.20, 204.15, 307.75 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 2.0, Confidence: 0.50, Bounding Box: 227.88, 182.09, 326.96, 253.86 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 261.69, 151.22, 274.67, 179.18 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 162.34, 277.01, 199.89, 320.67 for dataset/inria/Train/pos\\crop001600.png\n",
      "Class: 2.0, Confidence: 0.26, Bounding Box: 193.05, 163.39, 264.01, 195.13 for dataset/inria/Train/pos\\crop001600.png\n",
      "\n",
      "0: 640x320 1 person, 5 cars, 15.8ms\n",
      "Speed: 2.1ms preprocess, 15.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 64.12, 239.60, 243.68, 776.82 for dataset/inria/Train/pos\\crop001601.png\n",
      "Class: 2.0, Confidence: 0.84, Bounding Box: 323.27, 300.43, 465.26, 386.64 for dataset/inria/Train/pos\\crop001601.png\n",
      "Class: 2.0, Confidence: 0.78, Bounding Box: 0.44, 296.31, 117.95, 440.12 for dataset/inria/Train/pos\\crop001601.png\n",
      "Class: 2.0, Confidence: 0.58, Bounding Box: 195.35, 309.11, 322.77, 389.15 for dataset/inria/Train/pos\\crop001601.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 214.12, 294.14, 337.28, 358.18 for dataset/inria/Train/pos\\crop001601.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 204.27, 294.33, 339.73, 384.84 for dataset/inria/Train/pos\\crop001601.png\n",
      "\n",
      "0: 640x352 4 persons, 1 handbag, 27.0ms\n",
      "Speed: 3.3ms preprocess, 27.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 279.28, 354.66, 412.63, 767.82 for dataset/inria/Train/pos\\crop001603.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 201.43, 350.91, 280.16, 756.90 for dataset/inria/Train/pos\\crop001603.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 401.57, 328.24, 493.00, 808.50 for dataset/inria/Train/pos\\crop001603.png\n",
      "Class: 26.0, Confidence: 0.66, Bounding Box: 388.42, 564.32, 447.93, 652.87 for dataset/inria/Train/pos\\crop001603.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 379.65, 358.84, 448.48, 453.79 for dataset/inria/Train/pos\\crop001603.png\n",
      "\n",
      "0: 640x352 2 persons, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 179.81, 185.31, 243.01, 348.69 for dataset/inria/Train/pos\\crop001605.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 163.85, 169.57, 188.64, 226.03 for dataset/inria/Train/pos\\crop001605.png\n",
      "\n",
      "0: 640x576 2 persons, 1 car, 2 trucks, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 269.70, 149.40, 358.63, 261.47 for dataset/inria/Train/pos\\crop001606.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 174.12, 135.04, 220.18, 280.94 for dataset/inria/Train/pos\\crop001606.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 25.46, 76.13, 54.18, 153.50 for dataset/inria/Train/pos\\crop001606.png\n",
      "Class: 7.0, Confidence: 0.30, Bounding Box: 101.28, 25.62, 354.12, 260.28 for dataset/inria/Train/pos\\crop001606.png\n",
      "Class: 7.0, Confidence: 0.26, Bounding Box: 178.31, 25.80, 357.54, 258.24 for dataset/inria/Train/pos\\crop001606.png\n",
      "\n",
      "0: 640x416 3 persons, 47.0ms\n",
      "Speed: 2.0ms preprocess, 47.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.37, 320.04, 106.96, 586.91 for dataset/inria/Train/pos\\crop001608.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 369.38, 323.86, 452.94, 687.68 for dataset/inria/Train/pos\\crop001608.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 201.52, 423.61, 271.88, 556.36 for dataset/inria/Train/pos\\crop001608.png\n",
      "\n",
      "0: 512x640 10 persons, 22.1ms\n",
      "Speed: 3.0ms preprocess, 22.1ms inference, 33.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 247.10, 178.65, 309.15, 308.18 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 307.95, 186.91, 346.23, 306.12 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 400.25, 115.27, 435.31, 184.45 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 251.53, 384.99, 326.80, 457.42 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 1.64, 396.25, 81.10, 458.00 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 475.40, 128.33, 498.63, 187.16 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 512.97, 120.14, 536.37, 195.78 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 105.67, 420.90, 206.76, 457.39 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 527.88, 110.21, 551.49, 188.91 for dataset/inria/Train/pos\\crop001609.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 512.74, 116.48, 547.33, 195.08 for dataset/inria/Train/pos\\crop001609.png\n",
      "\n",
      "0: 416x640 3 persons, 2 chairs, 3 potted plants, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 323.59, 165.67, 398.63, 361.47 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 58.0, Confidence: 0.71, Bounding Box: 35.38, 64.80, 140.67, 202.27 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 58.0, Confidence: 0.65, Bounding Box: 0.19, 18.38, 69.27, 141.29 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 315.06, 160.13, 352.13, 354.00 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 58.0, Confidence: 0.51, Bounding Box: 445.51, 42.70, 464.92, 63.84 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 314.85, 160.36, 351.61, 294.20 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 511.59, 43.34, 544.31, 94.57 for dataset/inria/Train/pos\\crop001610.png\n",
      "Class: 56.0, Confidence: 0.27, Bounding Box: 524.30, 54.04, 575.29, 112.46 for dataset/inria/Train/pos\\crop001610.png\n",
      "\n",
      "0: 640x448 3 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 341.51, 364.16, 433.84, 686.45 for dataset/inria/Train/pos\\crop001611.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 244.54, 393.30, 331.58, 668.93 for dataset/inria/Train/pos\\crop001611.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 535.63, 144.80, 641.00, 734.20 for dataset/inria/Train/pos\\crop001611.png\n",
      "\n",
      "0: 608x640 12 persons, 1 traffic light, 1 backpack, 2 handbags, 17.0ms\n",
      "Speed: 5.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 412.97, 284.19, 521.51, 584.79 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 560.06, 290.01, 654.12, 519.62 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 282.41, 277.74, 417.22, 580.93 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 33.96, 304.88, 128.96, 579.82 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 131.29, 291.24, 157.06, 366.14 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 0.51, 291.35, 68.32, 525.29 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 154.51, 296.57, 174.49, 360.50 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 107.63, 297.27, 130.44, 362.29 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 180.61, 291.27, 207.32, 364.14 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 26.0, Confidence: 0.42, Bounding Box: 29.84, 402.02, 80.15, 462.20 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 170.54, 292.24, 194.30, 362.73 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 0.04, 290.09, 20.28, 370.36 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 24.0, Confidence: 0.29, Bounding Box: 277.81, 314.12, 343.77, 413.47 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 27.23, 334.65, 85.97, 463.73 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 9.0, Confidence: 0.27, Bounding Box: 771.22, 0.35, 817.14, 75.60 for dataset/inria/Train/pos\\crop001612.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 176.88, 291.36, 200.44, 364.79 for dataset/inria/Train/pos\\crop001612.png\n",
      "\n",
      "0: 576x640 2 persons, 1 tie, 18.0ms\n",
      "Speed: 5.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.00, 195.69, 83.17, 426.28 for dataset/inria/Train/pos\\crop001613.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 322.79, 190.48, 438.69, 490.86 for dataset/inria/Train/pos\\crop001613.png\n",
      "Class: 27.0, Confidence: 0.53, Bounding Box: 13.81, 237.59, 29.86, 335.84 for dataset/inria/Train/pos\\crop001613.png\n",
      "\n",
      "0: 544x640 14 persons, 2 umbrellas, 1 handbag, 2 potted plants, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 536.72, 355.44, 647.19, 583.99 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 463.74, 356.18, 545.84, 583.26 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 342.17, 457.94, 404.47, 640.43 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 208.51, 344.05, 286.00, 524.70 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 666.63, 363.24, 765.24, 640.76 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 384.69, 365.29, 470.69, 633.61 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 915.42, 347.04, 993.70, 643.76 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 768.41, 351.98, 794.19, 427.75 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 25.0, Confidence: 0.61, Bounding Box: 268.00, 418.32, 350.63, 459.93 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 638.91, 355.17, 704.87, 590.74 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 790.98, 355.22, 810.27, 421.79 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 25.0, Confidence: 0.49, Bounding Box: 479.17, 315.27, 591.41, 348.25 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 58.0, Confidence: 0.43, Bounding Box: 71.19, 395.55, 125.20, 535.26 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 740.67, 359.67, 765.91, 421.55 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 58.0, Confidence: 0.32, Bounding Box: 13.80, 432.95, 75.43, 549.57 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 197.22, 335.69, 226.01, 462.74 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 814.33, 349.49, 846.10, 425.42 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 451.05, 519.27, 482.96, 569.02 for dataset/inria/Train/pos\\crop001614.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 822.25, 354.07, 845.46, 424.56 for dataset/inria/Train/pos\\crop001614.png\n",
      "\n",
      "0: 576x640 6 persons, 1 bicycle, 3 potted plants, 18.0ms\n",
      "Speed: 5.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 0.00, 290.95, 166.83, 683.50 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 763.60, 324.73, 840.70, 592.88 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 720.50, 418.12, 782.98, 598.36 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 588.95, 304.65, 663.89, 484.94 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 247.88, 275.08, 346.81, 557.99 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 58.0, Confidence: 0.49, Bounding Box: 444.76, 363.23, 505.87, 495.94 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 58.0, Confidence: 0.48, Bounding Box: 189.70, 387.75, 270.59, 583.71 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 58.0, Confidence: 0.38, Bounding Box: 390.32, 396.18, 445.82, 509.15 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 1.0, Confidence: 0.30, Bounding Box: 4.68, 543.47, 429.57, 748.00 for dataset/inria/Train/pos\\crop001615.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 578.80, 297.21, 604.22, 423.15 for dataset/inria/Train/pos\\crop001615.png\n",
      "\n",
      "0: 352x640 15 persons, 1 handbag, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 1008.99, 265.42, 1246.74, 620.08 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 0.00, 181.92, 102.39, 556.80 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 171.73, 188.16, 300.89, 569.37 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 280.71, 154.80, 427.06, 576.97 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 650.78, 212.96, 774.96, 545.09 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 550.62, 205.29, 660.33, 546.14 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 900.47, 216.62, 965.03, 428.17 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 396.96, 253.20, 491.80, 591.58 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 486.92, 185.42, 579.42, 431.16 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 961.30, 213.93, 1002.04, 327.88 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 1087.91, 222.61, 1129.67, 340.24 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 1049.94, 210.57, 1091.84, 337.42 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 853.78, 206.57, 898.20, 330.64 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 429.68, 187.66, 500.48, 316.22 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 1002.79, 203.49, 1046.60, 330.36 for dataset/inria/Train/pos\\crop001616.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 60.09, 388.00, 123.32, 492.26 for dataset/inria/Train/pos\\crop001616.png\n",
      "\n",
      "0: 640x640 8 persons, 1 umbrella, 1 clock, 22.0ms\n",
      "Speed: 4.0ms preprocess, 22.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 0.11, 351.06, 358.38, 706.42 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 314.03, 285.54, 421.12, 591.79 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 210.87, 295.24, 318.10, 510.26 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 70.97, 307.31, 109.28, 419.80 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 33.99, 302.06, 70.30, 418.75 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 0.15, 290.65, 24.54, 409.32 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 74.0, Confidence: 0.46, Bounding Box: 258.87, 33.70, 352.09, 150.16 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 189.27, 305.15, 238.12, 355.05 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 43.01, 303.99, 73.97, 418.48 for dataset/inria/Train/pos\\crop001617.png\n",
      "Class: 25.0, Confidence: 0.25, Bounding Box: 36.00, 200.97, 430.16, 295.20 for dataset/inria/Train/pos\\crop001617.png\n",
      "\n",
      "0: 544x640 4 persons, 1 backpack, 1 skateboard, 1 banana, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 261.82, 262.25, 384.47, 705.57 for dataset/inria/Train/pos\\crop001618.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 916.11, 280.63, 997.96, 618.77 for dataset/inria/Train/pos\\crop001618.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 756.57, 289.83, 826.15, 621.32 for dataset/inria/Train/pos\\crop001618.png\n",
      "Class: 24.0, Confidence: 0.72, Bounding Box: 917.04, 340.52, 982.10, 454.16 for dataset/inria/Train/pos\\crop001618.png\n",
      "Class: 36.0, Confidence: 0.40, Bounding Box: 123.05, 551.13, 198.63, 754.04 for dataset/inria/Train/pos\\crop001618.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 1119.66, 366.92, 1142.73, 555.71 for dataset/inria/Train/pos\\crop001618.png\n",
      "Class: 46.0, Confidence: 0.33, Bounding Box: 534.17, 371.38, 587.09, 437.50 for dataset/inria/Train/pos\\crop001618.png\n",
      "\n",
      "0: 576x640 11 persons, 1 bench, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 162.26, 322.06, 276.49, 785.29 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 355.97, 302.66, 456.26, 568.84 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 350.30, 428.54, 757.56, 825.85 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 781.65, 324.54, 860.76, 538.57 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 607.45, 306.89, 693.78, 500.38 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 708.83, 321.30, 773.88, 532.85 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 1014.43, 328.30, 1055.68, 412.55 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 13.0, Confidence: 0.60, Bounding Box: 212.48, 647.15, 667.96, 956.00 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 470.66, 305.62, 552.69, 512.01 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 568.35, 450.50, 680.16, 571.57 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 856.12, 340.09, 887.96, 443.30 for dataset/inria/Train/pos\\crop001619.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 0.02, 292.30, 23.77, 647.81 for dataset/inria/Train/pos\\crop001619.png\n",
      "\n",
      "0: 480x640 20 persons, 1 traffic light, 1 handbag, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 172.21, 351.06, 437.15, 597.19 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 307.26, 232.13, 407.06, 459.14 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 679.41, 237.51, 732.67, 405.65 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 623.94, 249.91, 834.41, 596.40 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 161.79, 249.72, 224.45, 468.79 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 629.62, 256.71, 685.16, 412.72 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 479.32, 237.69, 526.52, 446.03 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 747.53, 252.29, 785.22, 363.63 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 445.57, 268.67, 488.11, 401.54 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 575.09, 265.75, 604.48, 360.12 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 90.85, 286.48, 131.17, 388.89 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 26.49, 291.03, 70.36, 399.80 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 140.66, 271.19, 164.66, 335.88 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 56.10, 277.58, 95.85, 389.85 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 0.08, 285.96, 19.53, 410.41 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 549.55, 265.38, 569.27, 298.87 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 230.20, 259.25, 304.00, 366.06 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 525.73, 257.46, 552.45, 324.23 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 746.24, 256.85, 773.52, 364.83 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 436.98, 333.88, 455.45, 379.04 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 9.0, Confidence: 0.28, Bounding Box: 5.39, 196.38, 35.73, 255.21 for dataset/inria/Train/pos\\crop001620.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 0.00, 294.68, 11.24, 416.05 for dataset/inria/Train/pos\\crop001620.png\n",
      "\n",
      "0: 480x640 14 persons, 1 traffic light, 1 umbrella, 29.5ms\n",
      "Speed: 2.0ms preprocess, 29.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 475.82, 240.54, 696.70, 504.76 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.00, 138.92, 285.49, 500.79 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 620.68, 118.25, 696.80, 349.59 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 474.34, 138.09, 537.23, 355.76 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 201.07, 172.13, 284.95, 401.93 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 278.01, 187.99, 334.90, 303.18 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 339.16, 181.53, 384.60, 292.36 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 401.93, 172.90, 441.14, 276.09 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 368.46, 166.10, 406.83, 279.96 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 9.0, Confidence: 0.53, Bounding Box: 315.85, 82.48, 348.15, 141.95 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 538.12, 152.86, 604.01, 259.91 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 25.0, Confidence: 0.38, Bounding Box: 180.43, 136.37, 251.05, 160.54 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 3.74, 170.81, 73.39, 309.52 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 452.63, 158.16, 475.83, 222.21 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 593.68, 170.12, 621.58, 231.47 for dataset/inria/Train/pos\\crop001621.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 316.21, 178.76, 348.02, 284.37 for dataset/inria/Train/pos\\crop001621.png\n",
      "\n",
      "0: 640x576 2 persons, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 201.09, 161.70, 248.34, 305.49 for dataset/inria/Train/pos\\crop001622.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 241.85, 170.60, 291.22, 298.18 for dataset/inria/Train/pos\\crop001622.png\n",
      "\n",
      "0: 576x640 12 persons, 1 umbrella, 3 handbags, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 287.20, 256.58, 495.77, 850.20 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.00, 241.77, 164.78, 879.19 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 579.95, 228.02, 814.51, 856.31 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 483.15, 229.88, 619.38, 686.00 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 26.0, Confidence: 0.76, Bounding Box: 704.18, 331.37, 820.96, 569.90 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 904.92, 256.72, 1082.74, 949.61 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 860.30, 267.41, 919.05, 428.83 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 944.98, 276.52, 1003.51, 458.37 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 803.99, 270.44, 876.70, 442.41 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 706.10, 462.18, 822.71, 567.39 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 703.45, 335.28, 781.99, 570.23 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 988.43, 257.86, 1082.52, 399.26 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 796.88, 270.58, 876.47, 505.96 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 725.20, 277.64, 781.35, 355.90 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 917.20, 261.81, 970.74, 419.98 for dataset/inria/Train/pos\\crop001623.png\n",
      "Class: 25.0, Confidence: 0.33, Bounding Box: 1035.96, 211.68, 1082.64, 268.91 for dataset/inria/Train/pos\\crop001623.png\n",
      "\n",
      "0: 640x608 13 persons, 1 backpack, 1 handbag, 19.0ms\n",
      "Speed: 4.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.61, 151.08, 200.99, 572.00 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 321.79, 186.34, 528.80, 571.55 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 339.01, 210.90, 416.83, 414.85 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 253.54, 199.56, 306.86, 384.64 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 173.44, 202.14, 267.75, 485.36 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 308.67, 186.08, 380.94, 368.37 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 26.0, Confidence: 0.67, Bounding Box: 70.39, 422.27, 205.40, 546.56 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 126.10, 211.20, 172.28, 306.24 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 94.11, 209.54, 161.14, 308.29 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 24.0, Confidence: 0.46, Bounding Box: 271.67, 229.46, 301.17, 282.61 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 301.77, 175.74, 340.95, 233.50 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 281.27, 185.60, 323.01, 365.80 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 400.36, 156.06, 509.78, 345.48 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 167.69, 214.15, 202.03, 290.15 for dataset/inria/Train/pos\\crop001624.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 408.41, 200.03, 434.25, 251.33 for dataset/inria/Train/pos\\crop001624.png\n",
      "\n",
      "0: 640x640 2 persons, 12 cars, 18.5ms\n",
      "Speed: 4.0ms preprocess, 18.5ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 139.57, 118.86, 178.38, 226.30 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.61, Bounding Box: 0.24, 176.32, 39.16, 236.58 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.59, Bounding Box: 257.25, 120.60, 330.75, 163.25 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.59, Bounding Box: 0.11, 148.65, 59.34, 219.93 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.57, Bounding Box: 264.27, 142.51, 330.53, 181.28 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.57, Bounding Box: 0.58, 138.76, 99.48, 210.13 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.50, Bounding Box: 304.21, 151.21, 331.00, 187.16 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 316.27, 158.84, 330.95, 188.21 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.33, Bounding Box: 309.06, 158.69, 331.00, 187.28 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 133.70, 119.42, 151.03, 168.87 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 12.79, 133.22, 107.64, 183.10 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 8.28, 131.81, 75.46, 152.01 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.28, Bounding Box: 12.60, 130.88, 108.53, 163.74 for dataset/inria/Train/pos\\crop001625.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 69.20, 159.91, 107.86, 184.57 for dataset/inria/Train/pos\\crop001625.png\n",
      "\n",
      "0: 640x480 2 persons, 4 cars, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 213.29, 193.25, 307.62, 422.12 for dataset/inria/Train/pos\\crop001626.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 168.18, 170.85, 208.00, 293.04 for dataset/inria/Train/pos\\crop001626.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 284.32, 199.71, 416.10, 233.86 for dataset/inria/Train/pos\\crop001626.png\n",
      "Class: 2.0, Confidence: 0.60, Bounding Box: 198.72, 192.65, 318.02, 255.18 for dataset/inria/Train/pos\\crop001626.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 272.65, 201.93, 317.42, 223.43 for dataset/inria/Train/pos\\crop001626.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 414.69, 198.06, 448.61, 219.99 for dataset/inria/Train/pos\\crop001626.png\n",
      "\n",
      "0: 640x480 1 person, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 262.53, 230.00, 504.36, 798.36 for dataset/inria/Train/pos\\crop001627.png\n",
      "\n",
      "0: 640x480 1 person, 1 backpack, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 96.45, 164.35, 267.98, 649.42 for dataset/inria/Train/pos\\crop001628.png\n",
      "Class: 24.0, Confidence: 0.60, Bounding Box: 141.98, 220.33, 245.89, 346.29 for dataset/inria/Train/pos\\crop001628.png\n",
      "\n",
      "0: 544x640 3 persons, 4 backpacks, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 630.97, 284.81, 733.07, 632.75 for dataset/inria/Train/pos\\crop001629.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 427.65, 224.19, 579.35, 693.45 for dataset/inria/Train/pos\\crop001629.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 343.74, 225.70, 463.27, 658.78 for dataset/inria/Train/pos\\crop001629.png\n",
      "Class: 24.0, Confidence: 0.63, Bounding Box: 368.50, 284.51, 453.64, 453.40 for dataset/inria/Train/pos\\crop001629.png\n",
      "Class: 24.0, Confidence: 0.48, Bounding Box: 633.56, 344.30, 718.40, 465.43 for dataset/inria/Train/pos\\crop001629.png\n",
      "Class: 24.0, Confidence: 0.44, Bounding Box: 427.22, 282.02, 512.09, 415.16 for dataset/inria/Train/pos\\crop001629.png\n",
      "Class: 24.0, Confidence: 0.33, Bounding Box: 382.07, 281.30, 512.00, 422.10 for dataset/inria/Train/pos\\crop001629.png\n",
      "\n",
      "0: 640x608 1 person, 2 boats, 1 surfboard, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 138.19, 87.33, 190.52, 200.58 for dataset/inria/Train/pos\\crop001630.png\n",
      "Class: 37.0, Confidence: 0.50, Bounding Box: 1.63, 171.55, 249.49, 236.43 for dataset/inria/Train/pos\\crop001630.png\n",
      "Class: 8.0, Confidence: 0.42, Bounding Box: 0.66, 177.94, 249.24, 250.02 for dataset/inria/Train/pos\\crop001630.png\n",
      "Class: 8.0, Confidence: 0.36, Bounding Box: 1.10, 154.18, 251.56, 231.30 for dataset/inria/Train/pos\\crop001630.png\n",
      "\n",
      "0: 640x384 1 person, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 166.62, 144.53, 223.26, 321.56 for dataset/inria/Train/pos\\crop001632.png\n",
      "\n",
      "0: 640x512 12 persons, 16.0ms\n",
      "Speed: 5.5ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 162.42, 282.39, 421.69, 859.08 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 410.90, 419.16, 545.56, 854.35 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 61.60, 371.31, 152.09, 628.46 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 598.92, 270.56, 662.30, 452.16 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 656.14, 266.44, 724.30, 452.16 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 500.62, 284.63, 575.33, 477.99 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 665.17, 334.42, 758.00, 945.18 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 566.72, 267.62, 607.83, 436.89 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 414.26, 251.37, 455.35, 394.29 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 120.52, 265.27, 225.23, 598.20 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 447.01, 271.64, 501.85, 421.27 for dataset/inria/Train/pos\\crop001635.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 352.73, 319.78, 415.98, 421.47 for dataset/inria/Train/pos\\crop001635.png\n",
      "\n",
      "0: 544x640 11 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 494.86, 286.58, 613.34, 595.46 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 599.67, 283.41, 719.33, 600.03 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 326.45, 317.86, 455.77, 639.20 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 649.93, 300.79, 994.21, 820.84 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 0.29, 318.72, 196.76, 824.45 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 80.87, 384.13, 182.82, 544.63 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 182.26, 260.15, 250.56, 500.28 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 234.10, 300.61, 327.97, 544.81 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 433.83, 274.05, 508.04, 571.15 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 656.74, 282.14, 922.52, 709.87 for dataset/inria/Train/pos\\crop001636.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 214.39, 516.92, 407.09, 825.23 for dataset/inria/Train/pos\\crop001636.png\n",
      "\n",
      "0: 512x640 5 persons, 6 cars, 21.0ms\n",
      "Speed: 4.0ms preprocess, 21.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 790.98, 510.15, 1062.30, 778.83 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 519.78, 276.66, 647.29, 634.92 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 436.63, 283.61, 495.79, 454.12 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 2.0, Confidence: 0.78, Bounding Box: 681.06, 467.15, 1052.37, 644.50 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 489.17, 288.31, 553.75, 480.22 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 2.0, Confidence: 0.61, Bounding Box: 913.83, 568.27, 1063.00, 777.57 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 2.0, Confidence: 0.58, Bounding Box: 666.28, 332.89, 1059.33, 508.13 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 339.99, 313.75, 435.65, 558.94 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 2.0, Confidence: 0.45, Bounding Box: 920.17, 666.86, 1062.52, 822.93 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 634.18, 286.69, 677.94, 360.35 for dataset/inria/Train/pos\\crop001637.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 675.35, 360.28, 1057.90, 643.60 for dataset/inria/Train/pos\\crop001637.png\n",
      "\n",
      "0: 640x416 1 person, 1 truck, 14.9ms\n",
      "Speed: 2.6ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 264.01, 171.72, 548.54, 811.81 for dataset/inria/Train/pos\\crop001640.png\n",
      "Class: 7.0, Confidence: 0.35, Bounding Box: 0.00, 4.10, 586.75, 607.11 for dataset/inria/Train/pos\\crop001640.png\n",
      "\n",
      "0: 512x640 6 persons, 2 cars, 1 traffic light, 2 handbags, 15.6ms\n",
      "Speed: 3.2ms preprocess, 15.6ms inference, 3.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 222.57, 219.98, 401.33, 706.37 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 451.84, 255.92, 623.54, 691.93 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 835.84, 262.99, 928.08, 506.12 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 901.61, 268.92, 1061.79, 702.51 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 9.0, Confidence: 0.75, Bounding Box: 1142.58, 17.30, 1258.89, 127.77 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 453.39, 220.89, 501.82, 372.00 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 2.0, Confidence: 0.62, Bounding Box: 1168.83, 527.51, 1272.32, 764.24 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 26.0, Confidence: 0.59, Bounding Box: 899.33, 311.01, 1038.29, 565.26 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 26.0, Confidence: 0.50, Bounding Box: 898.01, 411.60, 1028.42, 565.73 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 2.0, Confidence: 0.32, Bounding Box: 8.03, 270.51, 158.92, 406.41 for dataset/inria/Train/pos\\crop001642.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 372.29, 223.83, 455.13, 413.60 for dataset/inria/Train/pos\\crop001642.png\n",
      "\n",
      "0: 640x544 8 persons, 2 suitcases, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 251.94, 196.77, 405.00, 481.71 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 210.73, 150.04, 274.53, 346.92 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 1.12, 148.02, 143.06, 481.03 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 250.29, 165.94, 306.93, 336.18 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 105.96, 150.31, 205.93, 394.78 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 332.70, 159.03, 389.88, 241.82 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 28.0, Confidence: 0.35, Bounding Box: 143.36, 288.98, 217.03, 402.29 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 28.0, Confidence: 0.34, Bounding Box: 151.63, 319.13, 215.94, 401.53 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 259.16, 156.29, 278.08, 188.62 for dataset/inria/Train/pos\\crop001643.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 43.87, 202.29, 145.02, 481.76 for dataset/inria/Train/pos\\crop001643.png\n",
      "\n",
      "0: 640x416 2 persons, 1 handbag, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 130.14, 189.88, 254.68, 502.83 for dataset/inria/Train/pos\\crop001644.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 219.75, 195.99, 389.00, 641.00 for dataset/inria/Train/pos\\crop001644.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 217.98, 390.10, 384.56, 580.92 for dataset/inria/Train/pos\\crop001644.png\n",
      "\n",
      "0: 480x640 10 persons, 1 dog, 1 umbrella, 1 chair, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 345.41, 205.64, 434.61, 421.19 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 243.79, 225.86, 326.33, 434.74 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 425.80, 246.13, 495.35, 459.63 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 632.66, 245.57, 699.34, 425.07 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 479.82, 252.12, 524.91, 424.83 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 584.15, 261.24, 640.44, 424.59 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 537.65, 247.91, 576.82, 401.34 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 25.0, Confidence: 0.51, Bounding Box: 752.07, 206.21, 807.26, 335.85 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 56.0, Confidence: 0.49, Bounding Box: 739.44, 336.62, 804.34, 429.78 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 503.69, 244.56, 549.02, 417.45 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 16.0, Confidence: 0.42, Bounding Box: 201.13, 378.46, 228.46, 424.92 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 707.34, 265.96, 730.67, 347.23 for dataset/inria/Train/pos\\crop001646.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 10.10, 233.96, 50.10, 373.97 for dataset/inria/Train/pos\\crop001646.png\n",
      "\n",
      "0: 512x640 10 persons, 2 cars, 2 sheeps, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 254.31, 174.07, 303.19, 298.69 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 213.87, 169.93, 256.17, 305.25 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 2.0, Confidence: 0.63, Bounding Box: 427.32, 197.82, 488.46, 238.70 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 168.28, 190.79, 187.76, 245.83 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 127.89, 186.16, 149.95, 229.62 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 492.42, 188.39, 514.48, 227.40 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 64.08, 185.26, 82.20, 221.86 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 307.80, 185.33, 324.99, 228.82 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 2.0, Confidence: 0.43, Bounding Box: 332.13, 196.91, 376.00, 226.62 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 18.0, Confidence: 0.38, Bounding Box: 87.62, 312.05, 119.38, 329.34 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 150.01, 182.90, 171.20, 232.59 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 18.0, Confidence: 0.36, Bounding Box: 133.42, 311.37, 159.92, 325.89 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 299.71, 184.42, 324.88, 229.01 for dataset/inria/Train/pos\\crop001647.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 296.19, 184.39, 315.21, 229.08 for dataset/inria/Train/pos\\crop001647.png\n",
      "\n",
      "0: 512x640 4 persons, 2 cars, 1 handbag, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 0.57, 287.77, 250.07, 442.56 for dataset/inria/Train/pos\\crop001648.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 240.84, 260.76, 310.35, 468.15 for dataset/inria/Train/pos\\crop001648.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 459.05, 247.44, 558.55, 540.76 for dataset/inria/Train/pos\\crop001648.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 11.63, 313.60, 79.64, 420.38 for dataset/inria/Train/pos\\crop001648.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 345.58, 248.75, 436.45, 533.75 for dataset/inria/Train/pos\\crop001648.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 294.54, 375.83, 318.74, 431.97 for dataset/inria/Train/pos\\crop001648.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 789.70, 131.71, 886.69, 167.39 for dataset/inria/Train/pos\\crop001648.png\n",
      "\n",
      "0: 608x640 8 persons, 1 car, 2 trucks, 1 handbag, 1 chair, 16.5ms\n",
      "Speed: 4.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 114.16, 313.92, 221.07, 639.15 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 535.49, 322.50, 671.98, 755.69 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 286.49, 284.26, 408.42, 503.35 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 466.24, 274.31, 498.03, 371.98 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 26.0, Confidence: 0.66, Bounding Box: 600.21, 469.48, 668.14, 567.44 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 7.0, Confidence: 0.65, Bounding Box: 670.11, 176.24, 1049.44, 531.07 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 375.43, 276.93, 439.60, 452.27 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 358.22, 263.46, 394.85, 337.10 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 7.0, Confidence: 0.60, Bounding Box: 192.59, 200.30, 346.81, 394.26 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 2.0, Confidence: 0.54, Bounding Box: 669.31, 175.06, 1050.00, 530.08 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 635.75, 290.48, 676.87, 403.19 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 56.0, Confidence: 0.27, Bounding Box: 771.15, 833.27, 874.31, 958.95 for dataset/inria/Train/pos\\crop001649.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 609.16, 296.63, 641.84, 348.35 for dataset/inria/Train/pos\\crop001649.png\n",
      "\n",
      "0: 576x640 7 persons, 1 bus, 1 truck, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.95, Bounding Box: 444.04, 247.61, 591.86, 518.38 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 74.38, 195.32, 251.56, 517.11 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 204.72, 178.92, 310.29, 433.34 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 318.40, 170.84, 382.30, 319.38 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 181.69, 160.09, 228.95, 265.06 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 7.0, Confidence: 0.56, Bounding Box: 0.20, 74.74, 166.05, 335.44 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 536.28, 205.71, 591.54, 332.55 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 5.0, Confidence: 0.40, Bounding Box: 0.38, 74.50, 167.37, 340.42 for dataset/inria/Train/pos\\crop001650.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 241.58, 151.17, 285.28, 215.19 for dataset/inria/Train/pos\\crop001650.png\n",
      "\n",
      "0: 512x640 3 persons, 4 cars, 1 backpack, 1 handbag, 14.9ms\n",
      "Speed: 3.0ms preprocess, 14.9ms inference, 2.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 477.01, 279.80, 646.46, 722.87 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 266.13, 262.06, 406.34, 617.34 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 24.0, Confidence: 0.81, Bounding Box: 526.60, 348.47, 613.19, 493.43 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 433.11, 296.58, 517.63, 589.79 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 124.75, 279.88, 276.42, 397.77 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 2.0, Confidence: 0.59, Bounding Box: 638.67, 340.25, 1133.12, 564.75 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 2.0, Confidence: 0.56, Bounding Box: 955.05, 351.89, 1132.45, 578.83 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 0.04, 278.98, 48.58, 395.18 for dataset/inria/Train/pos\\crop001651.png\n",
      "Class: 26.0, Confidence: 0.45, Bounding Box: 252.89, 444.65, 295.65, 519.88 for dataset/inria/Train/pos\\crop001651.png\n",
      "\n",
      "0: 640x352 2 persons, 3 cars, 1 handbag, 14.3ms\n",
      "Speed: 13.0ms preprocess, 14.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 398.46, 207.60, 491.30, 448.62 for dataset/inria/Train/pos\\crop001652.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 38.41, 165.64, 236.94, 789.89 for dataset/inria/Train/pos\\crop001652.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 303.13, 220.41, 404.04, 297.64 for dataset/inria/Train/pos\\crop001652.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 184.39, 218.70, 250.64, 298.08 for dataset/inria/Train/pos\\crop001652.png\n",
      "Class: 2.0, Confidence: 0.47, Bounding Box: 76.61, 218.81, 132.15, 278.18 for dataset/inria/Train/pos\\crop001652.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 388.92, 331.28, 417.97, 383.28 for dataset/inria/Train/pos\\crop001652.png\n",
      "\n",
      "0: 544x640 8 persons, 1 handbag, 2 suitcases, 1 tv, 16.3ms\n",
      "Speed: 4.0ms preprocess, 16.3ms inference, 2.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 211.33, 263.92, 280.39, 439.91 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 79.06, 259.27, 127.34, 394.24 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 342.20, 269.47, 435.01, 512.75 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 426.47, 256.17, 530.94, 511.62 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 28.0, Confidence: 0.72, Bounding Box: 34.36, 366.02, 75.95, 478.88 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 0.00, 248.48, 74.69, 478.89 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 629.11, 285.77, 666.00, 367.47 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 0.00, 249.66, 31.16, 480.18 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 285.56, 265.03, 310.35, 319.37 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 28.0, Confidence: 0.33, Bounding Box: 61.24, 336.35, 92.91, 387.91 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 13.38, 273.93, 54.40, 369.44 for dataset/inria/Train/pos\\crop001655.png\n",
      "Class: 62.0, Confidence: 0.26, Bounding Box: 69.42, 185.44, 161.60, 210.43 for dataset/inria/Train/pos\\crop001655.png\n",
      "\n",
      "0: 480x640 3 persons, 1 suitcase, 2 chairs, 2 tvs, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 365.41, 181.35, 411.45, 358.47 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 56.0, Confidence: 0.62, Bounding Box: 694.07, 242.83, 789.00, 400.10 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 388.71, 181.13, 468.31, 363.64 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 28.0, Confidence: 0.45, Bounding Box: 258.79, 270.91, 308.89, 360.89 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 124.65, 186.87, 202.64, 290.17 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 62.0, Confidence: 0.32, Bounding Box: 619.97, 96.05, 681.59, 159.88 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 522.75, 312.55, 602.39, 431.24 for dataset/inria/Train/pos\\crop001656.png\n",
      "Class: 62.0, Confidence: 0.28, Bounding Box: 585.09, 95.44, 622.15, 154.33 for dataset/inria/Train/pos\\crop001656.png\n",
      "\n",
      "0: 416x640 9 persons, 1 handbag, 1 suitcase, 4 tvs, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 597.46, 165.29, 665.40, 339.53 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 463.00, 158.91, 512.71, 295.77 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 727.06, 169.21, 816.94, 416.08 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 141.36, 146.44, 234.29, 355.35 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 813.19, 157.80, 864.54, 411.42 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 359.01, 147.26, 460.34, 378.63 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 28.0, Confidence: 0.64, Bounding Box: 418.42, 252.13, 460.70, 379.75 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 62.0, Confidence: 0.53, Bounding Box: 284.30, 180.43, 328.49, 217.02 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 26.0, Confidence: 0.48, Bounding Box: 398.37, 173.92, 439.47, 270.58 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 62.0, Confidence: 0.46, Bounding Box: 281.19, 86.47, 327.29, 121.76 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 62.0, Confidence: 0.42, Bounding Box: 283.20, 131.57, 329.88, 170.57 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 669.96, 162.94, 696.07, 226.91 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 357.56, 153.84, 424.70, 379.88 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 62.0, Confidence: 0.32, Bounding Box: 455.79, 86.57, 545.57, 111.46 for dataset/inria/Train/pos\\crop001657.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 419.31, 159.91, 462.16, 272.09 for dataset/inria/Train/pos\\crop001657.png\n",
      "\n",
      "0: 512x640 3 persons, 2 cars, 1 truck, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 2.0, Confidence: 0.90, Bounding Box: 316.92, 208.32, 598.06, 355.99 for dataset/inria/Train/pos\\crop001662.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 100.77, 209.12, 149.06, 331.47 for dataset/inria/Train/pos\\crop001662.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 224.56, 221.85, 267.59, 340.09 for dataset/inria/Train/pos\\crop001662.png\n",
      "Class: 2.0, Confidence: 0.80, Bounding Box: 555.37, 250.39, 664.79, 356.15 for dataset/inria/Train/pos\\crop001662.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 183.98, 216.26, 228.59, 336.43 for dataset/inria/Train/pos\\crop001662.png\n",
      "Class: 7.0, Confidence: 0.39, Bounding Box: 509.94, 190.54, 664.33, 263.00 for dataset/inria/Train/pos\\crop001662.png\n",
      "\n",
      "0: 512x640 7 persons, 7 cars, 1 truck, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 2.0, Confidence: 0.85, Bounding Box: 0.14, 233.50, 113.47, 375.64 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 271.19, 170.53, 367.61, 384.16 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 528.25, 271.38, 590.26, 393.64 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 679.71, 285.48, 705.81, 356.51 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 594.84, 311.67, 637.83, 393.97 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 713.02, 289.67, 746.79, 358.50 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 2.0, Confidence: 0.54, Bounding Box: 381.80, 310.71, 457.95, 383.92 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 7.0, Confidence: 0.47, Bounding Box: 24.62, 209.25, 216.11, 298.41 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 497.11, 284.28, 530.71, 324.71 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 2.0, Confidence: 0.41, Bounding Box: 460.45, 323.01, 542.21, 390.07 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 76.42, 267.84, 274.33, 374.98 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 74.37, 268.61, 209.52, 373.88 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 2.0, Confidence: 0.33, Bounding Box: 224.15, 282.94, 293.69, 350.34 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 737.95, 351.37, 755.71, 399.54 for dataset/inria/Train/pos\\crop001663.png\n",
      "Class: 2.0, Confidence: 0.28, Bounding Box: 95.51, 320.19, 177.72, 372.89 for dataset/inria/Train/pos\\crop001663.png\n",
      "\n",
      "0: 512x640 6 persons, 1 traffic light, 1 bench, 1 horse, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 181.53, 295.58, 370.85, 685.11 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 721.55, 298.34, 799.60, 395.68 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 17.0, Confidence: 0.69, Bounding Box: 571.78, 192.02, 1153.70, 909.56 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 571.51, 190.72, 1154.63, 915.07 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 1094.70, 380.66, 1142.69, 464.63 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 981.92, 397.12, 1058.36, 565.19 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 187.79, 294.08, 227.21, 400.51 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 13.0, Confidence: 0.37, Bounding Box: 377.89, 339.99, 518.14, 431.60 for dataset/inria/Train/pos\\crop001665.png\n",
      "Class: 9.0, Confidence: 0.33, Bounding Box: 90.51, 183.95, 115.38, 247.69 for dataset/inria/Train/pos\\crop001665.png\n",
      "\n",
      "0: 640x608 5 persons, 2 handbags, 1 chair, 19.0ms\n",
      "Speed: 5.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 192.53, 349.56, 323.91, 693.76 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 413.80, 356.39, 551.48, 692.47 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 648.94, 402.66, 727.28, 517.19 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 26.0, Confidence: 0.57, Bounding Box: 492.02, 402.17, 557.07, 552.13 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 35.03, 393.46, 94.95, 491.47 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 56.0, Confidence: 0.47, Bounding Box: 710.53, 444.28, 785.18, 529.31 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 510.25, 472.05, 558.30, 551.08 for dataset/inria/Train/pos\\crop001666.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 695.65, 400.42, 783.75, 528.19 for dataset/inria/Train/pos\\crop001666.png\n",
      "\n",
      "0: 640x448 6 persons, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 155.40, 193.34, 403.85, 744.53 for dataset/inria/Train/pos\\crop001667.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 443.83, 256.33, 499.45, 406.13 for dataset/inria/Train/pos\\crop001667.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.00, 244.03, 70.15, 407.03 for dataset/inria/Train/pos\\crop001667.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 539.68, 258.27, 597.43, 406.35 for dataset/inria/Train/pos\\crop001667.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 218.33, 239.06, 253.63, 282.57 for dataset/inria/Train/pos\\crop001667.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 375.07, 279.29, 399.50, 335.81 for dataset/inria/Train/pos\\crop001667.png\n",
      "\n",
      "0: 448x640 11 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 441.07, 207.70, 483.29, 330.14 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 241.10, 23.54, 267.90, 101.26 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 182.24, 153.10, 220.37, 268.39 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 288.65, 112.20, 323.84, 200.39 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 675.47, 20.33, 709.45, 102.72 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 394.90, 120.35, 435.31, 203.58 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 786.25, 0.00, 803.82, 105.08 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 26.10, 80.87, 62.94, 157.64 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 367.09, 129.22, 399.32, 212.02 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 292.27, 29.62, 320.96, 105.17 for dataset/inria/Train/pos\\crop001668.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 602.55, 27.65, 636.67, 197.83 for dataset/inria/Train/pos\\crop001668.png\n",
      "\n",
      "0: 416x640 15 persons, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 173.10, 330.46, 217.64, 451.56 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 20.05, 235.56, 57.31, 321.14 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 518.46, 105.69, 554.64, 221.19 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 347.34, 99.78, 380.97, 175.56 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 407.98, 141.81, 443.29, 223.74 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 127.51, 241.28, 168.76, 324.71 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 392.68, 153.85, 418.51, 221.12 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 334.79, 161.87, 365.83, 250.52 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 582.55, 266.82, 613.10, 327.34 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 99.44, 251.38, 133.10, 332.59 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 25.27, 150.69, 55.25, 230.10 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 537.69, 208.57, 565.74, 282.16 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 208.78, 146.76, 232.75, 218.28 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 341.36, 240.96, 367.62, 321.60 for dataset/inria/Train/pos\\crop001669.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 91.95, 143.90, 130.16, 219.10 for dataset/inria/Train/pos\\crop001669.png\n",
      "\n",
      "0: 640x480 1 person, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 247.19, 296.40, 470.57, 789.57 for dataset/inria/Train/pos\\crop001671.png\n",
      "\n",
      "0: 640x480 2 persons, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 259.72, 266.28, 431.82, 775.73 for dataset/inria/Train/pos\\crop001672.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 432.98, 0.24, 632.64, 49.57 for dataset/inria/Train/pos\\crop001672.png\n",
      "\n",
      "0: 640x480 1 person, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 212.07, 172.69, 342.51, 480.93 for dataset/inria/Train/pos\\crop001673.png\n",
      "\n",
      "0: 640x480 1 person, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 249.06, 206.97, 453.64, 699.36 for dataset/inria/Train/pos\\crop001674.png\n",
      "\n",
      "0: 640x480 3 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 184.67, 302.46, 318.51, 594.41 for dataset/inria/Train/pos\\crop001675.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 0.00, 285.69, 96.55, 569.32 for dataset/inria/Train/pos\\crop001675.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 40.65, 251.39, 200.32, 446.45 for dataset/inria/Train/pos\\crop001675.png\n",
      "\n",
      "0: 608x640 2 persons, 16.2ms\n",
      "Speed: 3.0ms preprocess, 16.2ms inference, 4.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 73.27, 117.03, 157.84, 250.76 for dataset/inria/Train/pos\\crop001677.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 134.77, 120.63, 169.75, 220.20 for dataset/inria/Train/pos\\crop001677.png\n",
      "\n",
      "0: 544x640 3 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 80.80, 117.62, 116.14, 190.31 for dataset/inria/Train/pos\\crop001678.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 178.83, 115.01, 232.38, 191.99 for dataset/inria/Train/pos\\crop001678.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 140.50, 97.55, 182.56, 190.81 for dataset/inria/Train/pos\\crop001678.png\n",
      "\n",
      "0: 544x640 8 persons, 1 truck, 15.1ms\n",
      "Speed: 3.0ms preprocess, 15.1ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 790.89, 317.99, 997.47, 804.72 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 231.74, 273.14, 364.19, 638.74 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 559.12, 298.88, 698.19, 683.90 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 469.18, 310.79, 583.65, 645.59 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 682.26, 294.43, 792.21, 555.37 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 170.13, 274.85, 248.67, 518.71 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 7.0, Confidence: 0.75, Bounding Box: 0.26, 204.79, 153.99, 458.96 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 356.76, 290.72, 406.59, 416.54 for dataset/inria/Train/pos\\crop001679.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 412.85, 289.38, 452.32, 381.88 for dataset/inria/Train/pos\\crop001679.png\n",
      "\n",
      "0: 640x544 9 persons, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.58, 155.25, 152.10, 505.46 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 139.79, 187.55, 217.60, 381.61 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 317.08, 222.59, 415.00, 505.24 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 226.30, 188.73, 289.53, 327.00 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 305.94, 181.44, 322.65, 225.32 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 346.59, 184.66, 370.38, 249.04 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 322.58, 180.69, 338.46, 227.15 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 0.36, 172.62, 26.84, 251.23 for dataset/inria/Train/pos\\crop001680.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 336.80, 181.02, 351.56, 240.27 for dataset/inria/Train/pos\\crop001680.png\n",
      "\n",
      "0: 640x384 4 persons, 1 bicycle, 1 handbag, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 159.19, 225.53, 310.84, 611.48 for dataset/inria/Train/pos\\crop001681.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 308.07, 255.35, 458.00, 780.84 for dataset/inria/Train/pos\\crop001681.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.00, 208.40, 168.41, 782.00 for dataset/inria/Train/pos\\crop001681.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 82.56, 213.27, 124.16, 306.20 for dataset/inria/Train/pos\\crop001681.png\n",
      "Class: 1.0, Confidence: 0.55, Bounding Box: 274.53, 348.64, 353.59, 478.42 for dataset/inria/Train/pos\\crop001681.png\n",
      "Class: 26.0, Confidence: 0.42, Bounding Box: 0.00, 287.86, 103.30, 524.84 for dataset/inria/Train/pos\\crop001681.png\n",
      "\n",
      "0: 640x384 1 person, 1 snowboard, 12.1ms\n",
      "Speed: 2.0ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 71.09, 99.17, 157.90, 320.95 for dataset/inria/Train/pos\\crop001685.png\n",
      "Class: 31.0, Confidence: 0.80, Bounding Box: 33.56, 303.04, 177.07, 326.54 for dataset/inria/Train/pos\\crop001685.png\n",
      "\n",
      "0: 640x448 2 persons, 1 skis, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 72.98, 105.84, 105.71, 166.84 for dataset/inria/Train/pos\\crop001686.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 0.00, 70.33, 47.73, 256.28 for dataset/inria/Train/pos\\crop001686.png\n",
      "Class: 30.0, Confidence: 0.33, Bounding Box: 56.41, 161.85, 114.02, 169.75 for dataset/inria/Train/pos\\crop001686.png\n",
      "\n",
      "0: 640x480 1 person, 1 motorcycle, 16.5ms\n",
      "Speed: 3.0ms preprocess, 16.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 143.38, 131.25, 347.08, 701.99 for dataset/inria/Train/pos\\crop001687.png\n",
      "Class: 3.0, Confidence: 0.70, Bounding Box: 228.60, 451.16, 689.84, 956.97 for dataset/inria/Train/pos\\crop001687.png\n",
      "\n",
      "0: 640x384 7 persons, 1 car, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 2.0, Confidence: 0.92, Bounding Box: 311.56, 431.16, 529.99, 957.62 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 37.14, 146.12, 244.98, 753.46 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 273.35, 106.02, 409.86, 542.55 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 164.27, 92.78, 313.52, 662.20 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 365.00, 125.95, 472.43, 470.82 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 467.56, 88.10, 529.41, 436.38 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 438.67, 142.72, 513.10, 275.19 for dataset/inria/Train/pos\\crop001689.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 0.00, 128.18, 58.06, 795.00 for dataset/inria/Train/pos\\crop001689.png\n",
      "\n",
      "0: 480x640 17 persons, 2 cars, 1 truck, 1 traffic light, 1 handbag, 1 surfboard, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 256.95, 219.22, 354.06, 507.36 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 348.05, 176.31, 459.78, 454.30 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 297.36, 5.02, 355.65, 141.58 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 795.80, 189.37, 889.73, 476.72 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 582.97, 19.78, 661.80, 226.12 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 536.29, 213.15, 641.12, 480.60 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 498.80, 170.86, 588.98, 450.76 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 0.00, 471.11, 101.63, 665.00 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 651.75, 286.18, 770.17, 571.79 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 0.12, 135.45, 524.61, 457.73 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 431.37, 444.89, 558.57, 663.20 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 799.68, 101.78, 856.34, 268.40 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 7.0, Confidence: 0.53, Bounding Box: 437.55, 0.00, 817.64, 208.08 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 833.28, 41.02, 889.78, 201.94 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 254.45, 99.51, 298.20, 139.27 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 370.80, 22.36, 456.71, 153.45 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 2.0, Confidence: 0.45, Bounding Box: 440.73, 0.00, 808.49, 196.44 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 488.49, 333.91, 528.46, 395.03 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 546.89, 14.44, 596.17, 194.02 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 37.0, Confidence: 0.39, Bounding Box: 474.24, 547.69, 888.46, 665.00 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 9.0, Confidence: 0.37, Bounding Box: 830.81, 0.00, 870.37, 58.07 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 694.19, 179.81, 791.54, 372.25 for dataset/inria/Train/pos\\crop001690.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 817.16, 387.43, 889.73, 555.78 for dataset/inria/Train/pos\\crop001690.png\n",
      "\n",
      "0: 640x544 20 persons, 3 cars, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 190.27, 231.42, 309.99, 518.56 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.55, 58.67, 55.24, 264.08 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 2.0, Confidence: 0.79, Bounding Box: 0.00, 12.51, 213.87, 250.24 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 211.25, 440.87, 316.22, 592.23 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 44.71, 323.52, 166.62, 605.16 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 227.35, 77.13, 303.62, 232.83 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 2.0, Confidence: 0.71, Bounding Box: 355.16, 304.73, 542.00, 636.64 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 279.43, 460.69, 374.00, 605.39 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 189.30, 143.09, 248.80, 309.83 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 349.00, 213.13, 436.70, 459.09 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 450.59, 169.34, 534.02, 284.00 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 308.31, 128.48, 362.47, 357.07 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 417.15, 46.19, 484.99, 250.71 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 87.22, 221.99, 183.56, 413.11 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 0.00, 586.95, 539.78, 664.54 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 0.00, 251.03, 33.12, 516.99 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 309.36, 128.63, 362.61, 270.62 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 458.90, 15.15, 511.71, 203.98 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 88.29, 219.81, 171.51, 348.69 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 353.01, 46.22, 419.08, 263.31 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 224.03, 5.39, 267.06, 98.11 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 24.64, 237.91, 92.65, 506.33 for dataset/inria/Train/pos\\crop001691.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 173.07, 1.90, 229.83, 95.40 for dataset/inria/Train/pos\\crop001691.png\n",
      "\n",
      "0: 640x224 7 persons, 2 cars, 46.0ms\n",
      "Speed: 3.0ms preprocess, 46.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 35.01, 216.57, 163.82, 559.70 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 177.81, 288.88, 292.00, 653.94 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 137.24, 655.22, 301.77, 862.00 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 2.0, Confidence: 0.84, Bounding Box: 0.00, 509.33, 184.11, 776.87 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 251.77, 51.66, 301.59, 291.94 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 2.0, Confidence: 0.76, Bounding Box: 0.42, 168.61, 299.87, 459.02 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 150.29, 2.87, 241.70, 217.60 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 0.00, 32.68, 74.40, 163.22 for dataset/inria/Train/pos\\crop001692.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 92.31, 0.36, 153.58, 161.16 for dataset/inria/Train/pos\\crop001692.png\n",
      "\n",
      "0: 640x480 16 persons, 1 car, 1 handbag, 15.5ms\n",
      "Speed: 2.0ms preprocess, 15.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 189.90, 436.11, 318.12, 630.50 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 346.26, 512.48, 451.19, 630.75 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 307.14, 226.41, 394.72, 439.91 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 61.89, 452.64, 195.94, 630.72 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 245.02, 179.16, 314.16, 385.71 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 217.66, 103.07, 282.15, 284.15 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 401.89, 262.95, 454.67, 511.57 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 297.09, 166.94, 368.69, 296.80 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 137.95, 255.29, 228.15, 383.23 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 41.40, 304.11, 104.06, 374.54 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 2.0, Confidence: 0.39, Bounding Box: 9.72, 384.55, 453.12, 630.69 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 0.21, 76.21, 45.96, 164.58 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 0.00, 75.92, 43.84, 256.02 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 168.89, 212.42, 193.73, 256.25 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 0.00, 75.97, 46.43, 206.38 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 401.81, 161.12, 449.42, 238.94 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 163.91, 115.49, 205.02, 219.51 for dataset/inria/Train/pos\\crop001693.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 127.17, 169.89, 176.26, 322.25 for dataset/inria/Train/pos\\crop001693.png\n",
      "\n",
      "0: 544x640 20 persons, 4 cars, 1 motorcycle, 1 suitcase, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 109.62, 557.93, 318.11, 813.24 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 609.04, 0.00, 971.00, 289.70 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 255.05, 357.22, 452.60, 796.26 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 445.86, 633.88, 611.28, 814.75 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 457.00, 0.73, 551.58, 259.55 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 568.80, 2.39, 627.15, 182.07 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.00, 309.87, 103.92, 607.41 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 563.49, 301.13, 742.70, 757.84 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 902.92, 48.49, 970.84, 335.56 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 303.50, 78.82, 397.19, 275.73 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 473.21, 227.39, 599.37, 640.31 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 237.28, 101.38, 310.30, 238.16 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 138.74, 113.35, 237.98, 214.78 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 221.67, 701.44, 392.37, 813.50 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 542.79, 410.90, 701.82, 812.29 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 0.00, 481.62, 94.42, 813.08 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 305.82, 4.25, 377.27, 106.92 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 1.07, 189.81, 459.84, 648.12 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 356.55, 244.41, 457.98, 453.91 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 48.18, 2.93, 125.31, 83.96 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 228.18, 4.40, 288.33, 155.51 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 3.0, Confidence: 0.34, Bounding Box: 882.42, 542.72, 970.73, 810.39 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 28.0, Confidence: 0.32, Bounding Box: 72.64, 430.98, 142.53, 542.36 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 139.51, 3.21, 208.06, 74.62 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 1.89, 187.56, 583.64, 733.03 for dataset/inria/Train/pos\\crop001694.png\n",
      "Class: 2.0, Confidence: 0.26, Bounding Box: 882.99, 540.70, 970.73, 809.58 for dataset/inria/Train/pos\\crop001694.png\n",
      "\n",
      "0: 480x640 22 persons, 2 cars, 1 handbag, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 390.38, 337.48, 510.49, 571.91 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 246.52, 424.31, 359.84, 574.71 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 179.00, 156.10, 257.49, 364.30 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 97.00, 307.51, 182.40, 572.12 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 132.02, 351.96, 231.11, 572.90 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 0.00, 330.24, 54.37, 572.30 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 547.14, 311.88, 650.26, 555.72 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 506.96, 229.38, 565.30, 451.43 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 426.94, 179.14, 504.29, 338.49 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 2.0, Confidence: 0.65, Bounding Box: 35.48, 108.42, 422.82, 345.79 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 391.52, 241.66, 449.81, 409.36 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 26.0, Confidence: 0.62, Bounding Box: 83.65, 469.19, 122.14, 534.01 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 651.33, 269.82, 727.12, 383.41 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 621.69, 146.18, 682.95, 357.36 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 288.38, 318.55, 387.59, 510.90 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 660.06, 115.52, 716.02, 292.27 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 0.00, 143.07, 67.01, 295.80 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 283.52, 57.27, 332.50, 120.89 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 143.95, 152.69, 190.68, 334.00 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 725.54, 322.28, 787.27, 394.22 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 226.01, 336.42, 290.86, 554.03 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 335.44, 52.68, 375.32, 124.44 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 0.00, 289.84, 118.19, 497.68 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 291.17, 318.83, 372.89, 447.85 for dataset/inria/Train/pos\\crop001695.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 712.46, 286.51, 760.87, 367.98 for dataset/inria/Train/pos\\crop001695.png\n",
      "\n",
      "0: 512x640 15 persons, 1 motorcycle, 1 backpack, 1 chair, 15.6ms\n",
      "Speed: 3.0ms preprocess, 15.6ms inference, 2.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 813.09, 159.44, 959.57, 691.88 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 359.96, 181.70, 546.24, 552.29 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 297.54, 195.01, 385.76, 532.71 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 934.41, 206.65, 1012.91, 695.06 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 48.01, 157.64, 163.95, 250.15 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 626.25, 199.13, 794.49, 583.13 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 3.0, Confidence: 0.46, Bounding Box: 508.70, 325.56, 805.10, 581.06 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 959.02, 204.33, 1012.65, 698.42 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 56.0, Confidence: 0.43, Bounding Box: 0.00, 248.40, 233.01, 501.15 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 643.12, 186.29, 717.51, 327.08 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 237.87, 220.54, 308.55, 522.44 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 458.32, 190.16, 521.98, 312.42 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 191.23, 207.52, 259.43, 509.24 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 599.74, 199.42, 648.33, 321.90 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 245.95, 198.37, 323.26, 272.69 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 7.81, 182.22, 64.02, 249.32 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 24.0, Confidence: 0.27, Bounding Box: 707.34, 268.59, 794.42, 399.29 for dataset/inria/Train/pos\\crop001696.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 762.45, 203.66, 832.74, 344.48 for dataset/inria/Train/pos\\crop001696.png\n",
      "\n",
      "0: 640x480 9 persons, 1 car, 14.1ms\n",
      "Speed: 3.1ms preprocess, 14.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 255.14, 224.14, 465.32, 489.95 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 446.09, 230.39, 619.44, 439.70 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 2.0, Confidence: 0.76, Bounding Box: 194.73, 371.71, 698.00, 954.39 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 57.42, 200.85, 267.12, 768.56 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 0.00, 226.31, 122.29, 815.75 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 590.88, 234.06, 690.97, 429.67 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 390.03, 247.81, 445.53, 360.62 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 591.42, 235.40, 685.72, 370.23 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 536.15, 236.41, 597.86, 316.09 for dataset/inria/Train/pos\\crop001697.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 219.87, 248.21, 268.75, 336.60 for dataset/inria/Train/pos\\crop001697.png\n",
      "\n",
      "0: 512x640 9 persons, 2 cars, 15.9ms\n",
      "Speed: 4.0ms preprocess, 15.9ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 2.0, Confidence: 0.91, Bounding Box: 350.39, 245.09, 727.89, 552.54 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 0.00, 178.28, 104.19, 507.95 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 224.75, 151.11, 341.79, 415.48 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 69.57, 161.70, 221.22, 456.05 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 398.19, 168.70, 486.26, 410.51 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 2.0, Confidence: 0.53, Bounding Box: 511.16, 183.17, 728.51, 282.05 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 178.85, 169.38, 232.37, 421.07 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 134.68, 137.86, 183.59, 257.29 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 339.63, 155.80, 378.85, 262.74 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 340.09, 155.79, 379.77, 341.56 for dataset/inria/Train/pos\\crop001698.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 181.21, 173.75, 232.15, 275.20 for dataset/inria/Train/pos\\crop001698.png\n",
      "\n",
      "0: 640x448 11 persons, 1 car, 1 handbag, 15.0ms\n",
      "Speed: 12.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 0.00, 253.06, 106.93, 663.93 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 205.22, 203.94, 370.83, 751.37 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 2.0, Confidence: 0.75, Bounding Box: 302.02, 520.50, 669.00, 923.39 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 424.59, 275.35, 518.40, 525.58 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 350.96, 293.65, 428.51, 545.24 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 146.15, 267.41, 245.14, 647.83 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 66.31, 283.72, 146.09, 574.99 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 148.34, 264.41, 229.71, 581.40 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 26.0, Confidence: 0.35, Bounding Box: 400.91, 386.19, 474.72, 455.53 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 85.27, 249.35, 151.72, 363.42 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 521.75, 278.66, 583.07, 452.44 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 154.56, 311.74, 255.00, 679.99 for dataset/inria/Train/pos\\crop001699.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 339.58, 277.03, 389.14, 545.33 for dataset/inria/Train/pos\\crop001699.png\n",
      "\n",
      "0: 640x512 9 persons, 1 umbrella, 2 handbags, 17.1ms\n",
      "Speed: 3.0ms preprocess, 17.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 253.04, 241.96, 383.27, 605.32 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 144.66, 271.20, 259.71, 635.84 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 0.00, 134.46, 179.35, 849.00 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 0.86, 145.63, 176.83, 586.25 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 389.40, 256.35, 450.73, 502.02 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 332.85, 251.31, 406.82, 459.16 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 26.0, Confidence: 0.44, Bounding Box: 214.66, 357.81, 318.11, 506.58 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 25.0, Confidence: 0.40, Bounding Box: 106.07, 600.54, 649.00, 848.26 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 212.74, 406.99, 312.89, 508.77 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 234.81, 248.12, 275.11, 333.53 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 454.94, 222.16, 475.15, 251.20 for dataset/inria/Train/pos\\crop001700.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 352.47, 249.39, 408.78, 467.31 for dataset/inria/Train/pos\\crop001700.png\n",
      "\n",
      "0: 640x320 4 persons, 3 cars, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 251.31, 200.98, 392.72, 653.35 for dataset/inria/Train/pos\\crop001701.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 355.64, 225.13, 434.00, 543.19 for dataset/inria/Train/pos\\crop001701.png\n",
      "Class: 2.0, Confidence: 0.75, Bounding Box: 0.00, 513.23, 197.28, 868.83 for dataset/inria/Train/pos\\crop001701.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 191.93, 241.16, 280.15, 527.88 for dataset/inria/Train/pos\\crop001701.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 2.14, 283.23, 98.45, 476.42 for dataset/inria/Train/pos\\crop001701.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 108.97, 243.52, 167.74, 440.69 for dataset/inria/Train/pos\\crop001701.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 2.81, 251.22, 136.02, 477.20 for dataset/inria/Train/pos\\crop001701.png\n",
      "\n",
      "0: 640x640 9 persons, 2 cars, 21.5ms\n",
      "Speed: 6.0ms preprocess, 21.5ms inference, 15.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 393.69, 229.23, 472.14, 498.27 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 110.51, 235.83, 209.91, 474.84 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 211.23, 253.13, 302.50, 482.64 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 74.97, 228.70, 134.28, 460.52 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 0.00, 220.52, 79.80, 438.00 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 2.0, Confidence: 0.70, Bounding Box: 0.00, 387.93, 677.76, 687.20 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 311.93, 247.96, 375.62, 476.68 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 2.0, Confidence: 0.47, Bounding Box: 452.22, 278.25, 584.83, 471.84 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 553.82, 236.96, 614.57, 433.04 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 38.17, 228.41, 81.03, 349.05 for dataset/inria/Train/pos\\crop001702.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 358.98, 241.09, 420.15, 486.35 for dataset/inria/Train/pos\\crop001702.png\n",
      "\n",
      "0: 640x480 10 persons, 1 handbag, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 1.59, 302.01, 152.25, 883.57 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 357.40, 288.64, 504.96, 749.69 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 201.76, 300.91, 337.19, 739.59 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 549.93, 308.56, 684.00, 958.94 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 120.74, 330.47, 231.40, 709.92 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 510.80, 273.35, 628.53, 586.77 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 624.72, 293.31, 682.93, 376.69 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 56.36, 297.94, 181.91, 539.36 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 60.03, 305.43, 186.47, 728.23 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 609.49, 330.15, 633.93, 370.40 for dataset/inria/Train/pos\\crop001703.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 523.96, 529.10, 563.35, 649.86 for dataset/inria/Train/pos\\crop001703.png\n",
      "\n",
      "0: 640x512 10 persons, 2 cars, 1 backpack, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 41.73, 243.46, 197.80, 558.97 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 408.60, 232.72, 495.94, 542.78 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 329.88, 234.79, 419.76, 534.46 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 164.31, 243.46, 232.98, 431.88 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 0.00, 535.90, 460.33, 700.63 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 476.18, 238.89, 537.33, 582.32 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 287.10, 258.67, 358.98, 524.54 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 165.81, 245.37, 256.43, 527.82 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 2.0, Confidence: 0.46, Bounding Box: 0.00, 451.62, 459.74, 700.55 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 0.97, 221.88, 75.92, 469.46 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 7.77, 223.31, 77.57, 340.85 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 0.47, 273.90, 60.29, 472.14 for dataset/inria/Train/pos\\crop001705.png\n",
      "Class: 24.0, Confidence: 0.26, Bounding Box: 287.13, 301.13, 341.43, 390.16 for dataset/inria/Train/pos\\crop001705.png\n",
      "\n",
      "0: 576x640 13 persons, 1 tv, 18.0ms\n",
      "Speed: 12.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 455.96, 233.03, 654.22, 703.06 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 393.75, 218.83, 477.04, 488.88 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 263.28, 200.65, 373.01, 549.77 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 640.36, 222.81, 812.61, 702.39 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 196.26, 212.43, 268.70, 468.00 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 580.58, 215.27, 645.23, 353.19 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 356.84, 233.59, 417.38, 469.26 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 0.00, 132.16, 185.14, 668.00 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 345.35, 216.75, 377.47, 278.26 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 181.24, 213.51, 221.23, 458.37 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 643.22, 216.43, 741.25, 407.04 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 248.48, 218.65, 283.06, 278.60 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 631.79, 216.71, 741.00, 617.44 for dataset/inria/Train/pos\\crop001707.png\n",
      "Class: 62.0, Confidence: 0.25, Bounding Box: 0.29, 137.40, 191.13, 428.35 for dataset/inria/Train/pos\\crop001707.png\n",
      "\n",
      "0: 640x352 7 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 148.53, 205.52, 335.96, 717.44 for dataset/inria/Train/pos\\crop001708.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 0.71, 173.45, 86.05, 481.91 for dataset/inria/Train/pos\\crop001708.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 322.28, 202.78, 491.58, 761.43 for dataset/inria/Train/pos\\crop001708.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 102.58, 188.22, 179.65, 435.51 for dataset/inria/Train/pos\\crop001708.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 73.04, 204.12, 122.61, 408.82 for dataset/inria/Train/pos\\crop001708.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 60.17, 189.12, 89.23, 245.32 for dataset/inria/Train/pos\\crop001708.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 269.95, 187.33, 329.06, 309.46 for dataset/inria/Train/pos\\crop001708.png\n",
      "\n",
      "0: 544x640 2 persons, 8 cars, 1 traffic light, 15.3ms\n",
      "Speed: 3.9ms preprocess, 15.3ms inference, 3.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 2.0, Confidence: 0.91, Bounding Box: 0.05, 198.88, 104.53, 286.29 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 261.64, 115.35, 312.05, 214.42 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.84, Bounding Box: 297.26, 119.74, 400.63, 164.85 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 74.65, 98.30, 125.40, 190.56 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.75, Bounding Box: 392.29, 113.97, 456.91, 163.84 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 9.0, Confidence: 0.60, Bounding Box: 399.40, 0.00, 447.31, 86.27 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 229.63, 119.66, 283.09, 145.16 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.47, Bounding Box: 287.87, 118.17, 334.91, 145.51 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.41, Bounding Box: 287.15, 117.08, 353.56, 140.17 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 0.03, 118.63, 57.88, 155.31 for dataset/inria/Train/pos\\crop001709.png\n",
      "Class: 2.0, Confidence: 0.28, Bounding Box: 287.35, 117.74, 335.43, 135.96 for dataset/inria/Train/pos\\crop001709.png\n",
      "\n",
      "0: 576x640 6 persons, 3 cars, 1 backpack, 18.6ms\n",
      "Speed: 4.5ms preprocess, 18.6ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Class: 2.0, Confidence: 0.86, Bounding Box: 0.00, 227.11, 131.72, 323.92 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 355.40, 192.42, 389.46, 288.51 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 134.51, 190.81, 228.25, 408.23 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 210.06, 195.49, 262.59, 354.80 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 295.12, 200.48, 321.20, 277.38 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 430.46, 214.96, 465.69, 280.27 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 111.23, 178.71, 137.85, 265.74 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 24.0, Confidence: 0.41, Bounding Box: 107.53, 231.90, 160.39, 342.75 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 30.84, 205.25, 87.53, 243.27 for dataset/inria/Train/pos\\crop001710.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 4.90, 205.31, 85.76, 247.29 for dataset/inria/Train/pos\\crop001710.png\n",
      "\n",
      "0: 608x640 16 persons, 15.6ms\n",
      "Speed: 4.0ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 246.27, 297.95, 435.85, 720.76 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 187.20, 314.63, 302.67, 614.65 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 513.79, 296.65, 643.92, 649.84 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 417.96, 303.23, 495.65, 563.87 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 147.95, 272.97, 235.35, 535.04 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 19.31, 277.54, 105.43, 503.89 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 612.90, 291.87, 694.36, 403.68 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 99.29, 295.19, 161.73, 505.35 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 743.69, 326.20, 822.17, 403.63 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 841.51, 289.21, 965.62, 698.22 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 670.32, 303.44, 734.94, 401.96 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 479.68, 298.41, 519.49, 503.13 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 812.41, 350.72, 852.98, 402.57 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 359.67, 307.05, 396.76, 370.58 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 480.25, 298.67, 516.55, 367.86 for dataset/inria/Train/pos\\crop001711.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 483.25, 300.31, 531.44, 499.31 for dataset/inria/Train/pos\\crop001711.png\n",
      "\n",
      "0: 640x640 4 persons, 21.0ms\n",
      "Speed: 6.0ms preprocess, 21.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 407.21, 262.21, 522.25, 606.76 for dataset/inria/Train/pos\\crop001712.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 214.71, 361.87, 335.57, 682.06 for dataset/inria/Train/pos\\crop001712.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 648.39, 385.93, 691.00, 701.22 for dataset/inria/Train/pos\\crop001712.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 206.83, 230.64, 337.33, 679.05 for dataset/inria/Train/pos\\crop001712.png\n",
      "\n",
      "0: 640x320 8 persons, 1 backpack, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 60.58, 235.05, 320.26, 787.84 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 280.79, 247.47, 451.92, 649.15 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 3.87, 267.27, 123.27, 778.02 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 216.68, 260.69, 358.96, 623.49 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 0.90, 192.54, 68.47, 361.58 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 176.48, 202.19, 251.04, 333.44 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 161.15, 248.54, 246.12, 412.91 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 24.0, Confidence: 0.29, Bounding Box: 333.72, 530.44, 436.95, 657.42 for dataset/inria/Train/pos\\crop001713.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 176.76, 202.56, 228.02, 259.00 for dataset/inria/Train/pos\\crop001713.png\n",
      "\n",
      "0: 640x320 4 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 0.00, 284.70, 83.69, 765.74 for dataset/inria/Train/pos\\crop001714.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 145.94, 280.98, 242.78, 667.45 for dataset/inria/Train/pos\\crop001714.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 229.77, 168.15, 475.00, 960.00 for dataset/inria/Train/pos\\crop001714.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 222.97, 155.22, 327.07, 749.13 for dataset/inria/Train/pos\\crop001714.png\n",
      "\n",
      "0: 448x640 8 persons, 2 potted plants, 21.0ms\n",
      "Speed: 3.0ms preprocess, 21.0ms inference, 8.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 195.56, 280.81, 344.58, 444.81 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 58.0, Confidence: 0.88, Bounding Box: 288.33, 124.74, 444.92, 327.91 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 620.89, 35.37, 660.80, 156.99 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 214.45, 122.25, 275.72, 290.20 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 429.41, 364.83, 552.57, 444.66 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 542.34, 289.47, 660.51, 444.80 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 531.49, 382.46, 619.81, 446.14 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 472.34, 16.19, 502.05, 98.23 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 58.0, Confidence: 0.47, Bounding Box: 555.52, 32.10, 610.08, 170.87 for dataset/inria/Train/pos\\crop001717.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 0.08, 371.23, 56.29, 443.83 for dataset/inria/Train/pos\\crop001717.png\n",
      "\n",
      "0: 640x384 6 persons, 1 handbag, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 147.22, 372.14, 244.85, 568.48 for dataset/inria/Train/pos\\crop001720.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 266.64, 384.56, 326.32, 577.49 for dataset/inria/Train/pos\\crop001720.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 209.23, 392.35, 245.91, 508.99 for dataset/inria/Train/pos\\crop001720.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 384.28, 371.03, 411.98, 469.11 for dataset/inria/Train/pos\\crop001720.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 310.04, 367.07, 346.83, 504.77 for dataset/inria/Train/pos\\crop001720.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 260.04, 424.45, 286.70, 466.33 for dataset/inria/Train/pos\\crop001720.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 409.62, 374.02, 430.98, 498.97 for dataset/inria/Train/pos\\crop001720.png\n",
      "\n",
      "0: 640x640 1 person, 28.0ms\n",
      "Speed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 97.55, 85.55, 134.29, 177.79 for dataset/inria/Train/pos\\crop001721.png\n",
      "\n",
      "0: 640x640 10 persons, 3 boats, 21.0ms\n",
      "Speed: 4.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 197.74, 148.68, 281.85, 410.51 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 115.34, 183.68, 207.62, 405.24 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 353.30, 159.27, 412.07, 287.99 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 305.56, 169.96, 376.97, 396.12 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 412.89, 180.21, 476.03, 309.65 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 0.00, 202.08, 47.60, 345.87 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 498.47, 210.29, 540.51, 321.48 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 39.99, 208.17, 77.29, 346.15 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 293.07, 220.27, 318.37, 331.46 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 8.0, Confidence: 0.41, Bounding Box: 363.02, 273.83, 584.83, 397.69 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 8.0, Confidence: 0.36, Bounding Box: 387.64, 29.88, 475.40, 226.64 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 8.0, Confidence: 0.32, Bounding Box: 0.00, 276.72, 549.69, 514.53 for dataset/inria/Train/pos\\crop001806.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 479.96, 182.14, 521.14, 314.69 for dataset/inria/Train/pos\\crop001806.png\n",
      "\n",
      "0: 640x512 10 persons, 1 boat, 2 kites, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 228.00, 154.12, 270.52, 313.75 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 8.0, Confidence: 0.80, Bounding Box: 0.00, 234.66, 329.55, 374.49 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 33.0, Confidence: 0.77, Bounding Box: 304.12, 52.40, 328.73, 80.69 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 256.70, 158.80, 298.08, 317.35 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 179.58, 154.01, 231.37, 292.12 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 92.98, 327.30, 376.27, 480.25 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 358.93, 189.06, 393.71, 262.88 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 33.0, Confidence: 0.46, Bounding Box: 368.59, 77.64, 395.66, 134.20 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 0.43, 187.10, 42.34, 273.35 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 46.86, 194.84, 82.01, 250.32 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 130.97, 157.36, 194.21, 283.80 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 1.75, 186.21, 42.86, 239.00 for dataset/inria/Train/pos\\crop001807.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 80.78, 217.72, 109.55, 266.62 for dataset/inria/Train/pos\\crop001807.png\n",
      "\n",
      "0: 640x416 5 persons, 1 surfboard, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 183.92, 205.99, 280.68, 537.92 for dataset/inria/Train/pos\\crop001808.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 248.86, 274.58, 332.05, 536.90 for dataset/inria/Train/pos\\crop001808.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 0.27, 372.01, 44.84, 533.15 for dataset/inria/Train/pos\\crop001808.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 159.38, 273.57, 220.77, 528.21 for dataset/inria/Train/pos\\crop001808.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 159.78, 274.38, 205.39, 504.87 for dataset/inria/Train/pos\\crop001808.png\n",
      "Class: 37.0, Confidence: 0.26, Bounding Box: 102.22, 488.94, 185.00, 533.74 for dataset/inria/Train/pos\\crop001808.png\n",
      "\n",
      "0: 640x480 8 persons, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 102.85, 230.18, 212.26, 478.58 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 23.61, 329.75, 108.14, 478.44 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 288.71, 200.86, 383.82, 479.93 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 418.91, 320.83, 496.30, 450.14 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 242.30, 224.27, 329.05, 480.04 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 402.61, 317.69, 447.52, 422.31 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 387.78, 291.79, 442.44, 387.55 for dataset/inria/Train/pos\\crop001809.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 227.61, 204.03, 383.64, 480.64 for dataset/inria/Train/pos\\crop001809.png\n",
      "\n",
      "0: 640x448 4 persons, 1 surfboard, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 206.92, 244.96, 313.28, 574.28 for dataset/inria/Train/pos\\crop001810.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 287.23, 313.69, 369.49, 561.72 for dataset/inria/Train/pos\\crop001810.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 36.95, 288.28, 142.62, 477.21 for dataset/inria/Train/pos\\crop001810.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 0.00, 254.43, 73.12, 488.38 for dataset/inria/Train/pos\\crop001810.png\n",
      "Class: 37.0, Confidence: 0.37, Bounding Box: 300.22, 550.72, 458.02, 578.49 for dataset/inria/Train/pos\\crop001810.png\n",
      "\n",
      "0: 640x288 1 person, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 138.30, 177.43, 248.62, 526.05 for dataset/inria/Train/pos\\crop001811.png\n",
      "\n",
      "0: 512x640 4 persons, 1 umbrella, 2 surfboards, 2 chairs, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 292.86, 156.09, 358.56, 357.73 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 319.62, 350.40, 545.90, 432.82 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 176.41, 193.01, 216.75, 345.08 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 37.0, Confidence: 0.49, Bounding Box: 66.00, 343.44, 425.26, 404.70 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 37.0, Confidence: 0.45, Bounding Box: 0.09, 351.55, 46.46, 384.13 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 56.0, Confidence: 0.32, Bounding Box: 342.59, 259.18, 388.98, 351.47 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 25.0, Confidence: 0.30, Bounding Box: 355.05, 106.19, 545.62, 185.07 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 219.83, 268.12, 283.00, 339.44 for dataset/inria/Train/pos\\crop001812.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 14.98, 261.14, 40.88, 317.14 for dataset/inria/Train/pos\\crop001812.png\n",
      "\n",
      "0: 640x448 5 persons, 1 boat, 1 umbrella, 1 chair, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 128.55, 114.45, 174.16, 231.55 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 56.0, Confidence: 0.64, Bounding Box: 1.14, 153.23, 112.70, 262.16 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 8.0, Confidence: 0.40, Bounding Box: 207.64, 0.54, 249.00, 83.21 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 231.00, 134.63, 241.07, 149.90 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 25.0, Confidence: 0.37, Bounding Box: 0.24, 45.38, 57.76, 80.59 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 36.85, 134.17, 94.36, 189.16 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 15.96, 134.84, 93.95, 252.17 for dataset/inria/Train/pos\\crop001813.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 18.17, 118.77, 25.67, 127.20 for dataset/inria/Train/pos\\crop001813.png\n",
      "\n",
      "0: 640x576 1 person, 5 boats, 15.5ms\n",
      "Speed: 4.0ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 87.95, 91.61, 128.26, 203.25 for dataset/inria/Train/pos\\crop001814.png\n",
      "Class: 8.0, Confidence: 0.78, Bounding Box: 100.50, 31.19, 125.11, 56.79 for dataset/inria/Train/pos\\crop001814.png\n",
      "Class: 8.0, Confidence: 0.71, Bounding Box: 0.59, 170.81, 88.28, 237.31 for dataset/inria/Train/pos\\crop001814.png\n",
      "Class: 8.0, Confidence: 0.59, Bounding Box: 69.71, 29.42, 76.43, 43.36 for dataset/inria/Train/pos\\crop001814.png\n",
      "Class: 8.0, Confidence: 0.41, Bounding Box: 46.81, 29.39, 56.42, 43.96 for dataset/inria/Train/pos\\crop001814.png\n",
      "Class: 8.0, Confidence: 0.26, Bounding Box: 0.80, 30.89, 12.99, 44.78 for dataset/inria/Train/pos\\crop001814.png\n",
      "\n",
      "0: 640x640 4 persons, 1 boat, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 192.13, 130.71, 231.27, 243.14 for dataset/inria/Train/pos\\crop001815.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 151.29, 151.49, 189.56, 249.51 for dataset/inria/Train/pos\\crop001815.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 58.38, 163.63, 80.75, 224.14 for dataset/inria/Train/pos\\crop001815.png\n",
      "Class: 8.0, Confidence: 0.48, Bounding Box: 258.07, 189.27, 336.59, 209.56 for dataset/inria/Train/pos\\crop001815.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 146.14, 159.73, 160.13, 190.00 for dataset/inria/Train/pos\\crop001815.png\n",
      "\n",
      "0: 640x448 1 person, 1 surfboard, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 25.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 196.84, 193.80, 315.76, 487.74 for dataset/inria/Train/pos\\crop001816.png\n",
      "Class: 37.0, Confidence: 0.76, Bounding Box: 162.02, 323.52, 205.80, 364.79 for dataset/inria/Train/pos\\crop001816.png\n",
      "\n",
      "0: 640x512 3 persons, 2 boats, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 184.03, 179.94, 294.07, 293.37 for dataset/inria/Train/pos\\crop001817.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 120.78, 132.58, 190.80, 299.46 for dataset/inria/Train/pos\\crop001817.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 67.91, 235.40, 143.75, 320.99 for dataset/inria/Train/pos\\crop001817.png\n",
      "Class: 8.0, Confidence: 0.49, Bounding Box: 13.60, 0.71, 324.46, 324.29 for dataset/inria/Train/pos\\crop001817.png\n",
      "Class: 8.0, Confidence: 0.41, Bounding Box: 219.32, 200.59, 329.00, 366.10 for dataset/inria/Train/pos\\crop001817.png\n",
      "\n",
      "0: 640x384 2 persons, 2 umbrellas, 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 59.69, 155.23, 129.56, 325.11 for dataset/inria/Train/pos\\crop001818.png\n",
      "Class: 25.0, Confidence: 0.53, Bounding Box: 121.95, 104.10, 274.82, 168.17 for dataset/inria/Train/pos\\crop001818.png\n",
      "Class: 25.0, Confidence: 0.43, Bounding Box: 126.64, 103.39, 275.00, 299.26 for dataset/inria/Train/pos\\crop001818.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 202.50, 219.82, 227.31, 252.88 for dataset/inria/Train/pos\\crop001818.png\n",
      "\n",
      "0: 640x608 1 person, 4 umbrellas, 1 chair, 16.0ms\n",
      "Speed: 4.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 130.85, 103.47, 175.54, 205.31 for dataset/inria/Train/pos\\crop001819.png\n",
      "Class: 25.0, Confidence: 0.57, Bounding Box: 223.71, 89.93, 320.00, 198.82 for dataset/inria/Train/pos\\crop001819.png\n",
      "Class: 25.0, Confidence: 0.55, Bounding Box: 0.00, 48.93, 162.27, 254.54 for dataset/inria/Train/pos\\crop001819.png\n",
      "Class: 25.0, Confidence: 0.48, Bounding Box: 0.08, 49.38, 161.99, 115.57 for dataset/inria/Train/pos\\crop001819.png\n",
      "Class: 56.0, Confidence: 0.37, Bounding Box: 234.67, 154.54, 271.12, 201.17 for dataset/inria/Train/pos\\crop001819.png\n",
      "Class: 25.0, Confidence: 0.30, Bounding Box: 224.62, 90.10, 317.68, 116.54 for dataset/inria/Train/pos\\crop001819.png\n",
      "\n",
      "0: 640x448 2 persons, 1 umbrella, 2 chairs, 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 153.13, 152.66, 253.84, 450.77 for dataset/inria/Train/pos\\crop001820.png\n",
      "Class: 25.0, Confidence: 0.53, Bounding Box: 69.38, 96.01, 303.22, 215.54 for dataset/inria/Train/pos\\crop001820.png\n",
      "Class: 56.0, Confidence: 0.53, Bounding Box: 79.73, 233.38, 127.95, 332.84 for dataset/inria/Train/pos\\crop001820.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 249.46, 192.85, 283.58, 242.60 for dataset/inria/Train/pos\\crop001820.png\n",
      "Class: 56.0, Confidence: 0.27, Bounding Box: 111.42, 237.92, 174.12, 331.76 for dataset/inria/Train/pos\\crop001820.png\n",
      "\n",
      "0: 640x480 6 persons, 1 boat, 1 surfboard, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 95.71, 88.09, 137.07, 193.34 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 155.29, 92.11, 171.17, 112.51 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 36.79, 108.03, 65.27, 124.06 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 37.15, 107.93, 54.26, 123.74 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 54.42, 112.09, 65.53, 123.20 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 128.96, 93.23, 142.63, 111.22 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 8.0, Confidence: 0.29, Bounding Box: 41.79, 177.97, 217.42, 252.28 for dataset/inria/Train/pos\\crop001823.png\n",
      "Class: 37.0, Confidence: 0.25, Bounding Box: 60.76, 81.84, 99.29, 88.95 for dataset/inria/Train/pos\\crop001823.png\n",
      "\n",
      "0: 448x640 14 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 317.19, 148.28, 361.36, 233.83 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 77.24, 158.38, 98.69, 215.89 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 180.99, 160.73, 198.48, 221.56 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 121.96, 170.23, 139.25, 214.36 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 52.75, 165.61, 70.22, 215.24 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 399.61, 153.59, 428.79, 217.10 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 281.11, 147.11, 305.89, 230.10 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 194.92, 161.19, 211.24, 186.71 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 281.68, 147.77, 302.60, 212.27 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 294.72, 163.02, 309.04, 214.59 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 254.30, 164.05, 275.28, 210.54 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 354.45, 174.34, 370.46, 211.39 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 139.47, 205.15, 147.58, 214.79 for dataset/inria/Train/pos\\crop001824.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 354.15, 174.24, 376.18, 214.61 for dataset/inria/Train/pos\\crop001824.png\n",
      "\n",
      "0: 480x640 7 persons, 1 umbrella, 1 chair, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 302.33, 163.47, 340.06, 279.77 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 208.73, 163.91, 250.39, 278.53 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 146.57, 162.06, 184.86, 276.83 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 73.40, 184.98, 104.55, 243.17 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 56.0, Confidence: 0.69, Bounding Box: 310.13, 229.58, 544.72, 386.81 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 280.75, 161.36, 301.49, 277.32 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 107.59, 175.13, 134.04, 255.19 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 137.93, 167.10, 163.73, 274.73 for dataset/inria/Train/pos\\crop001829.png\n",
      "Class: 25.0, Confidence: 0.25, Bounding Box: 395.22, 32.68, 546.00, 89.31 for dataset/inria/Train/pos\\crop001829.png\n",
      "\n",
      "0: 640x576 2 persons, 6 umbrellas, 3 chairs, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 183.21, 172.69, 237.39, 320.11 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 56.0, Confidence: 0.79, Bounding Box: 116.25, 228.30, 155.77, 275.24 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 288.10, 207.19, 309.27, 261.71 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 56.0, Confidence: 0.57, Bounding Box: 48.40, 230.88, 87.38, 274.60 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 25.0, Confidence: 0.47, Bounding Box: 119.56, 193.48, 190.33, 211.01 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 56.0, Confidence: 0.43, Bounding Box: 311.09, 227.41, 365.57, 259.73 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 25.0, Confidence: 0.38, Bounding Box: 272.55, 188.14, 336.02, 206.30 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 25.0, Confidence: 0.35, Bounding Box: 229.08, 209.25, 278.85, 221.05 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 25.0, Confidence: 0.34, Bounding Box: 0.44, 197.33, 41.61, 219.10 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 25.0, Confidence: 0.31, Bounding Box: 52.11, 160.51, 164.36, 210.43 for dataset/inria/Train/pos\\crop001830.png\n",
      "Class: 25.0, Confidence: 0.29, Bounding Box: 317.99, 195.44, 376.63, 212.52 for dataset/inria/Train/pos\\crop001830.png\n",
      "\n",
      "0: 640x640 4 persons, 6 umbrellas, 2 chairs, 17.5ms\n",
      "Speed: 4.0ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 172.88, 189.02, 262.03, 259.60 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 279.23, 171.39, 323.35, 274.22 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 123.78, 125.54, 166.94, 253.45 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 25.0, Confidence: 0.52, Bounding Box: 153.21, 117.24, 248.35, 195.04 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 25.0, Confidence: 0.43, Bounding Box: 153.92, 118.03, 248.43, 151.43 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 25.0, Confidence: 0.35, Bounding Box: 277.10, 120.42, 379.37, 148.61 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 25.0, Confidence: 0.34, Bounding Box: 2.27, 162.09, 42.19, 189.68 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 56.0, Confidence: 0.33, Bounding Box: 18.11, 176.90, 64.39, 208.09 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 56.0, Confidence: 0.29, Bounding Box: 68.58, 175.69, 97.53, 206.71 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 25.0, Confidence: 0.29, Bounding Box: 301.53, 131.54, 384.37, 157.88 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 123.49, 126.47, 187.81, 241.76 for dataset/inria/Train/pos\\crop001831.png\n",
      "Class: 25.0, Confidence: 0.25, Bounding Box: 34.09, 126.54, 113.62, 157.43 for dataset/inria/Train/pos\\crop001831.png\n",
      "\n",
      "0: 608x640 2 persons, 5 horses, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 17.0, Confidence: 0.73, Bounding Box: 147.30, 84.72, 193.41, 179.39 for dataset/inria/Train/pos\\crop001832.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 69.65, 86.61, 102.22, 193.34 for dataset/inria/Train/pos\\crop001832.png\n",
      "Class: 17.0, Confidence: 0.55, Bounding Box: 69.67, 87.56, 102.51, 192.94 for dataset/inria/Train/pos\\crop001832.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 140.43, 101.27, 159.61, 161.68 for dataset/inria/Train/pos\\crop001832.png\n",
      "Class: 17.0, Confidence: 0.48, Bounding Box: 282.63, 90.19, 323.86, 134.40 for dataset/inria/Train/pos\\crop001832.png\n",
      "Class: 17.0, Confidence: 0.36, Bounding Box: 102.11, 113.43, 122.64, 161.86 for dataset/inria/Train/pos\\crop001832.png\n",
      "Class: 17.0, Confidence: 0.36, Bounding Box: 102.05, 110.91, 143.05, 161.24 for dataset/inria/Train/pos\\crop001832.png\n",
      "\n",
      "0: 608x640 4 persons, 2 horses, 23.0ms\n",
      "Speed: 6.0ms preprocess, 23.0ms inference, 7.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 194.09, 109.58, 223.10, 186.80 for dataset/inria/Train/pos\\crop001833.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 125.13, 90.86, 156.56, 178.90 for dataset/inria/Train/pos\\crop001833.png\n",
      "Class: 17.0, Confidence: 0.76, Bounding Box: 92.79, 99.05, 119.57, 178.73 for dataset/inria/Train/pos\\crop001833.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 165.49, 98.87, 193.04, 184.21 for dataset/inria/Train/pos\\crop001833.png\n",
      "Class: 17.0, Confidence: 0.59, Bounding Box: 5.59, 104.81, 39.37, 179.02 for dataset/inria/Train/pos\\crop001833.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 232.28, 107.19, 264.10, 187.42 for dataset/inria/Train/pos\\crop001833.png\n",
      "\n",
      "0: 640x544 2 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 347.08, 140.06, 551.58, 800.52 for dataset/inria/Train/pos\\crop001835.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 223.94, 183.88, 400.68, 780.62 for dataset/inria/Train/pos\\crop001835.png\n",
      "\n",
      "0: 544x640 4 persons, 3 cars, 1 bottle, 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 363.19, 118.01, 496.84, 646.16 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 251.46, 130.40, 392.15, 646.51 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 554.75, 150.33, 672.67, 669.76 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 474.96, 155.39, 578.39, 654.65 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 2.0, Confidence: 0.65, Bounding Box: 0.62, 164.39, 757.80, 566.25 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 0.00, 150.53, 554.43, 589.78 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 2.0, Confidence: 0.32, Bounding Box: 267.59, 201.27, 903.46, 623.92 for dataset/inria/Train/pos\\crop001836.png\n",
      "Class: 39.0, Confidence: 0.27, Bounding Box: 512.84, 248.41, 546.94, 299.28 for dataset/inria/Train/pos\\crop001836.png\n",
      "\n",
      "0: 480x640 13 persons, 3 surfboards, 11.0ms\n",
      "Speed: 2.5ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 127.48, 101.40, 307.01, 383.32 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 91.17, 103.14, 120.29, 181.89 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 173.10, 105.67, 196.53, 182.11 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 532.23, 149.57, 592.73, 209.07 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 37.0, Confidence: 0.64, Bounding Box: 52.39, 366.16, 440.77, 415.87 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 37.0, Confidence: 0.61, Bounding Box: 45.57, 226.51, 189.50, 240.02 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 442.23, 173.46, 545.40, 239.58 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 106.99, 186.28, 172.72, 230.93 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 106.63, 186.46, 147.21, 229.56 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 267.83, 74.30, 398.79, 338.76 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 371.70, 85.65, 442.23, 283.88 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 581.10, 105.21, 598.61, 160.55 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 37.0, Confidence: 0.46, Bounding Box: 384.87, 268.54, 569.95, 292.60 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 485.18, 102.31, 505.42, 164.01 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 206.32, 90.98, 212.79, 106.54 for dataset/inria/Train/pos\\crop001837.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 421.57, 150.31, 458.67, 201.50 for dataset/inria/Train/pos\\crop001837.png\n",
      "\n",
      "0: 480x640 10 persons, 11 surfboards, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 37.0, Confidence: 0.94, Bounding Box: 331.15, 7.28, 388.61, 366.85 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.90, Bounding Box: 195.43, 76.12, 233.84, 355.60 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.87, Bounding Box: 473.36, 45.87, 526.21, 400.60 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 531.14, 125.07, 623.20, 399.85 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 9.48, 151.34, 69.47, 368.05 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 146.17, 141.71, 207.42, 361.93 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 357.94, 128.03, 432.36, 385.75 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.75, Bounding Box: 418.95, 26.81, 474.80, 338.57 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 226.81, 134.65, 286.59, 377.08 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.74, Bounding Box: 588.42, 14.02, 621.20, 374.22 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 284.94, 145.59, 340.98, 372.69 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.68, Bounding Box: 276.18, 28.75, 324.66, 365.71 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 426.47, 135.30, 483.27, 392.05 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.65, Bounding Box: 115.62, 46.06, 148.05, 349.28 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 65.58, 148.06, 138.08, 365.23 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.48, Bounding Box: 404.38, 163.56, 439.49, 386.32 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.47, Bounding Box: 418.89, 27.88, 474.74, 190.30 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 471.45, 48.85, 527.73, 400.33 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.45, Bounding Box: 31.44, 61.37, 63.45, 160.41 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 37.0, Confidence: 0.33, Bounding Box: 276.02, 31.63, 324.57, 195.87 for dataset/inria/Train/pos\\crop001838.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 64.33, 63.98, 140.47, 363.96 for dataset/inria/Train/pos\\crop001838.png\n",
      "\n",
      "0: 480x640 11 persons, 7 surfboards, 13.5ms\n",
      "Speed: 9.0ms preprocess, 13.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 37.0, Confidence: 0.85, Bounding Box: 34.06, 314.88, 115.93, 386.05 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 44.39, 144.27, 109.58, 369.77 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 357.76, 134.33, 497.92, 430.03 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 249.23, 139.78, 338.95, 324.13 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 534.53, 139.25, 549.40, 174.27 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 37.0, Confidence: 0.63, Bounding Box: 205.22, 386.83, 595.80, 451.61 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 514.25, 137.99, 526.18, 174.97 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 192.00, 134.49, 206.73, 169.35 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 174.49, 148.35, 183.61, 168.89 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 37.0, Confidence: 0.45, Bounding Box: 167.98, 295.81, 324.22, 322.09 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 321.36, 142.71, 408.62, 372.62 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 312.49, 149.61, 382.40, 380.49 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 348.81, 136.18, 430.73, 429.83 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 37.0, Confidence: 0.33, Bounding Box: 206.96, 376.40, 577.19, 437.30 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 37.0, Confidence: 0.27, Bounding Box: 191.09, 372.91, 370.78, 411.87 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 37.0, Confidence: 0.26, Bounding Box: 214.05, 408.71, 369.88, 447.44 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 145.24, 155.72, 158.05, 165.88 for dataset/inria/Train/pos\\crop001839.png\n",
      "Class: 37.0, Confidence: 0.26, Bounding Box: 207.22, 406.80, 606.58, 451.10 for dataset/inria/Train/pos\\crop001839.png\n",
      "\n",
      "0: 384x640 10 persons, 1 chair, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 108.62, 99.19, 213.07, 344.28 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 405.73, 116.75, 478.81, 341.82 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 272.04, 111.80, 325.55, 341.46 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 316.12, 94.61, 366.91, 330.42 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 351.83, 112.71, 411.59, 340.20 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 229.76, 107.13, 287.80, 338.09 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 188.13, 100.90, 235.39, 337.76 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 56.0, Confidence: 0.37, Bounding Box: 46.23, 161.88, 94.95, 210.95 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 276.04, 90.51, 303.94, 148.14 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 216.92, 101.42, 251.85, 158.53 for dataset/inria/Train/pos\\crop001840.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 193.84, 96.74, 222.21, 146.93 for dataset/inria/Train/pos\\crop001840.png\n",
      "\n",
      "0: 480x640 8 persons, 1 frisbee, 6 surfboards, 47.5ms\n",
      "Speed: 3.0ms preprocess, 47.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 37.0, Confidence: 0.90, Bounding Box: 184.09, 272.27, 445.32, 395.27 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 279.27, 160.97, 341.45, 302.77 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 425.21, 146.30, 499.58, 379.82 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 37.0, Confidence: 0.84, Bounding Box: 391.94, 293.55, 483.48, 438.66 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 37.0, Confidence: 0.82, Bounding Box: 602.61, 27.02, 638.96, 396.54 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 341.41, 170.03, 404.86, 305.66 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 207.00, 158.62, 263.22, 331.83 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 99.91, 166.84, 140.91, 302.38 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 150.09, 164.70, 211.23, 318.92 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 555.34, 129.43, 609.29, 380.16 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 37.0, Confidence: 0.70, Bounding Box: 105.24, 297.20, 210.29, 379.75 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 135.14, 166.41, 167.92, 296.54 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 29.0, Confidence: 0.48, Bounding Box: 486.88, 280.14, 503.14, 313.03 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 37.0, Confidence: 0.28, Bounding Box: 54.14, 194.46, 151.65, 287.72 for dataset/inria/Train/pos\\crop001841.png\n",
      "Class: 37.0, Confidence: 0.27, Bounding Box: 54.07, 194.68, 112.43, 285.64 for dataset/inria/Train/pos\\crop001841.png\n",
      "\n",
      "0: 480x640 15 persons, 25.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 399.53, 219.63, 444.48, 354.77 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 339.23, 229.39, 390.67, 347.86 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 529.74, 223.73, 576.78, 384.39 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 593.01, 281.80, 634.21, 388.75 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 251.06, 166.71, 310.74, 318.87 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 434.45, 236.28, 495.58, 364.67 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 295.56, 199.23, 346.73, 328.53 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 482.98, 235.47, 527.30, 378.02 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 72.34, 178.06, 105.59, 262.65 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 139.04, 173.52, 170.20, 280.23 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 113.59, 174.17, 137.77, 265.14 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 209.42, 185.80, 250.76, 306.48 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 98.06, 174.51, 118.36, 259.98 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 68.56, 171.44, 85.78, 258.95 for dataset/inria/Train/pos\\crop001842.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 165.45, 192.20, 217.43, 297.12 for dataset/inria/Train/pos\\crop001842.png\n",
      "\n",
      "0: 480x640 10 persons, 18 surfboards, 25.4ms\n",
      "Speed: 2.2ms preprocess, 25.4ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 26.45, 244.77, 138.81, 478.96 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.91, Bounding Box: 7.49, 127.07, 78.45, 478.43 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.88, Bounding Box: 404.63, 101.12, 459.84, 408.75 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 146.97, 244.19, 223.46, 479.00 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 219.71, 231.13, 300.66, 478.56 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.80, Bounding Box: 251.11, 119.34, 336.38, 478.43 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 510.42, 191.05, 551.75, 367.52 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.77, Bounding Box: 325.45, 84.54, 410.96, 454.20 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 443.98, 180.73, 514.87, 417.08 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 535.45, 175.82, 582.61, 359.74 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.71, Bounding Box: 516.81, 107.15, 546.18, 214.57 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.70, Bounding Box: 571.77, 113.78, 628.21, 314.32 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 311.37, 206.82, 377.54, 479.00 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 488.09, 183.81, 521.63, 368.42 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.59, Bounding Box: 154.36, 119.23, 210.68, 477.78 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.50, Bounding Box: 155.42, 118.69, 202.80, 270.59 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 617.25, 158.32, 638.85, 294.72 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.47, Bounding Box: 250.86, 118.29, 310.97, 479.00 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.43, Bounding Box: 381.79, 213.69, 439.24, 416.44 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 380.26, 216.86, 439.96, 420.74 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.37, Bounding Box: 478.11, 90.41, 517.03, 218.23 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.37, Bounding Box: 251.58, 117.73, 299.21, 319.61 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.37, Bounding Box: 449.59, 121.77, 486.48, 218.54 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.33, Bounding Box: 618.94, 280.51, 638.71, 320.95 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.30, Bounding Box: 584.33, 197.75, 638.40, 310.25 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.28, Bounding Box: 571.13, 114.12, 621.87, 272.53 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.28, Bounding Box: 414.49, 162.49, 463.89, 405.54 for dataset/inria/Train/pos\\crop001843.png\n",
      "Class: 37.0, Confidence: 0.27, Bounding Box: 472.89, 90.56, 518.25, 352.21 for dataset/inria/Train/pos\\crop001843.png\n",
      "\n",
      "0: 640x544 1 person, 16.8ms\n",
      "Speed: 3.3ms preprocess, 16.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 190.55, 117.07, 432.55, 647.88 for dataset/inria/Train/pos\\crop_000010.png\n",
      "\n",
      "0: 640x544 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 165.63, 94.83, 375.78, 605.83 for dataset/inria/Train/pos\\crop_000011.png\n",
      "Class: 24.0, Confidence: 0.36, Bounding Box: 268.10, 186.89, 378.58, 323.25 for dataset/inria/Train/pos\\crop_000011.png\n",
      "\n",
      "0: 640x352 1 person, 1 backpack, 2 skiss, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 64.96, 37.90, 278.65, 643.30 for dataset/inria/Train/pos\\crop_000603.png\n",
      "Class: 30.0, Confidence: 0.66, Bounding Box: 0.93, 602.20, 362.00, 719.66 for dataset/inria/Train/pos\\crop_000603.png\n",
      "Class: 24.0, Confidence: 0.60, Bounding Box: 221.61, 199.42, 283.62, 317.18 for dataset/inria/Train/pos\\crop_000603.png\n",
      "Class: 30.0, Confidence: 0.44, Bounding Box: 1.63, 598.83, 362.00, 661.28 for dataset/inria/Train/pos\\crop_000603.png\n",
      "\n",
      "0: 480x640 10 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 286.51, 110.68, 363.53, 311.19 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 375.10, 125.20, 440.67, 263.08 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 223.14, 113.65, 303.92, 358.53 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 164.99, 129.95, 246.41, 379.82 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 459.08, 129.63, 483.15, 198.07 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 479.68, 133.15, 504.51, 189.81 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 26.0, Confidence: 0.50, Bounding Box: 245.35, 184.10, 290.84, 252.60 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 26.0, Confidence: 0.43, Bounding Box: 245.59, 213.39, 289.49, 252.10 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 426.58, 130.33, 459.21, 191.17 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 499.81, 132.30, 515.74, 175.54 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 510.18, 131.82, 522.50, 166.15 for dataset/inria/Train/pos\\crop_000606.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 521.98, 131.36, 533.75, 161.99 for dataset/inria/Train/pos\\crop_000606.png\n",
      "\n",
      "0: 544x640 3 persons, 1 backpack, 3 handbags, 1 suitcase, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 568.16, 75.51, 771.68, 701.13 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 194.87, 91.38, 429.95, 644.15 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 506.83, 97.25, 636.35, 647.52 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 24.0, Confidence: 0.64, Bounding Box: 295.18, 164.10, 440.33, 342.12 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 26.0, Confidence: 0.63, Bounding Box: 536.12, 409.52, 610.17, 625.77 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 26.0, Confidence: 0.62, Bounding Box: 703.49, 186.54, 767.79, 370.82 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 26.0, Confidence: 0.43, Bounding Box: 238.75, 166.83, 349.35, 389.59 for dataset/inria/Train/pos\\crop_000607.png\n",
      "Class: 28.0, Confidence: 0.29, Bounding Box: 536.45, 398.84, 609.97, 626.33 for dataset/inria/Train/pos\\crop_000607.png\n",
      "\n",
      "0: 640x512 1 person, 1 surfboard, 13.5ms\n",
      "Speed: 1.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 127.80, 120.64, 248.60, 386.20 for dataset/inria/Train/pos\\crop_000608.png\n",
      "Class: 37.0, Confidence: 0.65, Bounding Box: 0.00, 291.03, 368.00, 467.81 for dataset/inria/Train/pos\\crop_000608.png\n",
      "\n",
      "0: 480x640 3 persons, 1 umbrella, 2 chairs, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 289.63, 114.68, 393.93, 402.47 for dataset/inria/Train/pos\\person_044.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 20.44, 149.75, 66.34, 281.31 for dataset/inria/Train/pos\\person_044.png\n",
      "Class: 56.0, Confidence: 0.81, Bounding Box: 160.25, 203.93, 200.14, 256.38 for dataset/inria/Train/pos\\person_044.png\n",
      "Class: 56.0, Confidence: 0.75, Bounding Box: 200.77, 209.52, 240.72, 266.81 for dataset/inria/Train/pos\\person_044.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 0.03, 141.88, 23.14, 253.31 for dataset/inria/Train/pos\\person_044.png\n",
      "Class: 25.0, Confidence: 0.38, Bounding Box: 353.40, 0.10, 397.84, 58.32 for dataset/inria/Train/pos\\person_044.png\n",
      "\n",
      "0: 480x640 2 persons, 1 umbrella, 1 potted plant, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 338.58, 98.66, 452.72, 422.41 for dataset/inria/Train/pos\\person_050.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 87.01, 110.51, 121.32, 201.02 for dataset/inria/Train/pos\\person_050.png\n",
      "Class: 58.0, Confidence: 0.56, Bounding Box: 116.62, 96.21, 155.55, 171.76 for dataset/inria/Train/pos\\person_050.png\n",
      "Class: 25.0, Confidence: 0.33, Bounding Box: 273.08, 37.80, 578.61, 87.26 for dataset/inria/Train/pos\\person_050.png\n",
      "\n",
      "0: 480x640 1 person, 2 traffic lights, 14.2ms\n",
      "Speed: 1.0ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 281.01, 215.89, 340.52, 347.39 for dataset/inria/Train/pos\\person_060.png\n",
      "Class: 9.0, Confidence: 0.70, Bounding Box: 359.00, 52.54, 403.24, 157.87 for dataset/inria/Train/pos\\person_060.png\n",
      "Class: 9.0, Confidence: 0.29, Bounding Box: 333.49, 21.11, 403.35, 163.85 for dataset/inria/Train/pos\\person_060.png\n",
      "\n",
      "0: 480x640 1 person, 1 handbag, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 217.21, 171.45, 274.25, 367.31 for dataset/inria/Train/pos\\person_061.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 221.88, 299.77, 263.50, 341.31 for dataset/inria/Train/pos\\person_061.png\n",
      "\n",
      "0: 640x480 5 persons, 2 handbags, 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 222.70, 147.56, 329.77, 488.61 for dataset/inria/Train/pos\\person_086.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 210.94, 43.37, 309.02, 244.38 for dataset/inria/Train/pos\\person_086.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 0.00, 57.71, 63.48, 342.12 for dataset/inria/Train/pos\\person_086.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 451.61, 59.37, 479.84, 231.17 for dataset/inria/Train/pos\\person_086.png\n",
      "Class: 26.0, Confidence: 0.46, Bounding Box: 293.47, 211.10, 329.00, 340.89 for dataset/inria/Train/pos\\person_086.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 114.40, 77.30, 181.36, 146.98 for dataset/inria/Train/pos\\person_086.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 450.94, 96.61, 473.81, 120.00 for dataset/inria/Train/pos\\person_086.png\n",
      "\n",
      "0: 640x480 5 persons, 1 tie, 5 chairs, 14.3ms\n",
      "Speed: 3.2ms preprocess, 14.3ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 183.09, 178.96, 309.19, 487.00 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 23.55, 192.98, 55.62, 283.82 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 447.42, 177.36, 479.69, 422.82 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 115.24, 189.96, 153.72, 288.67 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 27.0, Confidence: 0.76, Bounding Box: 227.00, 233.22, 250.13, 304.50 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 56.0, Confidence: 0.74, Bounding Box: 372.42, 283.67, 435.26, 389.81 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 360.86, 194.41, 406.86, 284.68 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 56.0, Confidence: 0.41, Bounding Box: 424.14, 291.11, 460.37, 374.38 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 56.0, Confidence: 0.29, Bounding Box: 310.59, 267.59, 356.28, 293.19 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 56.0, Confidence: 0.29, Bounding Box: 286.33, 291.48, 319.97, 375.83 for dataset/inria/Train/pos\\person_091.png\n",
      "Class: 56.0, Confidence: 0.27, Bounding Box: 322.27, 293.01, 370.96, 381.78 for dataset/inria/Train/pos\\person_091.png\n",
      "\n",
      "0: 640x480 7 persons, 2 handbags, 11.8ms\n",
      "Speed: 1.2ms preprocess, 11.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 0.00, 116.34, 110.33, 423.00 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 253.06, 81.58, 388.03, 579.62 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 78.49, 126.74, 223.45, 582.08 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 26.0, Confidence: 0.81, Bounding Box: 236.59, 253.37, 310.25, 397.01 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 26.0, Confidence: 0.66, Bounding Box: 176.49, 272.85, 235.75, 451.08 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 0.00, 134.02, 28.89, 495.47 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 213.99, 139.40, 275.15, 438.84 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 0.07, 141.52, 21.38, 390.99 for dataset/inria/Train/pos\\person_097.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 218.39, 143.65, 274.54, 290.98 for dataset/inria/Train/pos\\person_097.png\n",
      "\n",
      "0: 640x480 7 persons, 1 backpack, 13.7ms\n",
      "Speed: 1.4ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.05, 210.24, 90.90, 521.47 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 185.25, 182.33, 304.11, 479.85 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 376.95, 219.83, 447.38, 420.04 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 354.46, 203.81, 388.88, 349.63 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 445.62, 211.00, 479.87, 386.94 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 271.88, 238.19, 311.19, 416.47 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 24.0, Confidence: 0.27, Bounding Box: 219.62, 236.84, 302.45, 314.86 for dataset/inria/Train/pos\\person_101.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 456.06, 210.56, 479.87, 379.29 for dataset/inria/Train/pos\\person_101.png\n",
      "\n",
      "0: 640x480 5 persons, 1 handbag, 33.0ms\n",
      "Speed: 7.0ms preprocess, 33.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 160.62, 57.58, 322.20, 456.90 for dataset/inria/Train/pos\\person_102.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 92.92, 35.36, 148.94, 215.80 for dataset/inria/Train/pos\\person_102.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 38.22, 98.10, 105.81, 276.27 for dataset/inria/Train/pos\\person_102.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 46.28, 32.77, 98.19, 142.06 for dataset/inria/Train/pos\\person_102.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 25.32, 144.45, 60.21, 200.88 for dataset/inria/Train/pos\\person_102.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 45.96, 32.71, 101.36, 189.10 for dataset/inria/Train/pos\\person_102.png\n",
      "\n",
      "0: 640x480 1 person, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 223.70, 226.67, 288.44, 377.59 for dataset/inria/Train/pos\\person_103.png\n",
      "\n",
      "0: 640x480 1 person, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 224.69, 272.14, 286.97, 404.47 for dataset/inria/Train/pos\\person_106.png\n",
      "\n",
      "0: 640x480 11 persons, 1 bicycle, 2 handbags, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 155.67, 132.12, 331.29, 607.85 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 29.46, 142.88, 144.19, 446.77 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 0.03, 221.43, 27.02, 328.36 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 403.45, 144.62, 479.38, 386.28 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 1.0, Confidence: 0.71, Bounding Box: 330.34, 215.38, 406.25, 323.57 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 147.63, 152.30, 194.58, 259.51 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 281.83, 143.41, 341.01, 290.23 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 26.0, Confidence: 0.44, Bounding Box: 401.87, 170.35, 455.18, 254.20 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 9.74, 166.91, 39.01, 254.76 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 102.06, 144.04, 168.90, 400.08 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 0.00, 159.00, 21.03, 231.87 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 146.63, 154.25, 194.37, 230.67 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 195.96, 206.29, 285.13, 341.11 for dataset/inria/Train/pos\\person_111.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 390.90, 140.13, 412.86, 215.74 for dataset/inria/Train/pos\\person_111.png\n",
      "\n",
      "0: 640x480 5 persons, 1 tie, 15.5ms\n",
      "Speed: 2.0ms preprocess, 15.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 273.06, 96.59, 420.49, 625.42 for dataset/inria/Train/pos\\person_112.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 181.22, 132.44, 300.18, 568.79 for dataset/inria/Train/pos\\person_112.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 0.26, 97.54, 153.30, 603.04 for dataset/inria/Train/pos\\person_112.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 107.49, 119.19, 185.45, 389.34 for dataset/inria/Train/pos\\person_112.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 106.30, 120.72, 185.58, 508.19 for dataset/inria/Train/pos\\person_112.png\n",
      "Class: 27.0, Confidence: 0.44, Bounding Box: 92.18, 181.71, 115.32, 247.04 for dataset/inria/Train/pos\\person_112.png\n",
      "\n",
      "0: 640x480 2 persons, 52.0ms\n",
      "Speed: 1.0ms preprocess, 52.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 29.51, 36.29, 246.98, 637.22 for dataset/inria/Train/pos\\person_114.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 235.87, 81.50, 441.44, 636.37 for dataset/inria/Train/pos\\person_114.png\n",
      "\n",
      "0: 640x480 4 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 173.64, 156.66, 333.36, 572.64 for dataset/inria/Train/pos\\person_117.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 86.60, 173.31, 142.57, 309.18 for dataset/inria/Train/pos\\person_117.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 38.14, 163.51, 93.75, 234.46 for dataset/inria/Train/pos\\person_117.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 7.58, 153.38, 39.89, 242.30 for dataset/inria/Train/pos\\person_117.png\n",
      "\n",
      "0: 640x480 3 persons, 1 handbag, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 167.88, 160.02, 292.37, 506.84 for dataset/inria/Train/pos\\person_119.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 345.26, 126.17, 367.87, 188.06 for dataset/inria/Train/pos\\person_119.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 140.63, 157.94, 172.72, 288.20 for dataset/inria/Train/pos\\person_119.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 166.37, 288.01, 195.06, 362.78 for dataset/inria/Train/pos\\person_119.png\n",
      "\n",
      "0: 640x480 7 persons, 1 handbag, 1 potted plant, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 191.66, 135.47, 320.70, 472.85 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 26.0, Confidence: 0.80, Bounding Box: 239.30, 258.59, 295.51, 381.03 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 147.72, 111.89, 182.25, 218.13 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 128.14, 114.39, 160.60, 216.44 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 460.94, 98.69, 479.84, 149.09 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 235.88, 109.83, 254.42, 151.47 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 58.0, Confidence: 0.47, Bounding Box: 341.23, 92.29, 374.77, 147.17 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 191.17, 112.56, 204.60, 143.48 for dataset/inria/Train/pos\\person_121.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 126.20, 101.72, 153.03, 147.56 for dataset/inria/Train/pos\\person_121.png\n",
      "\n",
      "0: 640x480 2 persons, 1 chair, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.95, Bounding Box: 29.17, 328.30, 251.46, 639.52 for dataset/inria/Train/pos\\person_124.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 253.09, 71.31, 325.62, 261.40 for dataset/inria/Train/pos\\person_124.png\n",
      "Class: 56.0, Confidence: 0.73, Bounding Box: 0.27, 455.64, 172.55, 639.72 for dataset/inria/Train/pos\\person_124.png\n",
      "\n",
      "0: 640x480 2 persons, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 107.72, 94.19, 218.73, 629.12 for dataset/inria/Train/pos\\person_125.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 266.59, 79.81, 474.21, 634.40 for dataset/inria/Train/pos\\person_125.png\n",
      "\n",
      "0: 640x480 3 persons, 1 traffic light, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 182.70, 104.30, 349.03, 514.44 for dataset/inria/Train/pos\\person_126.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 121.81, 157.34, 175.36, 302.59 for dataset/inria/Train/pos\\person_126.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 69.41, 175.77, 111.06, 304.01 for dataset/inria/Train/pos\\person_126.png\n",
      "Class: 9.0, Confidence: 0.37, Bounding Box: 12.55, 48.81, 28.53, 89.38 for dataset/inria/Train/pos\\person_126.png\n",
      "\n",
      "0: 480x640 13 persons, 3 cars, 1 truck, 1 handbag, 4 chairs, 1 potted plant, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 183.76, 107.50, 262.58, 324.79 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 361.01, 142.68, 430.04, 324.04 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 77.95, 102.43, 114.39, 200.65 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 558.81, 90.41, 608.00, 193.83 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 264.69, 114.93, 353.06, 325.78 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 56.0, Confidence: 0.73, Bounding Box: 546.17, 154.94, 579.32, 201.25 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 244.54, 95.74, 272.32, 220.36 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 188.10, 95.44, 219.45, 153.24 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 61.86, 105.21, 84.69, 180.94 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 26.0, Confidence: 0.51, Bounding Box: 370.80, 180.44, 419.26, 257.47 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 447.20, 99.29, 466.15, 157.03 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 58.0, Confidence: 0.45, Bounding Box: 469.08, 66.08, 508.79, 204.74 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 56.0, Confidence: 0.43, Bounding Box: 525.50, 153.64, 553.13, 201.79 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 2.0, Confidence: 0.42, Bounding Box: 373.50, 113.40, 407.77, 142.00 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 2.0, Confidence: 0.42, Bounding Box: 44.38, 127.39, 69.31, 165.03 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 264.26, 96.03, 288.62, 154.49 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 56.0, Confidence: 0.37, Bounding Box: 432.31, 157.90, 465.00, 204.69 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 2.0, Confidence: 0.36, Bounding Box: 44.33, 98.75, 132.18, 160.97 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 433.36, 103.59, 448.06, 142.00 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 548.91, 94.76, 573.30, 154.81 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 56.0, Confidence: 0.31, Bounding Box: 504.57, 147.57, 531.32, 201.81 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 7.0, Confidence: 0.29, Bounding Box: 65.98, 97.72, 132.40, 156.50 for dataset/inria/Train/pos\\person_128.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 447.87, 100.08, 466.24, 141.65 for dataset/inria/Train/pos\\person_128.png\n",
      "\n",
      "0: 480x640 7 persons, 1 traffic light, 1 handbag, 1 potted plant, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 169.18, 206.68, 233.89, 358.44 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 326.29, 162.96, 383.46, 357.74 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 9.0, Confidence: 0.72, Bounding Box: 91.44, 68.18, 127.42, 147.56 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 254.95, 172.16, 317.34, 362.61 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 58.0, Confidence: 0.69, Bounding Box: 0.00, 154.67, 63.80, 287.65 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 308.57, 169.27, 337.37, 271.96 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 244.98, 186.31, 278.85, 359.25 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 178.78, 168.68, 200.40, 210.70 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 200.19, 169.01, 216.57, 199.83 for dataset/inria/Train/pos\\person_129.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 165.36, 299.30, 181.50, 328.94 for dataset/inria/Train/pos\\person_129.png\n",
      "\n",
      "0: 480x640 7 persons, 2 benchs, 1 chair, 49.0ms\n",
      "Speed: 1.5ms preprocess, 49.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 200.97, 148.20, 379.31, 434.70 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 294.35, 84.11, 393.34, 424.95 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 264.13, 86.28, 290.97, 167.82 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 13.0, Confidence: 0.72, Bounding Box: 0.39, 219.94, 322.37, 452.16 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 226.40, 87.28, 243.93, 139.68 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 150.80, 81.43, 168.66, 134.03 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 200.32, 82.15, 222.20, 134.74 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 56.0, Confidence: 0.49, Bounding Box: 151.11, 130.67, 191.52, 184.48 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 13.0, Confidence: 0.40, Bounding Box: 104.76, 223.61, 323.88, 450.49 for dataset/inria/Train/pos\\person_130.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 202.07, 83.31, 221.70, 121.69 for dataset/inria/Train/pos\\person_130.png\n",
      "\n",
      "0: 480x640 4 persons, 1 backpack, 1 handbag, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 380.16, 170.67, 429.12, 342.52 for dataset/inria/Train/pos\\person_131.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 211.93, 177.66, 278.18, 385.31 for dataset/inria/Train/pos\\person_131.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 297.08, 173.76, 360.40, 361.81 for dataset/inria/Train/pos\\person_131.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 262.78, 181.51, 303.65, 346.51 for dataset/inria/Train/pos\\person_131.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 342.92, 262.61, 359.95, 294.71 for dataset/inria/Train/pos\\person_131.png\n",
      "Class: 24.0, Confidence: 0.25, Bounding Box: 306.73, 206.78, 359.10, 277.59 for dataset/inria/Train/pos\\person_131.png\n",
      "\n",
      "0: 480x640 8 persons, 2 handbags, 2 chairs, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 333.51, 152.72, 401.28, 368.32 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 233.92, 179.04, 306.48, 372.26 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 53.52, 189.50, 87.03, 280.02 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 96.03, 230.09, 145.43, 376.65 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 84.10, 191.72, 118.29, 275.85 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 0.00, 199.10, 22.96, 300.40 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 25.13, 200.10, 47.82, 269.40 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 56.0, Confidence: 0.56, Bounding Box: 299.78, 244.45, 334.89, 298.39 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 235.92, 280.76, 258.64, 315.07 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 242.23, 208.31, 264.18, 279.94 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 56.0, Confidence: 0.27, Bounding Box: 468.01, 238.56, 510.64, 296.24 for dataset/inria/Train/pos\\person_133.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 119.86, 216.18, 149.60, 262.80 for dataset/inria/Train/pos\\person_133.png\n",
      "\n",
      "0: 480x640 11 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 414.28, 119.72, 500.07, 348.40 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 53.42, 124.20, 137.94, 376.06 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 478.48, 130.50, 568.81, 347.21 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 249.74, 122.24, 349.90, 425.54 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 159.69, 147.47, 257.80, 439.76 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 120.59, 122.11, 173.88, 272.02 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 333.53, 135.77, 376.66, 300.13 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 361.51, 138.73, 410.78, 295.45 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 212.81, 124.11, 261.24, 250.87 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 317.81, 129.96, 338.47, 172.19 for dataset/inria/Train/pos\\person_139.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 365.23, 139.18, 396.84, 293.19 for dataset/inria/Train/pos\\person_139.png\n",
      "\n",
      "0: 480x640 4 persons, 1 umbrella, 6 chairs, 1 potted plant, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 243.70, 123.18, 366.52, 364.67 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 123.39, 123.67, 147.80, 195.00 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 56.0, Confidence: 0.73, Bounding Box: 72.75, 204.84, 119.29, 265.71 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 56.0, Confidence: 0.73, Bounding Box: 163.66, 196.55, 210.71, 255.66 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 56.0, Confidence: 0.68, Bounding Box: 0.02, 217.83, 29.41, 278.52 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 58.0, Confidence: 0.63, Bounding Box: 355.31, 82.40, 446.50, 233.79 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 25.0, Confidence: 0.61, Bounding Box: 173.93, 0.00, 218.59, 171.10 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 56.0, Confidence: 0.57, Bounding Box: 20.73, 209.03, 69.74, 272.94 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 480.67, 119.03, 495.62, 164.68 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 456.62, 119.89, 474.34, 165.09 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 56.0, Confidence: 0.44, Bounding Box: 241.65, 192.10, 269.97, 243.33 for dataset/inria/Train/pos\\person_141.png\n",
      "Class: 56.0, Confidence: 0.28, Bounding Box: 221.73, 189.94, 266.63, 243.14 for dataset/inria/Train/pos\\person_141.png\n",
      "\n",
      "0: 480x640 2 persons, 1 bicycle, 6 chairs, 1 potted plant, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 56.0, Confidence: 0.90, Bounding Box: 414.19, 174.86, 534.53, 304.17 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 122.85, 205.44, 208.27, 399.71 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 246.73, 10.94, 421.98, 415.88 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 1.0, Confidence: 0.61, Bounding Box: 395.25, 201.96, 639.90, 466.40 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 56.0, Confidence: 0.61, Bounding Box: 77.95, 162.83, 148.57, 287.36 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 56.0, Confidence: 0.60, Bounding Box: 162.23, 148.31, 228.09, 266.30 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 58.0, Confidence: 0.54, Bounding Box: 105.80, 32.41, 224.45, 154.50 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 56.0, Confidence: 0.53, Bounding Box: 0.00, 155.87, 77.50, 274.05 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 56.0, Confidence: 0.33, Bounding Box: 0.00, 142.45, 98.12, 275.92 for dataset/inria/Train/pos\\person_142.png\n",
      "Class: 56.0, Confidence: 0.31, Bounding Box: 45.85, 136.34, 117.84, 195.01 for dataset/inria/Train/pos\\person_142.png\n",
      "\n",
      "0: 480x640 4 persons, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 101.29, 78.75, 211.80, 376.23 for dataset/inria/Train/pos\\person_143.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 296.68, 67.74, 386.96, 378.16 for dataset/inria/Train/pos\\person_143.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 230.78, 82.66, 312.39, 371.25 for dataset/inria/Train/pos\\person_143.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 87.45, 99.39, 99.76, 147.89 for dataset/inria/Train/pos\\person_143.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 332.76, 231.78, 380.66, 309.97 for dataset/inria/Train/pos\\person_143.png\n",
      "\n",
      "0: 480x640 3 persons, 3 handbags, 1 broccoli, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 335.79, 200.85, 431.85, 400.01 for dataset/inria/Train/pos\\person_156.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 244.42, 191.38, 331.53, 418.53 for dataset/inria/Train/pos\\person_156.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 319.24, 7.96, 382.56, 174.69 for dataset/inria/Train/pos\\person_156.png\n",
      "Class: 26.0, Confidence: 0.68, Bounding Box: 344.25, 239.60, 388.10, 308.24 for dataset/inria/Train/pos\\person_156.png\n",
      "Class: 26.0, Confidence: 0.57, Bounding Box: 365.47, 69.31, 397.88, 118.19 for dataset/inria/Train/pos\\person_156.png\n",
      "Class: 26.0, Confidence: 0.47, Bounding Box: 343.81, 273.01, 386.05, 307.90 for dataset/inria/Train/pos\\person_156.png\n",
      "Class: 50.0, Confidence: 0.28, Bounding Box: 0.00, 270.30, 58.80, 479.70 for dataset/inria/Train/pos\\person_156.png\n",
      "\n",
      "0: 480x640 9 persons, 1 vase, 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 222.77, 135.55, 333.49, 409.78 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 55.91, 122.11, 187.78, 429.71 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 339.25, 121.41, 456.09, 410.47 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 0.00, 93.27, 65.44, 400.76 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 199.17, 118.91, 264.26, 287.56 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 139.58, 124.66, 188.65, 204.40 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 27.87, 120.41, 58.15, 160.88 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 75.0, Confidence: 0.36, Bounding Box: 530.95, 312.57, 639.81, 479.51 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 84.48, 111.09, 117.44, 176.34 for dataset/inria/Train/pos\\person_160.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 52.15, 119.22, 85.41, 198.27 for dataset/inria/Train/pos\\person_160.png\n",
      "\n",
      "0: 480x640 2 persons, 1 train, 1 handbag, 1 chair, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 309.79, 156.35, 418.13, 422.60 for dataset/inria/Train/pos\\person_162.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 216.28, 153.84, 296.42, 420.60 for dataset/inria/Train/pos\\person_162.png\n",
      "Class: 56.0, Confidence: 0.82, Bounding Box: 566.39, 316.11, 639.79, 431.56 for dataset/inria/Train/pos\\person_162.png\n",
      "Class: 6.0, Confidence: 0.34, Bounding Box: 4.63, 40.30, 640.00, 424.43 for dataset/inria/Train/pos\\person_162.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 188.21, 274.24, 245.88, 349.43 for dataset/inria/Train/pos\\person_162.png\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 1 stop sign, 1 handbag, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 24.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 275.65, 207.74, 354.07, 364.84 for dataset/inria/Train/pos\\person_165.png\n",
      "Class: 2.0, Confidence: 0.86, Bounding Box: 364.23, 177.65, 458.98, 222.01 for dataset/inria/Train/pos\\person_165.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 263.66, 179.67, 285.33, 249.08 for dataset/inria/Train/pos\\person_165.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 323.73, 177.07, 359.63, 272.40 for dataset/inria/Train/pos\\person_165.png\n",
      "Class: 11.0, Confidence: 0.63, Bounding Box: 169.29, 67.32, 223.89, 123.98 for dataset/inria/Train/pos\\person_165.png\n",
      "Class: 26.0, Confidence: 0.39, Bounding Box: 326.02, 280.32, 354.33, 325.43 for dataset/inria/Train/pos\\person_165.png\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 338.23, 162.34, 417.27, 374.69 for dataset/inria/Train/pos\\person_166.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 247.75, 157.55, 333.26, 381.19 for dataset/inria/Train/pos\\person_166.png\n",
      "Class: 26.0, Confidence: 0.57, Bounding Box: 244.30, 278.88, 269.34, 343.90 for dataset/inria/Train/pos\\person_166.png\n",
      "Class: 2.0, Confidence: 0.56, Bounding Box: 507.61, 198.47, 539.20, 241.12 for dataset/inria/Train/pos\\person_166.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 314.46, 168.17, 341.58, 251.93 for dataset/inria/Train/pos\\person_166.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 141.94, 450.81, 167.02, 479.76 for dataset/inria/Train/pos\\person_166.png\n",
      "\n",
      "0: 480x640 3 persons, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 262.64, 184.70, 317.75, 358.06 for dataset/inria/Train/pos\\person_167.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 308.58, 162.96, 385.89, 364.40 for dataset/inria/Train/pos\\person_167.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 362.97, 180.52, 388.13, 284.66 for dataset/inria/Train/pos\\person_167.png\n",
      "\n",
      "0: 480x640 5 persons, 1 sports ball, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 315.15, 197.99, 360.95, 324.90 for dataset/inria/Train/pos\\person_168.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 248.71, 209.72, 288.92, 326.90 for dataset/inria/Train/pos\\person_168.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 483.01, 193.30, 502.22, 249.39 for dataset/inria/Train/pos\\person_168.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 467.66, 203.59, 479.03, 239.60 for dataset/inria/Train/pos\\person_168.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 183.03, 195.40, 205.44, 234.66 for dataset/inria/Train/pos\\person_168.png\n",
      "Class: 32.0, Confidence: 0.28, Bounding Box: 304.44, 116.33, 323.89, 134.62 for dataset/inria/Train/pos\\person_168.png\n",
      "\n",
      "0: 480x640 2 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 278.39, 131.99, 392.34, 398.76 for dataset/inria/Train/pos\\person_169.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 179.54, 126.97, 276.47, 408.57 for dataset/inria/Train/pos\\person_169.png\n",
      "\n",
      "0: 480x640 3 persons, 2 cars, 1 chair, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 211.26, 156.38, 298.25, 394.94 for dataset/inria/Train/pos\\person_170.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 437.00, 161.39, 495.53, 377.82 for dataset/inria/Train/pos\\person_170.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 301.42, 163.36, 345.30, 279.17 for dataset/inria/Train/pos\\person_170.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 404.96, 164.91, 533.60, 255.65 for dataset/inria/Train/pos\\person_170.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 405.36, 186.87, 458.58, 255.94 for dataset/inria/Train/pos\\person_170.png\n",
      "Class: 56.0, Confidence: 0.36, Bounding Box: 0.05, 294.94, 14.00, 426.40 for dataset/inria/Train/pos\\person_170.png\n",
      "\n",
      "0: 480x640 4 persons, 4 chairs, 3 dining tables, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 205.25, 183.74, 295.18, 401.17 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 296.90, 166.03, 383.83, 415.72 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 56.0, Confidence: 0.86, Bounding Box: 556.97, 337.83, 638.04, 463.60 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 56.0, Confidence: 0.82, Bounding Box: 512.73, 314.96, 574.46, 450.58 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 569.29, 202.68, 638.42, 328.10 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 56.0, Confidence: 0.55, Bounding Box: 186.28, 256.11, 222.16, 331.30 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 56.0, Confidence: 0.43, Bounding Box: 361.41, 264.51, 486.31, 479.52 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 60.0, Confidence: 0.34, Bounding Box: 546.50, 324.39, 639.72, 372.42 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 60.0, Confidence: 0.31, Bounding Box: 546.72, 324.31, 639.70, 355.45 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 60.0, Confidence: 0.29, Bounding Box: 547.47, 323.91, 639.79, 427.03 for dataset/inria/Train/pos\\person_171.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 613.73, 182.08, 639.85, 293.01 for dataset/inria/Train/pos\\person_171.png\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 1 stop sign, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 290.50, 176.16, 366.54, 377.65 for dataset/inria/Train/pos\\person_172.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 379.51, 185.37, 457.99, 376.60 for dataset/inria/Train/pos\\person_172.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 239.05, 184.47, 291.26, 326.45 for dataset/inria/Train/pos\\person_172.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 0.25, 30.42, 143.39, 321.91 for dataset/inria/Train/pos\\person_172.png\n",
      "Class: 2.0, Confidence: 0.50, Bounding Box: 355.30, 198.02, 395.38, 255.05 for dataset/inria/Train/pos\\person_172.png\n",
      "Class: 11.0, Confidence: 0.48, Bounding Box: 544.38, 136.44, 563.99, 157.58 for dataset/inria/Train/pos\\person_172.png\n",
      "\n",
      "0: 480x640 9 persons, 1 umbrella, 1 chair, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 384.40, 134.22, 472.35, 426.32 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 304.88, 138.84, 387.51, 411.33 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 596.42, 158.60, 609.62, 196.73 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 25.0, Confidence: 0.50, Bounding Box: 148.78, 85.10, 179.97, 202.35 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 487.67, 176.33, 498.37, 197.77 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 73.70, 163.39, 95.58, 185.12 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 202.02, 164.47, 214.38, 193.05 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 212.90, 165.63, 225.56, 193.68 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 73.62, 163.25, 95.80, 194.89 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 132.29, 174.89, 170.07, 221.05 for dataset/inria/Train/pos\\person_173.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 73.66, 162.86, 95.75, 209.16 for dataset/inria/Train/pos\\person_173.png\n",
      "\n",
      "0: 480x640 3 persons, 1 potted plant, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 417.76, 226.06, 499.25, 370.50 for dataset/inria/Train/pos\\person_174.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 180.54, 185.86, 260.66, 369.81 for dataset/inria/Train/pos\\person_174.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 316.59, 102.02, 433.37, 372.36 for dataset/inria/Train/pos\\person_174.png\n",
      "Class: 58.0, Confidence: 0.76, Bounding Box: 408.58, 90.98, 486.23, 218.84 for dataset/inria/Train/pos\\person_174.png\n",
      "\n",
      "0: 480x640 7 persons, 2 backpacks, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 471.64, 153.54, 518.14, 301.55 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 344.54, 138.99, 404.28, 304.18 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 187.71, 153.13, 249.46, 326.63 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 103.89, 140.81, 177.82, 327.74 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 548.76, 136.72, 616.32, 327.94 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 76.45, 156.97, 110.97, 254.26 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 50.46, 160.04, 67.88, 221.56 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 24.0, Confidence: 0.48, Bounding Box: 85.68, 172.74, 105.40, 203.34 for dataset/inria/Train/pos\\person_175.png\n",
      "Class: 24.0, Confidence: 0.43, Bounding Box: 360.12, 163.91, 400.16, 217.40 for dataset/inria/Train/pos\\person_175.png\n",
      "\n",
      "0: 480x640 8 persons, 1 umbrella, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 335.92, 103.68, 473.60, 422.81 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 243.40, 119.67, 352.52, 422.31 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 69.69, 141.10, 114.36, 261.84 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 200.74, 135.92, 260.00, 287.46 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 18.70, 142.45, 59.57, 260.84 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 160.12, 151.63, 201.06, 225.34 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 241.63, 135.39, 263.71, 166.17 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 25.0, Confidence: 0.33, Bounding Box: 481.09, 0.27, 572.66, 190.10 for dataset/inria/Train/pos\\person_176.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 158.65, 151.98, 203.72, 287.51 for dataset/inria/Train/pos\\person_176.png\n",
      "\n",
      "0: 480x640 6 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.6ms preprocess, 11.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 333.66, 136.05, 411.87, 390.15 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 244.52, 128.60, 307.88, 398.32 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 527.55, 152.79, 569.52, 285.44 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 40.45, 146.42, 77.71, 228.06 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 236.89, 273.00, 257.98, 324.25 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 520.47, 228.86, 544.94, 262.61 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 288.92, 155.42, 324.34, 246.85 for dataset/inria/Train/pos\\person_177.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 289.30, 154.93, 336.65, 279.59 for dataset/inria/Train/pos\\person_177.png\n",
      "\n",
      "0: 480x640 6 persons, 1 motorcycle, 1 traffic light, 1 bench, 16.1ms\n",
      "Speed: 1.9ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 156.70, 132.78, 269.49, 417.04 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 445.16, 153.98, 533.53, 417.68 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 366.77, 150.82, 463.36, 399.61 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 333.64, 170.86, 346.96, 211.05 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 51.86, 172.54, 75.80, 218.28 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 3.0, Confidence: 0.56, Bounding Box: 0.06, 205.13, 73.97, 320.06 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 542.22, 171.52, 555.29, 203.72 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 13.0, Confidence: 0.37, Bounding Box: 577.83, 197.08, 638.83, 236.83 for dataset/inria/Train/pos\\person_178.png\n",
      "Class: 9.0, Confidence: 0.28, Bounding Box: 573.94, 126.10, 587.27, 155.47 for dataset/inria/Train/pos\\person_178.png\n",
      "\n",
      "0: 480x640 1 person, 1 handbag, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 280.28, 145.37, 410.66, 440.80 for dataset/inria/Train/pos\\person_179.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 245.11, 213.81, 303.85, 297.52 for dataset/inria/Train/pos\\person_179.png\n",
      "\n",
      "0: 480x640 5 persons, 1 handbag, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 375.07, 176.73, 461.78, 378.08 for dataset/inria/Train/pos\\person_181.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 66.40, 145.83, 208.19, 479.17 for dataset/inria/Train/pos\\person_181.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 279.97, 181.21, 336.67, 382.97 for dataset/inria/Train/pos\\person_181.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 0.00, 214.90, 21.33, 471.45 for dataset/inria/Train/pos\\person_181.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 195.64, 180.86, 218.19, 238.76 for dataset/inria/Train/pos\\person_181.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 266.85, 295.35, 306.90, 340.01 for dataset/inria/Train/pos\\person_181.png\n",
      "\n",
      "0: 480x640 6 persons, 1 car, 1 backpack, 1 handbag, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 392.66, 177.14, 463.53, 380.07 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 310.05, 187.45, 383.92, 388.56 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 2.0, Confidence: 0.69, Bounding Box: 0.06, 206.87, 32.34, 260.24 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 243.86, 200.99, 297.31, 370.48 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 202.29, 232.59, 260.37, 373.84 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 345.75, 178.50, 395.63, 368.63 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 24.0, Confidence: 0.58, Bounding Box: 198.54, 254.29, 222.77, 314.78 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 455.93, 192.30, 531.03, 345.60 for dataset/inria/Train/pos\\person_185.png\n",
      "Class: 26.0, Confidence: 0.52, Bounding Box: 269.22, 260.52, 289.78, 310.20 for dataset/inria/Train/pos\\person_185.png\n",
      "\n",
      "0: 480x640 11 persons, 1 backpack, 5 handbags, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 337.91, 231.65, 403.19, 428.55 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 24.0, Confidence: 0.82, Bounding Box: 388.86, 289.95, 430.84, 364.34 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 492.21, 213.58, 557.20, 400.17 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 197.19, 194.92, 234.95, 292.25 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 287.57, 224.68, 343.24, 415.79 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 534.14, 218.77, 580.96, 393.23 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 395.34, 214.71, 457.16, 294.20 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 409.97, 267.04, 455.37, 433.85 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 230.35, 207.89, 263.14, 307.41 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 271.27, 224.21, 306.96, 341.13 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 184.71, 187.94, 211.52, 270.80 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 255.87, 202.73, 282.39, 299.54 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 319.80, 253.18, 342.69, 328.22 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 333.30, 264.51, 365.99, 334.66 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 333.89, 273.27, 360.14, 335.09 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 548.06, 251.91, 581.59, 308.74 for dataset/inria/Train/pos\\person_187.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 334.82, 300.31, 359.12, 334.29 for dataset/inria/Train/pos\\person_187.png\n",
      "\n",
      "0: 480x640 16 persons, 4 chairs, 1 dining table, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 319.38, 168.47, 402.29, 377.84 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 64.05, 179.03, 113.03, 359.12 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 183.92, 182.62, 226.77, 282.90 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 264.30, 187.45, 320.37, 378.76 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 23.72, 177.59, 76.09, 347.49 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 234.71, 178.68, 266.24, 279.48 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 469.01, 176.41, 495.63, 233.94 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 120.18, 187.86, 159.73, 284.67 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 576.66, 172.26, 638.14, 332.73 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 533.27, 183.17, 574.26, 293.48 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 56.0, Confidence: 0.47, Bounding Box: 491.19, 238.66, 526.69, 288.61 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 56.0, Confidence: 0.43, Bounding Box: 1.06, 267.83, 43.68, 335.25 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 56.0, Confidence: 0.35, Bounding Box: 444.43, 233.67, 478.50, 289.80 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 451.90, 177.52, 471.04, 233.15 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 502.48, 177.10, 542.74, 243.57 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 56.0, Confidence: 0.32, Bounding Box: 468.75, 240.98, 498.78, 289.36 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 115.93, 180.35, 136.46, 227.93 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 225.15, 179.13, 243.76, 233.07 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 144.76, 173.22, 170.63, 238.27 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 60.0, Confidence: 0.26, Bounding Box: 447.35, 234.18, 526.51, 292.18 for dataset/inria/Train/pos\\person_195.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 171.33, 174.35, 190.06, 233.57 for dataset/inria/Train/pos\\person_195.png\n",
      "\n",
      "0: 480x640 12 persons, 2 handbags, 1 potted plant, 13.0ms\n",
      "Speed: 8.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 185.94, 98.22, 287.61, 389.20 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 332.01, 77.94, 445.88, 396.69 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 85.99, 95.29, 117.66, 206.44 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 434.95, 102.18, 473.02, 209.35 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 495.05, 111.31, 525.89, 202.05 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 114.47, 99.25, 159.82, 205.71 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 466.15, 106.34, 495.11, 201.77 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 272.26, 102.61, 304.34, 207.82 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 58.0, Confidence: 0.56, Bounding Box: 575.27, 100.41, 636.82, 199.45 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 199.09, 101.27, 230.76, 152.26 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 163.20, 104.26, 199.78, 205.60 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 26.0, Confidence: 0.39, Bounding Box: 161.35, 124.45, 181.11, 152.83 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 289.90, 97.48, 321.27, 206.16 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 146.49, 111.97, 168.87, 203.87 for dataset/inria/Train/pos\\person_196.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 306.06, 248.63, 359.18, 322.64 for dataset/inria/Train/pos\\person_196.png\n",
      "\n",
      "0: 480x640 8 persons, 1 handbag, 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 521.40, 180.96, 581.04, 290.96 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 214.69, 177.20, 274.87, 385.22 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 136.80, 191.09, 214.74, 400.34 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 459.19, 191.90, 508.30, 286.29 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 390.67, 190.35, 451.21, 348.14 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 268.36, 183.00, 316.95, 372.40 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 294.75, 174.97, 358.75, 391.80 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 345.27, 181.56, 402.51, 384.26 for dataset/inria/Train/pos\\person_199.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 487.10, 208.27, 506.66, 255.05 for dataset/inria/Train/pos\\person_199.png\n",
      "\n",
      "0: 480x640 17 persons, 3 cars, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 169.17, 231.93, 203.75, 331.37 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 607.15, 230.73, 635.67, 302.45 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 136.30, 231.27, 171.17, 331.64 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 108.59, 228.10, 144.59, 330.86 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 450.29, 227.70, 482.10, 308.47 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 479.38, 225.49, 507.73, 305.34 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 594.27, 225.83, 613.36, 292.20 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 512.17, 227.74, 539.99, 309.17 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 280.04, 234.79, 309.50, 313.10 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 380.24, 226.60, 409.06, 312.20 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 249.51, 233.56, 279.57, 311.90 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 316.67, 225.11, 351.81, 343.39 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 201.25, 230.90, 225.56, 308.67 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 2.0, Confidence: 0.48, Bounding Box: 408.86, 242.84, 435.04, 268.36 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 224.89, 239.60, 248.73, 308.98 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 2.0, Confidence: 0.39, Bounding Box: 0.10, 254.14, 55.78, 283.77 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 0.00, 254.66, 39.14, 283.80 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 529.44, 227.64, 549.10, 303.90 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 341.65, 234.94, 358.06, 312.06 for dataset/inria/Train/pos\\person_201.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 362.37, 230.68, 384.31, 310.33 for dataset/inria/Train/pos\\person_201.png\n",
      "\n",
      "0: 480x640 11 persons, 1 car, 1 traffic light, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 424.17, 152.53, 506.88, 374.16 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 348.43, 110.28, 407.39, 271.21 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 197.04, 108.75, 277.33, 377.34 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 536.58, 107.55, 590.96, 252.15 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 272.42, 117.65, 315.95, 239.86 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 473.50, 118.80, 519.08, 221.34 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 323.62, 114.55, 352.01, 191.37 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 2.0, Confidence: 0.74, Bounding Box: 175.97, 137.57, 229.89, 199.62 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 26.0, Confidence: 0.69, Bounding Box: 191.99, 193.74, 222.65, 260.98 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 66.25, 123.88, 84.93, 194.76 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 100.65, 125.81, 151.11, 199.05 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 9.0, Confidence: 0.30, Bounding Box: 511.46, 36.80, 523.57, 53.14 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 94.30, 123.53, 113.58, 187.72 for dataset/inria/Train/pos\\person_202.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 304.04, 117.00, 326.47, 190.19 for dataset/inria/Train/pos\\person_202.png\n",
      "\n",
      "0: 480x640 12 persons, 1 car, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 1.19, 197.65, 43.95, 320.03 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 188.46, 199.61, 227.06, 311.12 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 231.68, 204.78, 262.41, 310.27 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 468.83, 191.90, 517.52, 314.61 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 344.20, 202.61, 389.50, 311.45 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 2.0, Confidence: 0.66, Bounding Box: 144.66, 209.24, 197.09, 276.78 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 409.41, 194.79, 441.61, 304.14 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 440.69, 199.91, 474.99, 308.85 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 264.25, 197.25, 310.80, 306.75 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 305.87, 201.67, 335.79, 304.46 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 329.85, 202.04, 357.35, 304.59 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 376.54, 200.12, 403.28, 305.58 for dataset/inria/Train/pos\\person_203.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 247.66, 192.51, 278.28, 306.13 for dataset/inria/Train/pos\\person_203.png\n",
      "\n",
      "0: 480x640 9 persons, 3 umbrellas, 2 chairs, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 302.63, 107.81, 418.57, 443.75 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 418.16, 93.50, 465.91, 236.07 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 21.10, 103.10, 125.15, 267.67 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 291.01, 102.67, 328.18, 152.76 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 207.77, 98.01, 240.06, 153.23 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 222.64, 110.35, 272.89, 155.34 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 56.0, Confidence: 0.56, Bounding Box: 50.20, 193.58, 224.55, 466.43 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 370.88, 101.85, 394.11, 158.20 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 0.01, 129.99, 23.81, 227.95 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 25.0, Confidence: 0.36, Bounding Box: 302.49, 289.53, 330.93, 400.67 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 56.0, Confidence: 0.31, Bounding Box: 604.75, 172.79, 639.69, 261.70 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 25.0, Confidence: 0.30, Bounding Box: 3.89, 5.12, 215.09, 75.56 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 25.0, Confidence: 0.26, Bounding Box: 1.07, 3.71, 260.05, 155.65 for dataset/inria/Train/pos\\person_208.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 225.05, 92.98, 261.42, 144.53 for dataset/inria/Train/pos\\person_208.png\n",
      "\n",
      "0: 480x640 10 persons, 2 chairs, 12.7ms\n",
      "Speed: 1.1ms preprocess, 12.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 298.37, 146.80, 413.06, 479.76 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 228.68, 148.72, 307.77, 384.48 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 572.49, 148.20, 639.73, 479.18 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 402.59, 158.47, 435.00, 284.43 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 137.15, 169.99, 178.16, 291.54 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 527.62, 163.00, 559.34, 214.89 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 598.04, 158.81, 624.72, 205.68 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 56.0, Confidence: 0.45, Bounding Box: 546.77, 256.49, 594.07, 371.43 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 192.73, 170.45, 225.45, 222.12 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 219.25, 155.68, 244.30, 218.43 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 56.0, Confidence: 0.33, Bounding Box: 180.56, 238.98, 235.85, 316.76 for dataset/inria/Train/pos\\person_209.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 410.80, 165.00, 431.86, 284.38 for dataset/inria/Train/pos\\person_209.png\n",
      "\n",
      "0: 480x640 2 persons, 13.3ms\n",
      "Speed: 1.6ms preprocess, 13.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 397.16, 165.15, 468.13, 415.01 for dataset/inria/Train/pos\\person_211.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 220.51, 40.44, 295.43, 195.33 for dataset/inria/Train/pos\\person_211.png\n",
      "\n",
      "0: 480x640 5 persons, 2 bicycles, 12.7ms\n",
      "Speed: 1.5ms preprocess, 12.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 377.67, 135.48, 572.78, 437.05 for dataset/inria/Train/pos\\person_214.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 359.23, 130.46, 431.13, 426.16 for dataset/inria/Train/pos\\person_214.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 183.98, 130.42, 307.25, 402.14 for dataset/inria/Train/pos\\person_214.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 384.92, 101.79, 425.13, 142.06 for dataset/inria/Train/pos\\person_214.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 144.20, 179.93, 188.61, 281.07 for dataset/inria/Train/pos\\person_214.png\n",
      "Class: 1.0, Confidence: 0.26, Bounding Box: 497.53, 319.42, 639.70, 429.32 for dataset/inria/Train/pos\\person_214.png\n",
      "Class: 1.0, Confidence: 0.25, Bounding Box: 258.12, 242.36, 384.22, 437.13 for dataset/inria/Train/pos\\person_214.png\n",
      "\n",
      "0: 480x640 9 persons, 1 chair, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 361.33, 129.39, 469.51, 437.04 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 186.80, 143.87, 286.10, 263.69 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 292.38, 186.64, 369.41, 431.10 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 105.32, 154.39, 155.48, 310.38 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 63.33, 156.45, 113.48, 273.87 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 35.77, 149.83, 60.60, 212.33 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 59.65, 146.75, 89.49, 228.74 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 146.67, 174.00, 181.50, 303.01 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 56.0, Confidence: 0.37, Bounding Box: 189.09, 256.97, 312.49, 457.26 for dataset/inria/Train/pos\\person_219.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 485.88, 149.74, 515.46, 206.93 for dataset/inria/Train/pos\\person_219.png\n",
      "\n",
      "0: 480x640 4 persons, 1 handbag, 59.5ms\n",
      "Speed: 2.0ms preprocess, 59.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 352.04, 211.64, 423.91, 393.49 for dataset/inria/Train/pos\\person_220.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 231.80, 117.53, 289.05, 266.16 for dataset/inria/Train/pos\\person_220.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 312.20, 119.42, 372.55, 264.66 for dataset/inria/Train/pos\\person_220.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 207.63, 170.37, 242.73, 254.88 for dataset/inria/Train/pos\\person_220.png\n",
      "Class: 26.0, Confidence: 0.52, Bounding Box: 309.36, 196.90, 334.87, 241.70 for dataset/inria/Train/pos\\person_220.png\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.93, Bounding Box: 497.82, 175.68, 639.60, 349.79 for dataset/inria/Train/pos\\person_221.png\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 221.33, 145.60, 378.86, 479.69 for dataset/inria/Train/pos\\person_221.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 322.95, 143.39, 405.25, 356.29 for dataset/inria/Train/pos\\person_221.png\n",
      "\n",
      "0: 480x640 6 persons, 2 handbags, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 346.79, 60.99, 487.82, 405.59 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 196.81, 60.58, 348.05, 435.04 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 26.0, Confidence: 0.74, Bounding Box: 338.35, 252.40, 381.58, 331.61 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 297.78, 67.81, 368.03, 309.96 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 443.10, 47.24, 470.40, 117.99 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 466.12, 52.89, 481.47, 116.42 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 297.89, 68.17, 367.22, 237.57 for dataset/inria/Train/pos\\person_223.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 196.80, 274.40, 227.86, 377.58 for dataset/inria/Train/pos\\person_223.png\n",
      "\n",
      "0: 480x640 4 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 233.24, 119.49, 332.45, 435.62 for dataset/inria/Train/pos\\person_224.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 471.59, 172.03, 553.76, 390.63 for dataset/inria/Train/pos\\person_224.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 375.42, 122.45, 474.45, 408.09 for dataset/inria/Train/pos\\person_224.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 314.66, 144.48, 384.68, 392.24 for dataset/inria/Train/pos\\person_224.png\n",
      "\n",
      "0: 480x640 4 persons, 5 handbags, 1 cell phone, 18.0ms\n",
      "Speed: 1.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 207.73, 68.93, 332.04, 431.43 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 65.47, 18.90, 176.22, 356.76 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 392.57, 62.25, 530.45, 444.40 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 67.0, Confidence: 0.62, Bounding Box: 314.94, 165.86, 333.06, 178.81 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 26.0, Confidence: 0.48, Bounding Box: 517.54, 198.71, 542.53, 254.08 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 26.0, Confidence: 0.46, Bounding Box: 518.44, 212.41, 542.12, 252.97 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 26.0, Confidence: 0.35, Bounding Box: 278.18, 214.03, 331.15, 309.76 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 389.54, 48.59, 451.97, 307.04 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 272.71, 186.49, 331.91, 308.69 for dataset/inria/Train/pos\\person_225.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 515.82, 154.39, 543.07, 256.72 for dataset/inria/Train/pos\\person_225.png\n",
      "\n",
      "0: 480x640 10 persons, 1 dog, 2 handbags, 1 potted plant, 22.5ms\n",
      "Speed: 2.0ms preprocess, 22.5ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 219.95, 102.15, 308.70, 401.62 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 282.80, 104.24, 407.48, 439.77 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 131.76, 100.35, 178.06, 208.79 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 16.0, Confidence: 0.73, Bounding Box: 0.09, 218.48, 44.98, 292.19 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 367.20, 116.32, 438.65, 418.53 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 584.01, 114.43, 602.85, 159.76 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 372.20, 102.63, 394.54, 161.12 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 285.09, 83.01, 323.63, 169.35 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 58.0, Confidence: 0.49, Bounding Box: 454.65, 104.29, 502.82, 179.31 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 26.0, Confidence: 0.35, Bounding Box: 366.61, 291.66, 429.11, 409.69 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 284.06, 83.45, 313.70, 165.41 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 387.95, 117.13, 439.57, 312.13 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 162.64, 152.48, 189.95, 194.09 for dataset/inria/Train/pos\\person_228.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 348.59, 93.94, 360.26, 103.40 for dataset/inria/Train/pos\\person_228.png\n",
      "\n",
      "0: 480x640 9 persons, 1 umbrella, 2 handbags, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 223.85, 88.08, 339.00, 478.86 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.05, 100.42, 96.51, 455.87 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 348.47, 90.36, 474.94, 479.04 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 320.45, 100.90, 365.60, 245.18 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 26.0, Confidence: 0.76, Bounding Box: 395.26, 305.84, 479.20, 391.10 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 25.0, Confidence: 0.65, Bounding Box: 589.04, 47.80, 639.98, 92.05 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 238.31, 95.66, 277.96, 164.27 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 501.48, 93.23, 524.98, 159.48 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 435.24, 88.61, 495.24, 196.33 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 26.0, Confidence: 0.48, Bounding Box: 395.35, 255.59, 479.13, 390.15 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 435.01, 89.23, 494.44, 241.32 for dataset/inria/Train/pos\\person_229.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 476.36, 86.93, 501.61, 158.18 for dataset/inria/Train/pos\\person_229.png\n",
      "\n",
      "0: 480x640 6 persons, 4 handbags, 2 potted plants, 20.5ms\n",
      "Speed: 1.0ms preprocess, 20.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 11.82, 48.55, 105.97, 251.22 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 269.73, 61.22, 384.33, 476.88 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 238.66, 74.21, 266.65, 144.67 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 75.57, 72.19, 230.13, 442.33 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 58.0, Confidence: 0.77, Bounding Box: 581.10, 138.16, 619.65, 219.42 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 358.51, 100.60, 451.41, 460.75 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 26.0, Confidence: 0.72, Bounding Box: 160.80, 159.81, 208.77, 280.68 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 100.51, 55.37, 169.07, 160.33 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 58.0, Confidence: 0.64, Bounding Box: 554.54, 135.95, 586.16, 208.98 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 26.0, Confidence: 0.46, Bounding Box: 250.89, 211.69, 282.40, 292.43 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 434.29, 169.68, 473.75, 265.50 for dataset/inria/Train/pos\\person_231.png\n",
      "Class: 26.0, Confidence: 0.33, Bounding Box: 161.27, 204.93, 206.53, 280.68 for dataset/inria/Train/pos\\person_231.png\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 39.42, 80.74, 114.56, 238.20 for dataset/inria/Train/pos\\person_232.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 187.09, 13.98, 364.23, 455.04 for dataset/inria/Train/pos\\person_232.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 343.76, 66.01, 492.94, 454.88 for dataset/inria/Train/pos\\person_232.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 0.05, 72.06, 36.09, 208.37 for dataset/inria/Train/pos\\person_232.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 146.58, 0.07, 219.75, 182.84 for dataset/inria/Train/pos\\person_232.png\n",
      "\n",
      "0: 480x640 4 persons, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 175.22, 163.59, 267.11, 409.88 for dataset/inria/Train/pos\\person_233.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 414.94, 166.25, 509.02, 399.56 for dataset/inria/Train/pos\\person_233.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 254.78, 167.43, 285.22, 246.45 for dataset/inria/Train/pos\\person_233.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 87.52, 165.51, 96.54, 186.96 for dataset/inria/Train/pos\\person_233.png\n",
      "\n",
      "0: 480x640 5 persons, 1 handbag, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 333.15, 167.77, 413.93, 362.44 for dataset/inria/Train/pos\\person_234.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 230.97, 171.23, 306.41, 363.00 for dataset/inria/Train/pos\\person_234.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 116.31, 169.10, 178.43, 328.64 for dataset/inria/Train/pos\\person_234.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 214.24, 172.03, 256.56, 323.47 for dataset/inria/Train/pos\\person_234.png\n",
      "Class: 26.0, Confidence: 0.49, Bounding Box: 292.98, 275.17, 314.05, 321.60 for dataset/inria/Train/pos\\person_234.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 0.07, 175.00, 7.35, 214.04 for dataset/inria/Train/pos\\person_234.png\n",
      "\n",
      "0: 480x640 3 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.94, Bounding Box: 361.05, 79.11, 491.31, 427.64 for dataset/inria/Train/pos\\person_235.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 194.06, 116.81, 329.83, 448.14 for dataset/inria/Train/pos\\person_235.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 245.21, 97.75, 300.44, 169.85 for dataset/inria/Train/pos\\person_235.png\n",
      "\n",
      "0: 480x640 7 persons, 4 handbags, 1 tie, 2 potted plants, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 222.20, 145.25, 322.20, 407.47 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 307.04, 118.80, 409.90, 411.95 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 453.90, 137.10, 487.43, 226.72 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 500.66, 127.83, 540.13, 225.46 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 548.62, 137.72, 578.41, 201.90 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 299.51, 136.79, 333.91, 189.58 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 26.0, Confidence: 0.71, Bounding Box: 378.41, 222.98, 419.39, 284.18 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 58.0, Confidence: 0.70, Bounding Box: 132.99, 216.01, 172.94, 358.45 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 257.35, 128.27, 293.45, 183.32 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 27.0, Confidence: 0.43, Bounding Box: 260.95, 195.60, 278.39, 240.66 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 58.0, Confidence: 0.39, Bounding Box: 114.61, 224.56, 146.49, 354.18 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 375.09, 171.80, 418.65, 284.45 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 294.14, 199.24, 323.04, 285.59 for dataset/inria/Train/pos\\person_237.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 294.03, 228.39, 321.02, 285.09 for dataset/inria/Train/pos\\person_237.png\n",
      "\n",
      "0: 480x640 6 persons, 1 dog, 3 chairs, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 342.77, 91.61, 461.00, 423.50 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 185.80, 124.32, 301.82, 414.42 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 16.0, Confidence: 0.84, Bounding Box: 246.67, 280.75, 352.10, 438.25 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 56.0, Confidence: 0.80, Bounding Box: 558.90, 164.96, 606.73, 221.36 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 298.09, 98.12, 320.91, 162.36 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 56.0, Confidence: 0.66, Bounding Box: 472.95, 154.70, 523.49, 205.09 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 56.0, Confidence: 0.63, Bounding Box: 528.40, 161.82, 566.05, 217.23 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 83.90, 91.70, 106.99, 182.78 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 101.27, 96.85, 121.97, 162.89 for dataset/inria/Train/pos\\person_238.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 120.85, 96.92, 134.04, 143.48 for dataset/inria/Train/pos\\person_238.png\n",
      "\n",
      "0: 480x640 2 persons, 13.5ms\n",
      "Speed: 1.0ms preprocess, 13.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 273.62, 84.08, 394.13, 431.70 for dataset/inria/Train/pos\\person_239.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 203.19, 83.25, 311.97, 442.15 for dataset/inria/Train/pos\\person_239.png\n",
      "\n",
      "0: 480x640 4 persons, 1 handbag, 38.0ms\n",
      "Speed: 3.0ms preprocess, 38.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 0.29, 0.79, 106.72, 356.86 for dataset/inria/Train/pos\\person_240.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 318.57, 76.28, 455.19, 450.24 for dataset/inria/Train/pos\\person_240.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 205.77, 93.66, 339.44, 458.83 for dataset/inria/Train/pos\\person_240.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 63.88, 0.11, 154.75, 351.20 for dataset/inria/Train/pos\\person_240.png\n",
      "Class: 26.0, Confidence: 0.76, Bounding Box: 393.82, 132.71, 462.15, 278.71 for dataset/inria/Train/pos\\person_240.png\n",
      "\n",
      "0: 480x640 2 persons, 5 chairs, 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 216.27, 89.91, 328.37, 426.24 for dataset/inria/Train/pos\\person_241.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 350.21, 92.79, 466.65, 422.69 for dataset/inria/Train/pos\\person_241.png\n",
      "Class: 56.0, Confidence: 0.87, Bounding Box: 449.02, 253.88, 527.12, 375.68 for dataset/inria/Train/pos\\person_241.png\n",
      "Class: 56.0, Confidence: 0.87, Bounding Box: 579.01, 272.53, 639.79, 412.25 for dataset/inria/Train/pos\\person_241.png\n",
      "Class: 56.0, Confidence: 0.68, Bounding Box: 522.07, 259.65, 611.80, 386.19 for dataset/inria/Train/pos\\person_241.png\n",
      "Class: 56.0, Confidence: 0.46, Bounding Box: 515.18, 236.48, 580.81, 250.10 for dataset/inria/Train/pos\\person_241.png\n",
      "Class: 56.0, Confidence: 0.34, Bounding Box: 495.80, 216.32, 537.79, 246.05 for dataset/inria/Train/pos\\person_241.png\n",
      "\n",
      "0: 480x640 2 persons, 1 potted plant, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 365.23, 52.74, 483.58, 444.29 for dataset/inria/Train/pos\\person_242.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 228.33, 89.12, 362.42, 439.94 for dataset/inria/Train/pos\\person_242.png\n",
      "Class: 58.0, Confidence: 0.37, Bounding Box: 99.72, 20.94, 236.21, 190.92 for dataset/inria/Train/pos\\person_242.png\n",
      "\n",
      "0: 480x640 4 persons, 3 cars, 1 truck, 5 traffic lights, 1 handbag, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 185.19, 134.77, 274.78, 423.18 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 108.17, 137.04, 205.42, 427.14 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 2.0, Confidence: 0.81, Bounding Box: 267.29, 167.16, 322.33, 213.19 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 2.0, Confidence: 0.79, Bounding Box: 332.41, 168.97, 380.59, 203.74 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 9.0, Confidence: 0.70, Bounding Box: 43.62, 93.60, 58.58, 123.40 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 110.65, 160.15, 129.29, 213.61 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 26.0, Confidence: 0.64, Bounding Box: 105.99, 194.94, 157.45, 291.47 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 616.09, 160.51, 630.90, 202.34 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 9.0, Confidence: 0.60, Bounding Box: 75.50, 94.55, 97.91, 138.93 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 2.0, Confidence: 0.52, Bounding Box: 452.63, 167.22, 489.99, 191.58 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 9.0, Confidence: 0.49, Bounding Box: 27.32, 92.93, 42.51, 131.08 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 9.0, Confidence: 0.44, Bounding Box: 31.63, 93.26, 44.84, 131.03 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 7.0, Confidence: 0.32, Bounding Box: 382.09, 147.82, 413.15, 182.67 for dataset/inria/Train/pos\\person_243.png\n",
      "Class: 9.0, Confidence: 0.25, Bounding Box: 40.67, 92.96, 58.44, 131.87 for dataset/inria/Train/pos\\person_243.png\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 1 handbag, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 296.27, 58.16, 432.29, 447.62 for dataset/inria/Train/pos\\person_244.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 220.16, 110.03, 308.60, 414.57 for dataset/inria/Train/pos\\person_244.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 393.86, 87.04, 490.52, 437.84 for dataset/inria/Train/pos\\person_244.png\n",
      "Class: 26.0, Confidence: 0.55, Bounding Box: 452.16, 146.09, 490.98, 236.42 for dataset/inria/Train/pos\\person_244.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 443.95, 111.05, 490.68, 137.68 for dataset/inria/Train/pos\\person_244.png\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 14.0ms\n",
      "Speed: 13.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 102.36, 59.53, 258.59, 442.51 for dataset/inria/Train/pos\\person_245.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 231.64, 179.68, 340.03, 426.93 for dataset/inria/Train/pos\\person_245.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 292.26, 149.84, 484.35, 395.34 for dataset/inria/Train/pos\\person_245.png\n",
      "Class: 56.0, Confidence: 0.62, Bounding Box: 291.77, 196.75, 509.90, 471.49 for dataset/inria/Train/pos\\person_245.png\n",
      "\n",
      "0: 480x640 5 persons, 1 backpack, 1 potted plant, 14.2ms\n",
      "Speed: 1.0ms preprocess, 14.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 214.52, 171.48, 307.15, 427.04 for dataset/inria/Train/pos\\person_252.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 368.44, 134.10, 496.76, 429.55 for dataset/inria/Train/pos\\person_252.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 187.36, 167.36, 242.57, 404.23 for dataset/inria/Train/pos\\person_252.png\n",
      "Class: 24.0, Confidence: 0.50, Bounding Box: 384.61, 169.00, 476.87, 301.14 for dataset/inria/Train/pos\\person_252.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 187.45, 168.23, 242.90, 316.24 for dataset/inria/Train/pos\\person_252.png\n",
      "Class: 58.0, Confidence: 0.32, Bounding Box: 503.61, 207.94, 525.16, 251.48 for dataset/inria/Train/pos\\person_252.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 363.52, 179.09, 418.77, 427.38 for dataset/inria/Train/pos\\person_252.png\n",
      "\n",
      "0: 480x640 9 persons, 5 cars, 5 handbags, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 251.87, 133.64, 325.19, 347.94 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 332.27, 142.48, 458.51, 439.59 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 202.67, 180.74, 281.16, 363.36 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 153.92, 124.41, 207.47, 359.89 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 74.77, 140.22, 118.67, 237.59 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 2.0, Confidence: 0.74, Bounding Box: 0.06, 210.02, 62.59, 479.21 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 201.27, 143.94, 241.76, 219.51 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 0.00, 166.05, 94.60, 317.60 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 33.33, 136.70, 68.89, 172.95 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 451.00, 129.50, 521.13, 293.50 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 26.0, Confidence: 0.52, Bounding Box: 136.44, 151.74, 186.69, 247.65 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 135.25, 133.64, 170.03, 202.58 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 134.57, 134.70, 170.18, 251.24 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 0.00, 165.43, 94.90, 246.26 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 0.00, 201.58, 71.55, 399.32 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 199.66, 283.02, 216.59, 322.30 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 136.08, 187.49, 172.78, 249.06 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 135.13, 139.17, 173.06, 243.67 for dataset/inria/Train/pos\\person_253.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 449.96, 171.53, 474.90, 259.18 for dataset/inria/Train/pos\\person_253.png\n",
      "\n",
      "0: 480x640 3 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 364.95, 189.81, 456.38, 377.91 for dataset/inria/Train/pos\\person_254.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 276.39, 130.90, 358.12, 373.56 for dataset/inria/Train/pos\\person_254.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 206.18, 108.18, 306.78, 385.91 for dataset/inria/Train/pos\\person_254.png\n",
      "\n",
      "0: 480x640 2 persons, 1 bench, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 316.44, 106.97, 446.44, 431.24 for dataset/inria/Train/pos\\person_256.png\n",
      "Class: 13.0, Confidence: 0.58, Bounding Box: 192.70, 268.58, 497.19, 406.15 for dataset/inria/Train/pos\\person_256.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 242.03, 174.31, 334.88, 338.32 for dataset/inria/Train/pos\\person_256.png\n",
      "\n",
      "0: 480x640 2 persons, 1 stop sign, 1 bench, 1 backpack, 1 sports ball, 15.5ms\n",
      "Speed: 3.0ms preprocess, 15.5ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 251.90, 186.73, 332.53, 363.48 for dataset/inria/Train/pos\\person_257.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 345.24, 183.12, 410.05, 358.30 for dataset/inria/Train/pos\\person_257.png\n",
      "Class: 32.0, Confidence: 0.58, Bounding Box: 362.87, 113.50, 383.55, 132.57 for dataset/inria/Train/pos\\person_257.png\n",
      "Class: 24.0, Confidence: 0.50, Bounding Box: 279.87, 205.44, 318.86, 263.84 for dataset/inria/Train/pos\\person_257.png\n",
      "Class: 13.0, Confidence: 0.39, Bounding Box: 0.00, 291.95, 30.24, 388.57 for dataset/inria/Train/pos\\person_257.png\n",
      "Class: 11.0, Confidence: 0.35, Bounding Box: 69.65, 0.00, 147.52, 18.06 for dataset/inria/Train/pos\\person_257.png\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 1 traffic light, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 376.17, 175.29, 418.85, 324.33 for dataset/inria/Train/pos\\person_258.png\n",
      "Class: 2.0, Confidence: 0.63, Bounding Box: 509.42, 200.05, 639.81, 307.03 for dataset/inria/Train/pos\\person_258.png\n",
      "Class: 2.0, Confidence: 0.63, Bounding Box: 123.12, 196.71, 486.99, 327.32 for dataset/inria/Train/pos\\person_258.png\n",
      "Class: 9.0, Confidence: 0.32, Bounding Box: 576.61, 81.24, 605.15, 141.94 for dataset/inria/Train/pos\\person_258.png\n",
      "\n",
      "0: 480x640 6 persons, 4 cars, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 231.62, 185.65, 278.28, 330.39 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 182.14, 181.35, 223.19, 337.33 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 337.09, 173.04, 396.59, 336.54 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 275.40, 173.85, 321.03, 334.98 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 0.00, 171.54, 135.23, 247.98 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 267.44, 163.24, 280.47, 194.78 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 120.58, 178.02, 194.18, 233.10 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 87.45, 171.21, 133.38, 253.04 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 2.0, Confidence: 0.33, Bounding Box: 0.01, 217.74, 18.34, 269.82 for dataset/inria/Train/pos\\person_259.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 197.00, 159.84, 224.08, 197.19 for dataset/inria/Train/pos\\person_259.png\n",
      "\n",
      "0: 480x640 3 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 337.39, 178.75, 387.60, 314.48 for dataset/inria/Train/pos\\person_260.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 289.55, 200.31, 334.90, 318.21 for dataset/inria/Train/pos\\person_260.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 409.74, 188.31, 428.91, 238.91 for dataset/inria/Train/pos\\person_260.png\n",
      "\n",
      "0: 480x640 3 persons, 2 bicycles, 1 bench, 3 handbags, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 385.95, 69.25, 551.43, 478.73 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 93.35, 68.18, 236.31, 466.22 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 547.17, 98.56, 622.58, 307.26 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 1.0, Confidence: 0.75, Bounding Box: 195.97, 183.75, 410.89, 396.07 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 13.0, Confidence: 0.61, Bounding Box: 0.19, 210.28, 116.27, 456.01 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 26.0, Confidence: 0.51, Bounding Box: 84.36, 152.71, 151.89, 340.81 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 84.93, 198.42, 135.55, 339.42 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 1.0, Confidence: 0.25, Bounding Box: 222.27, 164.98, 303.54, 256.24 for dataset/inria/Train/pos\\person_266.png\n",
      "Class: 26.0, Confidence: 0.25, Bounding Box: 452.82, 297.91, 540.69, 382.00 for dataset/inria/Train/pos\\person_266.png\n",
      "\n",
      "0: 480x640 5 persons, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 32.06, 34.19, 163.62, 479.71 for dataset/inria/Train/pos\\person_269.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 433.72, 80.62, 591.19, 473.51 for dataset/inria/Train/pos\\person_269.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 153.54, 64.79, 277.96, 463.73 for dataset/inria/Train/pos\\person_269.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 260.84, 59.30, 381.52, 427.25 for dataset/inria/Train/pos\\person_269.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 369.91, 78.24, 472.13, 437.03 for dataset/inria/Train/pos\\person_269.png\n",
      "\n",
      "0: 480x640 4 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 74.04, 47.85, 231.07, 479.50 for dataset/inria/Train/pos\\person_270.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 331.19, 87.81, 462.28, 480.00 for dataset/inria/Train/pos\\person_270.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 209.75, 65.93, 330.16, 480.00 for dataset/inria/Train/pos\\person_270.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 3.80, 128.90, 35.19, 203.11 for dataset/inria/Train/pos\\person_270.png\n",
      "\n",
      "0: 480x640 8 persons, 1 backpack, 1 handbag, 1 bottle, 2 vases, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 485.29, 176.21, 540.95, 324.29 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 534.43, 183.54, 579.11, 321.37 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 300.99, 200.76, 375.24, 472.52 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 269.44, 181.43, 348.12, 411.40 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 390.15, 211.21, 495.17, 479.64 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 352.87, 166.05, 411.44, 293.47 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 26.0, Confidence: 0.62, Bounding Box: 458.09, 330.30, 504.58, 435.71 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 24.0, Confidence: 0.58, Bounding Box: 271.88, 232.82, 319.78, 325.13 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 358.01, 212.56, 428.78, 478.43 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 611.49, 284.18, 639.84, 441.84 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 75.0, Confidence: 0.38, Bounding Box: 114.64, 284.69, 136.14, 335.56 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 75.0, Confidence: 0.28, Bounding Box: 31.04, 258.95, 69.92, 334.05 for dataset/inria/Train/pos\\person_273.png\n",
      "Class: 39.0, Confidence: 0.26, Bounding Box: 400.53, 273.56, 440.09, 346.47 for dataset/inria/Train/pos\\person_273.png\n",
      "\n",
      "0: 480x640 6 persons, 1 handbag, 29.3ms\n",
      "Speed: 7.0ms preprocess, 29.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 439.17, 123.07, 585.74, 479.39 for dataset/inria/Train/pos\\person_274.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 282.78, 143.98, 359.78, 378.50 for dataset/inria/Train/pos\\person_274.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 366.22, 152.32, 388.74, 217.95 for dataset/inria/Train/pos\\person_274.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 393.69, 140.98, 423.17, 217.78 for dataset/inria/Train/pos\\person_274.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 339.79, 152.65, 372.93, 234.64 for dataset/inria/Train/pos\\person_274.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 116.68, 146.36, 128.67, 196.30 for dataset/inria/Train/pos\\person_274.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 314.61, 183.03, 360.05, 274.53 for dataset/inria/Train/pos\\person_274.png\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 32.9ms\n",
      "Speed: 1.6ms preprocess, 32.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 268.95, 183.79, 345.95, 397.49 for dataset/inria/Train/pos\\person_275.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 534.97, 192.65, 559.30, 253.74 for dataset/inria/Train/pos\\person_275.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 382.40, 200.30, 413.30, 220.93 for dataset/inria/Train/pos\\person_275.png\n",
      "\n",
      "0: 480x640 6 persons, 1 tennis racket, 23.2ms\n",
      "Speed: 2.7ms preprocess, 23.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 286.56, 360.06, 355.98, 479.58 for dataset/inria/Train/pos\\person_279.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 473.40, 204.08, 519.81, 365.27 for dataset/inria/Train/pos\\person_279.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 123.52, 186.95, 170.18, 307.68 for dataset/inria/Train/pos\\person_279.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 250.61, 213.08, 316.24, 401.94 for dataset/inria/Train/pos\\person_279.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 200.80, 177.04, 241.78, 325.98 for dataset/inria/Train/pos\\person_279.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 113.28, 60.23, 158.36, 139.25 for dataset/inria/Train/pos\\person_279.png\n",
      "Class: 38.0, Confidence: 0.40, Bounding Box: 630.80, 266.01, 639.77, 304.47 for dataset/inria/Train/pos\\person_279.png\n",
      "\n",
      "0: 480x640 3 persons, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 183.79, 142.46, 251.19, 327.13 for dataset/inria/Train/pos\\person_281.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 292.45, 143.21, 382.26, 399.26 for dataset/inria/Train/pos\\person_281.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 254.12, 142.47, 324.92, 389.13 for dataset/inria/Train/pos\\person_281.png\n",
      "\n",
      "0: 480x640 5 persons, 1 car, 1 bench, 1 handbag, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.93, Bounding Box: 489.14, 38.18, 639.51, 478.89 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 270.17, 129.90, 450.41, 468.47 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 396.66, 117.32, 432.97, 191.78 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 549.02, 111.41, 568.88, 158.42 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 429.94, 118.63, 445.36, 160.95 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 13.0, Confidence: 0.47, Bounding Box: 505.17, 121.38, 549.88, 143.36 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 2.0, Confidence: 0.41, Bounding Box: 266.05, 150.85, 290.64, 170.72 for dataset/inria/Train/pos\\person_283.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 295.78, 194.65, 370.32, 327.31 for dataset/inria/Train/pos\\person_283.png\n",
      "\n",
      "0: 480x640 6 persons, 2 benchs, 2 umbrellas, 2 handbags, 3 chairs, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 247.01, 147.96, 357.18, 369.44 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 562.12, 121.88, 606.56, 224.97 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 56.0, Confidence: 0.81, Bounding Box: 60.36, 253.96, 149.56, 359.61 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 56.0, Confidence: 0.80, Bounding Box: 181.04, 240.08, 255.40, 331.84 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 26.0, Confidence: 0.78, Bounding Box: 336.59, 275.87, 368.40, 352.88 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 618.05, 129.83, 639.83, 224.00 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 166.95, 132.79, 196.38, 210.40 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 0.00, 136.49, 21.79, 184.65 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 56.0, Confidence: 0.69, Bounding Box: 0.00, 269.77, 53.95, 404.93 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 13.0, Confidence: 0.61, Bounding Box: 502.52, 164.31, 568.00, 220.10 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 13.0, Confidence: 0.40, Bounding Box: 464.10, 172.62, 514.52, 224.96 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 25.0, Confidence: 0.35, Bounding Box: 0.11, 85.63, 86.22, 126.78 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 558.77, 123.00, 575.61, 178.56 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 25.0, Confidence: 0.30, Bounding Box: 0.67, 76.40, 141.43, 208.56 for dataset/inria/Train/pos\\person_284.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 251.67, 268.40, 283.00, 326.95 for dataset/inria/Train/pos\\person_284.png\n",
      "\n",
      "0: 480x640 2 persons, 1 handbag, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 273.22, 173.26, 345.63, 370.56 for dataset/inria/Train/pos\\person_285.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 94.74, 156.80, 112.95, 200.77 for dataset/inria/Train/pos\\person_285.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 320.73, 203.34, 343.51, 263.77 for dataset/inria/Train/pos\\person_285.png\n",
      "\n",
      "0: 480x640 11 persons, 1 truck, 1 traffic light, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 365.78, 129.50, 424.96, 269.53 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 468.34, 124.69, 493.99, 178.89 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 591.03, 128.53, 612.30, 179.21 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 173.86, 109.58, 346.85, 411.97 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 7.0, Confidence: 0.74, Bounding Box: 437.34, 107.52, 516.79, 177.28 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 324.34, 129.24, 338.45, 166.19 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 262.14, 134.02, 332.22, 327.43 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 256.30, 130.66, 346.20, 408.19 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 481.39, 127.08, 494.99, 177.23 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 340.02, 130.40, 349.27, 157.18 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 0.03, 188.36, 8.81, 249.10 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 337.80, 130.84, 348.19, 157.94 for dataset/inria/Train/pos\\person_286.png\n",
      "Class: 9.0, Confidence: 0.27, Bounding Box: 507.30, 63.17, 520.53, 82.70 for dataset/inria/Train/pos\\person_286.png\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 potted plant, 1 clock, 41.0ms\n",
      "Speed: 2.0ms preprocess, 41.0ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 261.99, 149.76, 356.07, 404.19 for dataset/inria/Train/pos\\person_287.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 203.53, 164.90, 293.14, 386.73 for dataset/inria/Train/pos\\person_287.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 445.62, 141.52, 496.87, 317.54 for dataset/inria/Train/pos\\person_287.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 415.46, 153.97, 443.23, 243.16 for dataset/inria/Train/pos\\person_287.png\n",
      "Class: 74.0, Confidence: 0.73, Bounding Box: 37.85, 288.97, 80.71, 350.88 for dataset/inria/Train/pos\\person_287.png\n",
      "Class: 56.0, Confidence: 0.41, Bounding Box: 589.56, 176.75, 639.70, 302.79 for dataset/inria/Train/pos\\person_287.png\n",
      "Class: 58.0, Confidence: 0.25, Bounding Box: 536.25, 154.23, 561.63, 227.66 for dataset/inria/Train/pos\\person_287.png\n",
      "\n",
      "0: 480x640 3 persons, 2 potted plants, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 197.83, 160.11, 275.10, 427.73 for dataset/inria/Train/pos\\person_288.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 262.83, 144.19, 348.33, 415.13 for dataset/inria/Train/pos\\person_288.png\n",
      "Class: 58.0, Confidence: 0.84, Bounding Box: 0.00, 384.20, 90.28, 479.80 for dataset/inria/Train/pos\\person_288.png\n",
      "Class: 58.0, Confidence: 0.41, Bounding Box: 0.04, 193.48, 88.64, 475.54 for dataset/inria/Train/pos\\person_288.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 156.56, 156.71, 177.23, 257.26 for dataset/inria/Train/pos\\person_288.png\n",
      "\n",
      "0: 480x640 3 persons, 1 backpack, 1 handbag, 19.1ms\n",
      "Speed: 2.7ms preprocess, 19.1ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 335.09, 171.57, 399.93, 421.04 for dataset/inria/Train/pos\\person_289.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 260.30, 163.05, 342.50, 443.17 for dataset/inria/Train/pos\\person_289.png\n",
      "Class: 26.0, Confidence: 0.73, Bounding Box: 276.88, 287.43, 308.52, 340.29 for dataset/inria/Train/pos\\person_289.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 109.69, 244.46, 269.68, 479.94 for dataset/inria/Train/pos\\person_289.png\n",
      "Class: 24.0, Confidence: 0.25, Bounding Box: 107.38, 243.59, 269.68, 479.51 for dataset/inria/Train/pos\\person_289.png\n",
      "\n",
      "0: 480x640 8 persons, 2 handbags, 1 potted plant, 12.0ms\n",
      "Speed: 2.1ms preprocess, 12.0ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 224.59, 142.62, 353.73, 479.13 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 335.69, 144.38, 447.51, 479.17 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 596.91, 142.49, 639.86, 371.64 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 26.0, Confidence: 0.74, Bounding Box: 412.01, 194.96, 447.28, 320.89 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 490.37, 162.64, 503.24, 194.00 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 503.43, 161.01, 514.09, 193.63 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 544.48, 156.26, 557.30, 193.22 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 0.88, 161.78, 31.81, 238.87 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 26.0, Confidence: 0.52, Bounding Box: 414.45, 237.24, 446.55, 320.71 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 456.16, 159.72, 467.71, 187.16 for dataset/inria/Train/pos\\person_291.png\n",
      "Class: 58.0, Confidence: 0.27, Bounding Box: 59.01, 95.11, 198.40, 419.71 for dataset/inria/Train/pos\\person_291.png\n",
      "\n",
      "0: 480x640 2 persons, 3 cars, 1 stop sign, 1 clock, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 274.88, 160.32, 375.63, 427.66 for dataset/inria/Train/pos\\person_292.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 247.79, 167.14, 293.52, 330.75 for dataset/inria/Train/pos\\person_292.png\n",
      "Class: 2.0, Confidence: 0.82, Bounding Box: 391.00, 193.89, 524.51, 271.82 for dataset/inria/Train/pos\\person_292.png\n",
      "Class: 74.0, Confidence: 0.77, Bounding Box: 0.00, 113.91, 40.07, 160.81 for dataset/inria/Train/pos\\person_292.png\n",
      "Class: 11.0, Confidence: 0.59, Bounding Box: 557.11, 17.08, 621.84, 86.68 for dataset/inria/Train/pos\\person_292.png\n",
      "Class: 2.0, Confidence: 0.48, Bounding Box: 534.09, 196.99, 639.69, 259.88 for dataset/inria/Train/pos\\person_292.png\n",
      "Class: 2.0, Confidence: 0.33, Bounding Box: 394.74, 197.23, 410.65, 221.61 for dataset/inria/Train/pos\\person_292.png\n",
      "\n",
      "0: 480x640 8 persons, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 279.20, 170.05, 361.95, 412.37 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 49.73, 194.90, 76.11, 260.50 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 0.00, 200.51, 21.75, 268.08 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 201.90, 188.29, 218.34, 224.26 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 244.84, 186.32, 260.09, 223.63 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 223.03, 191.07, 234.67, 223.40 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 232.42, 184.79, 248.02, 222.71 for dataset/inria/Train/pos\\person_294.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 264.14, 184.88, 277.57, 212.69 for dataset/inria/Train/pos\\person_294.png\n",
      "\n",
      "0: 480x640 2 persons, 14 chairs, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 56.0, Confidence: 0.89, Bounding Box: 359.78, 235.52, 454.29, 361.93 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.86, Bounding Box: 485.96, 263.39, 558.47, 369.81 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.83, Bounding Box: 203.18, 233.09, 260.47, 328.40 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 253.71, 136.66, 339.12, 395.00 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 251.70, 111.00, 293.23, 222.09 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.60, Bounding Box: 327.46, 223.43, 366.22, 316.27 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.46, Bounding Box: 491.85, 213.71, 553.53, 263.63 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.44, Bounding Box: 529.47, 175.63, 570.26, 249.73 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.43, Bounding Box: 473.53, 169.95, 518.56, 206.12 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.35, Bounding Box: 434.98, 223.32, 488.15, 239.49 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.32, Bounding Box: 438.33, 197.66, 481.26, 222.42 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.32, Bounding Box: 35.67, 156.92, 74.64, 227.70 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.31, Bounding Box: 473.82, 170.31, 516.72, 191.91 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 428.04, 166.78, 466.65, 211.98 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.28, Bounding Box: 379.84, 168.55, 419.30, 228.15 for dataset/inria/Train/pos\\person_295.png\n",
      "Class: 56.0, Confidence: 0.26, Bounding Box: 541.95, 176.11, 570.97, 249.87 for dataset/inria/Train/pos\\person_295.png\n",
      "\n",
      "0: 480x640 4 persons, 7 chairs, 1 dining table, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 224.16, 127.19, 319.81, 443.43 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.80, Bounding Box: 503.07, 369.41, 639.91, 479.68 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 295.73, 260.31, 381.16, 432.77 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 307.95, 135.33, 395.27, 270.88 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 60.0, Confidence: 0.51, Bounding Box: 402.79, 337.46, 569.68, 462.81 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.48, Bounding Box: 320.98, 373.61, 469.80, 479.90 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 145.79, 125.58, 180.82, 171.35 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.44, Bounding Box: 359.65, 447.05, 525.12, 479.69 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.36, Bounding Box: 326.25, 322.50, 469.51, 479.31 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.30, Bounding Box: 512.91, 253.04, 606.33, 328.22 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.29, Bounding Box: 501.70, 312.89, 639.60, 479.76 for dataset/inria/Train/pos\\person_296.png\n",
      "Class: 56.0, Confidence: 0.28, Bounding Box: 605.80, 257.56, 639.83, 355.27 for dataset/inria/Train/pos\\person_296.png\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 potted plant, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 175.94, 156.33, 274.26, 405.77 for dataset/inria/Train/pos\\person_297.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 273.48, 164.25, 339.75, 396.73 for dataset/inria/Train/pos\\person_297.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 319.36, 153.67, 393.96, 422.97 for dataset/inria/Train/pos\\person_297.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 4.70, 183.77, 43.17, 311.58 for dataset/inria/Train/pos\\person_297.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 34.54, 176.40, 57.02, 283.31 for dataset/inria/Train/pos\\person_297.png\n",
      "Class: 58.0, Confidence: 0.52, Bounding Box: 485.74, 94.31, 565.64, 246.40 for dataset/inria/Train/pos\\person_297.png\n",
      "Class: 27.0, Confidence: 0.39, Bounding Box: 240.06, 198.84, 255.46, 247.12 for dataset/inria/Train/pos\\person_297.png\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 342.66, 224.29, 410.71, 401.77 for dataset/inria/Train/pos\\person_298.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 245.91, 154.07, 331.57, 426.89 for dataset/inria/Train/pos\\person_298.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 245.80, 154.38, 332.81, 340.14 for dataset/inria/Train/pos\\person_298.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 149.64, 249.09, 212.86, 381.62 for dataset/inria/Train/pos\\person_298.png\n",
      "Class: 56.0, Confidence: 0.40, Bounding Box: 133.41, 223.87, 232.95, 394.36 for dataset/inria/Train/pos\\person_298.png\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 317.60, 216.44, 393.78, 432.76 for dataset/inria/Train/pos\\person_299.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 189.32, 117.60, 286.61, 470.19 for dataset/inria/Train/pos\\person_299.png\n",
      "Class: 56.0, Confidence: 0.64, Bounding Box: 50.89, 220.57, 171.32, 430.60 for dataset/inria/Train/pos\\person_299.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 74.59, 241.75, 147.60, 400.69 for dataset/inria/Train/pos\\person_299.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 56.19, 243.72, 147.74, 369.58 for dataset/inria/Train/pos\\person_299.png\n",
      "\n",
      "0: 480x640 3 persons, 1 handbag, 4 chairs, 1 dining table, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 252.64, 104.66, 381.26, 403.46 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 341.99, 70.00, 375.95, 184.49 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 384.35, 90.22, 431.06, 177.83 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 56.0, Confidence: 0.78, Bounding Box: 425.13, 242.93, 529.99, 397.34 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 56.0, Confidence: 0.78, Bounding Box: 354.02, 181.75, 408.39, 260.48 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 56.0, Confidence: 0.68, Bounding Box: 517.32, 247.14, 639.62, 376.72 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 56.0, Confidence: 0.57, Bounding Box: 429.00, 188.84, 491.23, 256.55 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 26.0, Confidence: 0.39, Bounding Box: 260.83, 288.09, 314.80, 387.34 for dataset/inria/Train/pos\\person_300.png\n",
      "Class: 60.0, Confidence: 0.38, Bounding Box: 381.99, 170.74, 490.12, 257.51 for dataset/inria/Train/pos\\person_300.png\n",
      "\n",
      "0: 480x640 7 persons, 1 backpack, 1 handbag, 2 sports balls, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 85.96, 173.09, 121.54, 239.70 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 253.14, 155.35, 346.49, 409.35 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 57.85, 163.00, 86.53, 248.03 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 0.01, 169.88, 37.54, 248.86 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 34.70, 164.01, 59.41, 248.96 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 590.22, 169.11, 607.26, 215.35 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 32.0, Confidence: 0.54, Bounding Box: 18.20, 226.38, 48.14, 251.30 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 32.0, Confidence: 0.52, Bounding Box: 5.17, 225.79, 48.38, 251.47 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 24.0, Confidence: 0.45, Bounding Box: 294.27, 201.63, 347.97, 283.28 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 26.0, Confidence: 0.26, Bounding Box: 294.92, 202.80, 348.04, 281.88 for dataset/inria/Train/pos\\person_301.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 394.61, 168.42, 407.60, 199.49 for dataset/inria/Train/pos\\person_301.png\n",
      "\n",
      "0: 480x640 12 persons, 1 car, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 336.51, 159.81, 441.73, 445.76 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 181.47, 157.47, 310.45, 467.31 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 541.49, 174.75, 567.74, 242.32 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 509.32, 179.24, 521.03, 209.24 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 37.84, 182.64, 52.16, 225.27 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 297.15, 176.94, 311.88, 219.45 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 157.35, 185.04, 168.72, 219.14 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 128.12, 185.01, 141.92, 221.91 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 0.04, 189.89, 11.41, 294.00 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 309.28, 180.79, 322.74, 218.86 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 498.49, 177.93, 509.98, 210.05 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 0.07, 190.17, 8.08, 292.28 for dataset/inria/Train/pos\\person_302.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 364.17, 183.42, 379.77, 192.26 for dataset/inria/Train/pos\\person_302.png\n",
      "\n",
      "0: 480x640 10 persons, 2 umbrellas, 3 handbags, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 300.54, 87.71, 418.96, 463.84 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 133.03, 69.14, 200.41, 291.53 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 55.76, 71.31, 168.90, 322.48 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 272.56, 93.67, 330.34, 239.71 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 0.08, 93.29, 70.92, 409.86 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 412.61, 51.81, 519.61, 315.82 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 20.11, 77.60, 63.23, 145.20 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 381.80, 61.06, 427.08, 191.00 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 381.24, 62.00, 427.48, 250.24 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 26.0, Confidence: 0.45, Bounding Box: 470.58, 83.78, 521.78, 188.63 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 25.0, Confidence: 0.45, Bounding Box: 0.05, 80.33, 30.70, 113.76 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 26.0, Confidence: 0.41, Bounding Box: 172.73, 111.82, 205.11, 190.81 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 26.0, Confidence: 0.38, Bounding Box: 0.73, 126.35, 83.37, 255.42 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 25.0, Confidence: 0.31, Bounding Box: 363.69, 0.04, 478.19, 66.79 for dataset/inria/Train/pos\\person_304.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 81.73, 102.60, 169.39, 322.09 for dataset/inria/Train/pos\\person_304.png\n",
      "\n",
      "0: 480x640 7 persons, 1 bicycle, 19.0ms\n",
      "Speed: 1.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 277.47, 176.57, 344.49, 359.38 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 170.08, 191.37, 262.63, 435.04 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 263.14, 174.94, 298.38, 279.96 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 233.76, 178.27, 263.90, 257.96 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 1.0, Confidence: 0.55, Bounding Box: 261.19, 284.51, 346.94, 379.72 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 396.31, 206.66, 472.38, 276.87 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 394.75, 206.90, 472.63, 347.09 for dataset/inria/Train/pos\\person_305.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 233.23, 177.48, 263.38, 316.14 for dataset/inria/Train/pos\\person_305.png\n",
      "\n",
      "0: 480x640 10 persons, 1 bicycle, 2 cars, 2 dogs, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 16.0, Confidence: 0.90, Bounding Box: 153.73, 313.26, 219.85, 406.13 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 281.12, 189.05, 353.22, 390.98 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 594.27, 170.76, 633.05, 272.89 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 560.99, 173.36, 597.68, 277.87 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 515.46, 167.39, 542.65, 250.40 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 379.55, 175.64, 400.44, 230.60 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 484.42, 174.63, 512.16, 249.66 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 2.0, Confidence: 0.70, Bounding Box: 432.27, 175.90, 490.78, 225.54 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 16.0, Confidence: 0.68, Bounding Box: 202.63, 242.81, 244.70, 329.41 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 288.31, 178.91, 304.86, 222.52 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 167.38, 165.56, 214.75, 322.77 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.52, Bounding Box: 256.97, 195.27, 276.39, 241.33 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 340.88, 181.58, 360.55, 195.21 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 1.0, Confidence: 0.26, Bounding Box: 171.94, 263.92, 201.55, 320.62 for dataset/inria/Train/pos\\person_319.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 360.26, 182.17, 369.59, 204.99 for dataset/inria/Train/pos\\person_319.png\n",
      "\n",
      "0: 480x640 7 persons, 1 bench, 1 handbag, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 263.60, 119.44, 354.49, 427.57 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 477.37, 130.37, 516.50, 228.48 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 327.35, 140.22, 458.45, 480.00 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 116.80, 147.43, 135.46, 193.06 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 13.0, Confidence: 0.59, Bounding Box: 528.07, 160.96, 590.86, 196.00 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 626.26, 126.64, 639.78, 162.41 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 98.90, 144.04, 118.82, 192.47 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 607.27, 128.00, 615.57, 155.76 for dataset/inria/Train/pos\\person_320.png\n",
      "Class: 26.0, Confidence: 0.31, Bounding Box: 319.41, 308.12, 348.57, 371.11 for dataset/inria/Train/pos\\person_320.png\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 223.95, 189.64, 341.64, 281.23 for dataset/inria/Train/pos\\person_321.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 381.31, 169.42, 425.32, 279.67 for dataset/inria/Train/pos\\person_321.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 435.10, 166.60, 475.37, 280.51 for dataset/inria/Train/pos\\person_321.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 322.81, 192.72, 383.92, 251.97 for dataset/inria/Train/pos\\person_321.png\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 truck, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 426.51, 169.31, 483.24, 314.36 for dataset/inria/Train/pos\\person_324.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 377.08, 164.76, 432.57, 306.89 for dataset/inria/Train/pos\\person_324.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 192.84, 160.98, 222.97, 180.33 for dataset/inria/Train/pos\\person_324.png\n",
      "Class: 2.0, Confidence: 0.61, Bounding Box: 57.73, 162.14, 155.50, 233.54 for dataset/inria/Train/pos\\person_324.png\n",
      "Class: 7.0, Confidence: 0.50, Bounding Box: 89.86, 138.98, 161.57, 213.40 for dataset/inria/Train/pos\\person_324.png\n",
      "\n",
      "0: 480x640 1 person, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 221.10, 111.76, 284.42, 328.50 for dataset/inria/Train/pos\\person_327.png\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 truck, 1 potted plant, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 96.97, 228.22, 137.51, 351.52 for dataset/inria/Train/pos\\person_328.png\n",
      "Class: 2.0, Confidence: 0.47, Bounding Box: 44.46, 254.74, 80.34, 267.76 for dataset/inria/Train/pos\\person_328.png\n",
      "Class: 7.0, Confidence: 0.43, Bounding Box: 332.11, 239.61, 357.67, 265.87 for dataset/inria/Train/pos\\person_328.png\n",
      "Class: 58.0, Confidence: 0.37, Bounding Box: 464.97, 237.96, 507.43, 269.95 for dataset/inria/Train/pos\\person_328.png\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 traffic light, 1 stop sign, 1 bench, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 13.0, Confidence: 0.89, Bounding Box: 30.53, 257.64, 507.29, 435.44 for dataset/inria/Train/pos\\person_329.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 434.23, 117.98, 503.28, 322.91 for dataset/inria/Train/pos\\person_329.png\n",
      "Class: 9.0, Confidence: 0.80, Bounding Box: 359.39, 21.24, 402.73, 110.50 for dataset/inria/Train/pos\\person_329.png\n",
      "Class: 2.0, Confidence: 0.79, Bounding Box: 541.88, 148.30, 640.00, 233.47 for dataset/inria/Train/pos\\person_329.png\n",
      "Class: 11.0, Confidence: 0.34, Bounding Box: 260.92, 162.18, 290.56, 194.83 for dataset/inria/Train/pos\\person_329.png\n",
      "\n",
      "0: 480x640 4 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 145.82, 65.42, 290.03, 472.07 for dataset/inria/Train/pos\\person_331.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 392.46, 142.81, 444.90, 282.51 for dataset/inria/Train/pos\\person_331.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 357.66, 156.95, 391.98, 238.70 for dataset/inria/Train/pos\\person_331.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 297.38, 146.97, 344.10, 321.89 for dataset/inria/Train/pos\\person_331.png\n",
      "Class: 26.0, Confidence: 0.70, Bounding Box: 298.00, 177.31, 328.97, 231.06 for dataset/inria/Train/pos\\person_331.png\n",
      "\n",
      "0: 480x640 9 persons, 1 car, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 220.19, 172.46, 280.21, 329.90 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 303.79, 174.45, 346.69, 288.42 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 369.77, 174.24, 408.73, 285.65 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 283.60, 183.73, 305.01, 246.09 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 345.74, 178.87, 370.38, 249.48 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 2.0, Confidence: 0.74, Bounding Box: 476.51, 190.42, 557.43, 223.81 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.71, Bounding Box: 31.68, 380.43, 146.32, 479.85 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 167.39, 179.22, 249.71, 335.85 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 265.77, 183.71, 283.40, 241.95 for dataset/inria/Train/pos\\person_333.png\n",
      "Class: 0.0, Confidence: 0.29, Bounding Box: 261.92, 182.24, 285.74, 245.03 for dataset/inria/Train/pos\\person_333.png\n",
      "\n",
      "0: 480x640 4 persons, 3 handbags, 10.1ms\n",
      "Speed: 1.0ms preprocess, 10.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 160.73, 94.12, 295.23, 466.33 for dataset/inria/Train/pos\\person_334.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 272.16, 86.67, 396.13, 480.00 for dataset/inria/Train/pos\\person_334.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 390.10, 127.17, 456.02, 402.29 for dataset/inria/Train/pos\\person_334.png\n",
      "Class: 26.0, Confidence: 0.64, Bounding Box: 405.14, 192.41, 458.36, 246.42 for dataset/inria/Train/pos\\person_334.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 263.65, 120.81, 294.56, 158.94 for dataset/inria/Train/pos\\person_334.png\n",
      "Class: 26.0, Confidence: 0.40, Bounding Box: 404.99, 208.23, 458.84, 245.94 for dataset/inria/Train/pos\\person_334.png\n",
      "Class: 26.0, Confidence: 0.30, Bounding Box: 405.84, 170.66, 457.88, 247.72 for dataset/inria/Train/pos\\person_334.png\n",
      "\n",
      "0: 480x640 6 persons, 1 car, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 50.03, 147.04, 159.22, 424.62 for dataset/inria/Train/pos\\person_339.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 117.42, 146.29, 197.29, 386.07 for dataset/inria/Train/pos\\person_339.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 545.37, 183.08, 596.56, 207.47 for dataset/inria/Train/pos\\person_339.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 210.54, 144.63, 265.71, 294.37 for dataset/inria/Train/pos\\person_339.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 182.93, 152.03, 238.09, 306.13 for dataset/inria/Train/pos\\person_339.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 276.51, 162.28, 304.27, 229.03 for dataset/inria/Train/pos\\person_339.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 350.49, 166.23, 362.99, 196.67 for dataset/inria/Train/pos\\person_339.png\n",
      "\n",
      "0: 480x640 9 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 167.43, 116.74, 281.42, 421.69 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 282.15, 119.36, 413.97, 419.44 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 499.65, 145.58, 566.86, 211.95 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 561.79, 5.20, 605.46, 73.03 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 484.92, 66.98, 543.61, 188.91 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 485.55, 67.70, 543.80, 152.71 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.41, Bounding Box: 516.13, 68.98, 561.29, 155.12 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 523.14, 68.39, 581.88, 145.02 for dataset/inria/Train/pos\\person_340.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 541.32, 67.50, 581.22, 135.16 for dataset/inria/Train/pos\\person_340.png\n",
      "\n",
      "0: 480x640 6 persons, 1 skateboard, 10.4ms\n",
      "Speed: 1.0ms preprocess, 10.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 323.13, 144.10, 464.82, 480.00 for dataset/inria/Train/pos\\person_341.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 234.38, 64.68, 364.83, 440.10 for dataset/inria/Train/pos\\person_341.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 169.99, 98.64, 264.92, 373.43 for dataset/inria/Train/pos\\person_341.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 0.02, 67.28, 133.44, 478.28 for dataset/inria/Train/pos\\person_341.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 88.50, 89.59, 161.64, 402.55 for dataset/inria/Train/pos\\person_341.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 0.00, 51.90, 48.87, 168.82 for dataset/inria/Train/pos\\person_341.png\n",
      "Class: 36.0, Confidence: 0.47, Bounding Box: 408.63, 328.66, 456.71, 479.57 for dataset/inria/Train/pos\\person_341.png\n",
      "\n",
      "0: 480x640 3 persons, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 206.82, 191.31, 309.29, 413.50 for dataset/inria/Train/pos\\person_342.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 73.62, 110.87, 198.80, 231.86 for dataset/inria/Train/pos\\person_342.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 281.72, 167.37, 339.76, 407.16 for dataset/inria/Train/pos\\person_342.png\n",
      "\n",
      "0: 480x640 2 persons, 25.5ms\n",
      "Speed: 1.0ms preprocess, 25.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 191.91, 188.58, 291.91, 361.81 for dataset/inria/Train/pos\\person_343.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 158.39, 242.48, 193.52, 354.94 for dataset/inria/Train/pos\\person_343.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bus, 1 handbag, 19.0ms\n",
      "Speed: 1.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 376.51, 191.72, 506.23, 479.67 for dataset/inria/Train/pos\\person_344.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 439.35, 115.61, 566.09, 451.21 for dataset/inria/Train/pos\\person_344.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 71.55, 71.10, 158.13, 366.67 for dataset/inria/Train/pos\\person_344.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 96.69, 131.08, 152.92, 253.92 for dataset/inria/Train/pos\\person_344.png\n",
      "Class: 5.0, Confidence: 0.30, Bounding Box: 513.10, 0.67, 639.51, 249.81 for dataset/inria/Train/pos\\person_344.png\n",
      "\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 209.39, 163.69, 299.73, 412.88 for dataset/inria/Train/pos\\person_346.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 305.34, 172.07, 390.64, 414.40 for dataset/inria/Train/pos\\person_346.png\n",
      "\n",
      "0: 480x640 8 persons, 1 car, 1 handbag, 11.5ms\n",
      "Speed: 2.0ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 317.75, 166.89, 373.71, 328.91 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 69.64, 155.48, 115.00, 314.28 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 230.18, 155.54, 288.92, 315.09 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 136.14, 170.39, 173.21, 282.60 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 166.58, 169.25, 195.95, 276.34 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 194.10, 206.06, 215.21, 267.55 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 300.64, 198.06, 314.32, 242.55 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 431.23, 204.88, 439.64, 226.32 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 359.72, 186.71, 392.08, 208.16 for dataset/inria/Train/pos\\person_348.png\n",
      "Class: 26.0, Confidence: 0.37, Bounding Box: 68.26, 208.03, 84.87, 245.21 for dataset/inria/Train/pos\\person_348.png\n",
      "\n",
      "0: 480x640 6 persons, 1 tie, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 33.81, 131.94, 120.47, 388.06 for dataset/inria/Train/pos\\person_349.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 445.82, 91.46, 583.20, 478.36 for dataset/inria/Train/pos\\person_349.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 333.45, 111.47, 429.52, 467.59 for dataset/inria/Train/pos\\person_349.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 124.79, 140.80, 186.04, 372.16 for dataset/inria/Train/pos\\person_349.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 298.73, 119.40, 353.96, 410.07 for dataset/inria/Train/pos\\person_349.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 156.93, 137.81, 246.33, 372.08 for dataset/inria/Train/pos\\person_349.png\n",
      "Class: 27.0, Confidence: 0.30, Bounding Box: 324.02, 173.21, 346.39, 303.31 for dataset/inria/Train/pos\\person_349.png\n",
      "\n",
      "0: 640x480 2 persons, 1 handbag, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 160.60, 113.90, 325.61, 589.77 for dataset/inria/Train/pos\\person_356.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 107.52, 142.58, 192.23, 556.02 for dataset/inria/Train/pos\\person_356.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 108.51, 271.18, 184.87, 367.65 for dataset/inria/Train/pos\\person_356.png\n",
      "\n",
      "0: 640x480 2 persons, 1 motorcycle, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 346.52, 183.85, 412.80, 375.71 for dataset/inria/Train/pos\\person_357.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 398.26, 204.74, 457.48, 392.03 for dataset/inria/Train/pos\\person_357.png\n",
      "Class: 3.0, Confidence: 0.68, Bounding Box: 0.04, 52.09, 78.60, 147.28 for dataset/inria/Train/pos\\person_357.png\n",
      "\n",
      "0: 640x480 1 person, 8 cars, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 253.53, 223.92, 307.01, 356.64 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.80, Bounding Box: 355.44, 289.09, 479.44, 403.56 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.67, Bounding Box: 387.21, 236.92, 464.54, 291.08 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 400.46, 231.98, 464.51, 259.94 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.62, Bounding Box: 290.23, 236.30, 331.07, 286.94 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.62, Bounding Box: 455.56, 223.97, 479.67, 249.00 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 208.79, 237.15, 240.99, 280.60 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 26.0, Confidence: 0.36, Bounding Box: 293.73, 299.40, 308.81, 342.18 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 148.94, 246.67, 186.18, 273.84 for dataset/inria/Train/pos\\person_358.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 420.01, 223.52, 456.61, 241.28 for dataset/inria/Train/pos\\person_358.png\n",
      "\n",
      "0: 480x640 6 persons, 1 car, 29.3ms\n",
      "Speed: 15.0ms preprocess, 29.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 205.41, 190.99, 258.45, 331.36 for dataset/inria/Train/pos\\person_376.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 371.40, 183.99, 419.11, 300.45 for dataset/inria/Train/pos\\person_376.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 278.54, 195.90, 326.37, 330.48 for dataset/inria/Train/pos\\person_376.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 140.94, 201.60, 162.78, 282.28 for dataset/inria/Train/pos\\person_376.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 329.52, 183.68, 346.19, 235.15 for dataset/inria/Train/pos\\person_376.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 273.67, 173.04, 284.62, 204.59 for dataset/inria/Train/pos\\person_376.png\n",
      "Class: 2.0, Confidence: 0.43, Bounding Box: 236.54, 168.49, 270.40, 196.55 for dataset/inria/Train/pos\\person_376.png\n",
      "\n",
      "0: 480x640 8 persons, 1 potted plant, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 329.31, 190.66, 388.22, 357.59 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 445.52, 194.44, 485.59, 304.95 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 405.01, 202.13, 445.09, 308.58 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 63.33, 206.27, 108.85, 328.69 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 378.45, 205.00, 408.70, 306.82 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 308.42, 198.06, 343.04, 312.17 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 31.34, 210.62, 69.53, 330.34 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 58.0, Confidence: 0.66, Bounding Box: 476.67, 169.19, 528.88, 269.66 for dataset/inria/Train/pos\\person_377.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 21.62, 263.48, 50.65, 332.24 for dataset/inria/Train/pos\\person_377.png\n",
      "\n",
      "0: 480x640 13 persons, 1 backpack, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 351.58, 233.43, 390.72, 337.30 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 11.28, 220.04, 47.56, 352.85 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 75.47, 229.75, 121.93, 354.67 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 119.34, 222.75, 153.13, 356.86 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 520.72, 222.47, 552.72, 321.13 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 182.80, 230.02, 224.84, 328.35 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 491.33, 226.95, 525.97, 322.13 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 456.48, 225.78, 491.49, 321.96 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 289.95, 224.44, 325.43, 342.17 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 572.86, 217.41, 593.12, 297.77 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 435.26, 226.12, 455.16, 285.79 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 24.0, Confidence: 0.36, Bounding Box: 456.91, 241.78, 478.20, 276.23 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 103.93, 225.20, 120.21, 257.65 for dataset/inria/Train/pos\\person_378.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 462.39, 212.49, 492.14, 250.54 for dataset/inria/Train/pos\\person_378.png\n",
      "\n",
      "0: 480x640 3 persons, 3 cars, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 223.60, 156.64, 304.44, 428.51 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 291.57, 174.71, 347.10, 404.96 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 26.0, Confidence: 0.64, Bounding Box: 324.07, 298.14, 348.44, 348.35 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 0.0, Confidence: 0.51, Bounding Box: 326.43, 190.90, 369.88, 362.21 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 26.0, Confidence: 0.45, Bounding Box: 366.92, 239.12, 381.67, 274.95 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 2.0, Confidence: 0.43, Bounding Box: 625.27, 197.83, 639.75, 224.97 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 504.30, 176.23, 531.53, 194.72 for dataset/inria/Train/pos\\person_387.png\n",
      "Class: 2.0, Confidence: 0.33, Bounding Box: 523.76, 178.95, 559.27, 201.28 for dataset/inria/Train/pos\\person_387.png\n",
      "\n",
      "0: 480x640 4 persons, 5 cars, 1 giraffe, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 204.89, 176.42, 274.09, 322.87 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 146.56, 162.90, 208.83, 320.54 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 352.96, 206.53, 384.13, 286.82 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 325.67, 204.95, 356.08, 286.08 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 2.0, Confidence: 0.60, Bounding Box: 85.85, 219.11, 273.23, 300.15 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 2.0, Confidence: 0.56, Bounding Box: 481.06, 201.88, 546.51, 233.08 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 2.0, Confidence: 0.54, Bounding Box: 401.21, 210.47, 472.30, 237.98 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 2.0, Confidence: 0.47, Bounding Box: 535.64, 203.33, 580.34, 231.46 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 2.0, Confidence: 0.41, Bounding Box: 462.21, 210.03, 504.08, 234.60 for dataset/inria/Train/pos\\person_395.png\n",
      "Class: 23.0, Confidence: 0.28, Bounding Box: 94.88, 0.00, 196.27, 96.48 for dataset/inria/Train/pos\\person_395.png\n",
      "\n",
      "0: 480x640 4 persons, 1 dog, 1 skateboard, 15.5ms\n",
      "Speed: 1.0ms preprocess, 15.5ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 267.91, 123.74, 340.57, 305.36 for dataset/inria/Train/pos\\person_414.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 476.57, 112.16, 521.98, 224.94 for dataset/inria/Train/pos\\person_414.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 106.00, 126.21, 174.38, 306.18 for dataset/inria/Train/pos\\person_414.png\n",
      "Class: 16.0, Confidence: 0.73, Bounding Box: 336.59, 242.97, 366.94, 278.40 for dataset/inria/Train/pos\\person_414.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 166.18, 156.01, 207.06, 249.33 for dataset/inria/Train/pos\\person_414.png\n",
      "Class: 36.0, Confidence: 0.26, Bounding Box: 488.50, 213.13, 513.66, 227.32 for dataset/inria/Train/pos\\person_414.png\n",
      "\n",
      "0: 480x640 9 persons, 2 benchs, 1 handbag, 1 chair, 13.4ms\n",
      "Speed: 1.0ms preprocess, 13.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 173.98, 81.82, 219.94, 200.78 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 125.45, 90.15, 163.55, 197.32 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 67.85, 73.86, 121.69, 241.06 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 323.14, 100.97, 388.47, 238.01 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 13.0, Confidence: 0.72, Bounding Box: 565.96, 220.43, 639.72, 311.78 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 110.74, 75.94, 140.08, 192.81 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 407.87, 128.17, 460.43, 192.92 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 356.96, 122.57, 413.93, 236.58 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 323.90, 108.60, 412.60, 235.61 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 13.0, Confidence: 0.28, Bounding Box: 374.17, 166.51, 472.59, 259.96 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 368.29, 122.72, 414.18, 220.12 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 26.0, Confidence: 0.25, Bounding Box: 173.76, 106.32, 202.60, 176.95 for dataset/inria/Train/pos\\person_425.png\n",
      "Class: 56.0, Confidence: 0.25, Bounding Box: 373.93, 164.72, 469.94, 259.58 for dataset/inria/Train/pos\\person_425.png\n",
      "\n",
      "0: 640x480 3 persons, 1 bicycle, 27.8ms\n",
      "Speed: 1.5ms preprocess, 27.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 158.45, 90.64, 291.77, 415.75 for dataset/inria/Train/pos\\person_and_bike_002.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 26.42, 116.33, 68.46, 243.17 for dataset/inria/Train/pos\\person_and_bike_002.png\n",
      "Class: 1.0, Confidence: 0.87, Bounding Box: 179.51, 252.47, 279.87, 469.33 for dataset/inria/Train/pos\\person_and_bike_002.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 0.00, 134.38, 24.95, 241.67 for dataset/inria/Train/pos\\person_and_bike_002.png\n",
      "\n",
      "0: 640x480 2 persons, 9 cars, 2 motorcycles, 1 backpack, 12.7ms\n",
      "Speed: 1.0ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 181.15, 153.40, 295.87, 479.32 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.81, Bounding Box: 31.22, 202.87, 151.24, 328.23 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 24.0, Confidence: 0.80, Bounding Box: 197.23, 194.77, 276.93, 311.77 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.69, Bounding Box: 0.01, 222.12, 62.59, 630.66 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 164.90, 161.29, 222.14, 298.55 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 0.05, 190.59, 33.28, 227.93 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 91.44, 196.06, 169.73, 282.88 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.50, Bounding Box: 31.49, 182.91, 83.85, 218.66 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 99.32, 192.73, 168.09, 234.56 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.37, Bounding Box: 160.04, 196.37, 185.70, 228.24 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 3.0, Confidence: 0.37, Bounding Box: 82.14, 248.74, 132.68, 372.98 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 3.0, Confidence: 0.31, Bounding Box: 83.38, 299.76, 132.70, 373.35 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.28, Bounding Box: 83.71, 188.06, 113.00, 200.30 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 79.13, 204.08, 149.42, 327.16 for dataset/inria/Train/pos\\person_and_bike_003.png\n",
      "\n",
      "0: 640x480 3 persons, 7 bicycles, 4 cars, 1 bus, 74.5ms\n",
      "Speed: 1.0ms preprocess, 74.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 2.0, Confidence: 0.91, Bounding Box: 283.86, 223.99, 442.53, 366.85 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 171.30, 195.42, 232.05, 351.42 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 271.91, 159.69, 290.45, 214.39 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 404.58, 176.76, 479.81, 249.40 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.73, Bounding Box: 0.05, 322.03, 37.42, 480.36 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 2.0, Confidence: 0.66, Bounding Box: 304.35, 179.72, 423.34, 254.58 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 301.72, 163.83, 312.03, 191.65 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.56, Bounding Box: 177.95, 274.55, 222.37, 365.30 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.48, Bounding Box: 176.26, 509.59, 479.54, 639.73 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.43, Bounding Box: 212.19, 382.45, 479.11, 535.03 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.29, Bounding Box: 204.14, 404.95, 352.46, 527.41 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.29, Bounding Box: 186.83, 278.80, 215.81, 366.52 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 319.13, 168.18, 377.68, 192.18 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 1.0, Confidence: 0.26, Bounding Box: 240.00, 481.62, 479.51, 568.91 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "Class: 5.0, Confidence: 0.25, Bounding Box: 336.91, 142.55, 367.44, 171.42 for dataset/inria/Train/pos\\person_and_bike_005.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 19.0ms\n",
      "Speed: 8.0ms preprocess, 19.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.92, Bounding Box: 88.74, 173.32, 279.54, 419.48 for dataset/inria/Train/pos\\person_and_bike_007.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 243.96, 46.26, 360.93, 406.81 for dataset/inria/Train/pos\\person_and_bike_007.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bicycle, 3 cars, 1 truck, 1 backpack, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.92, Bounding Box: 475.08, 199.67, 639.68, 437.92 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 371.93, 185.00, 537.29, 305.53 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 292.64, 158.52, 375.76, 390.60 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 1.0, Confidence: 0.85, Bounding Box: 0.07, 252.57, 151.48, 383.90 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 217.08, 157.78, 293.22, 376.19 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 24.0, Confidence: 0.84, Bounding Box: 311.34, 202.57, 363.53, 283.61 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 204.86, 130.13, 261.57, 256.18 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 2.0, Confidence: 0.35, Bounding Box: 277.20, 155.35, 317.42, 201.01 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "Class: 7.0, Confidence: 0.26, Bounding Box: 284.23, 133.59, 424.74, 226.13 for dataset/inria/Train/pos\\person_and_bike_008.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bicycle, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 585.71, 145.35, 639.41, 288.05 for dataset/inria/Train/pos\\person_and_bike_009.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 310.10, 175.38, 396.17, 354.93 for dataset/inria/Train/pos\\person_and_bike_009.png\n",
      "Class: 1.0, Confidence: 0.75, Bounding Box: 309.15, 272.90, 375.89, 388.77 for dataset/inria/Train/pos\\person_and_bike_009.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 620.97, 138.33, 639.85, 245.89 for dataset/inria/Train/pos\\person_and_bike_009.png\n",
      "\n",
      "0: 480x640 2 persons, 1 bicycle, 5 cars, 1 traffic light, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 263.46, 105.17, 387.69, 349.25 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 557.28, 96.12, 597.16, 194.89 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 2.0, Confidence: 0.84, Bounding Box: 0.00, 144.46, 172.31, 372.93 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 2.0, Confidence: 0.75, Bounding Box: 359.17, 132.82, 477.75, 259.85 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 1.0, Confidence: 0.73, Bounding Box: 278.57, 197.18, 364.16, 357.84 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 9.0, Confidence: 0.67, Bounding Box: 436.08, 23.52, 452.13, 70.32 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 2.0, Confidence: 0.43, Bounding Box: 21.06, 130.97, 219.41, 311.03 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 264.99, 130.29, 478.38, 260.53 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "Class: 2.0, Confidence: 0.26, Bounding Box: 45.77, 93.90, 230.16, 215.50 for dataset/inria/Train/pos\\person_and_bike_010.png\n",
      "\n",
      "0: 480x640 8 persons, 1 bicycle, 1 car, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 113.85, 211.59, 218.05, 294.61 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 399.27, 189.13, 432.35, 282.36 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 285.72, 188.64, 323.39, 297.43 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 239.09, 196.84, 275.15, 284.46 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 331.12, 187.16, 385.26, 297.17 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 1.0, Confidence: 0.63, Bounding Box: 505.96, 243.54, 623.11, 328.68 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 270.50, 198.52, 295.32, 289.20 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.50, Bounding Box: 218.95, 199.94, 236.37, 253.82 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 383.66, 193.20, 402.37, 252.36 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "Class: 0.0, Confidence: 0.28, Bounding Box: 331.11, 187.83, 367.66, 298.05 for dataset/inria/Train/pos\\person_and_bike_011.png\n",
      "\n",
      "0: 480x640 8 persons, 2 bicycles, 26.5ms\n",
      "Speed: 2.0ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 102.05, 191.78, 218.30, 479.63 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 288.10, 132.29, 347.61, 309.46 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 0.17, 130.96, 58.63, 379.77 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 524.10, 147.98, 630.75, 475.67 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 1.0, Confidence: 0.77, Bounding Box: 218.96, 193.48, 287.49, 302.08 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 47.96, 143.81, 99.13, 374.92 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 96.16, 155.36, 144.72, 291.40 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 572.18, 116.19, 639.16, 451.50 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 1.0, Confidence: 0.60, Bounding Box: 322.84, 205.42, 367.76, 309.17 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 533.15, 113.13, 575.52, 161.81 for dataset/inria/Train/pos\\person_and_bike_013.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bicycle, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.90, Bounding Box: 285.23, 226.86, 450.10, 391.97 for dataset/inria/Train/pos\\person_and_bike_015.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 317.77, 116.09, 430.00, 339.56 for dataset/inria/Train/pos\\person_and_bike_015.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 586.21, 141.15, 614.55, 251.82 for dataset/inria/Train/pos\\person_and_bike_015.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 609.26, 137.12, 633.38, 238.83 for dataset/inria/Train/pos\\person_and_bike_015.png\n",
      "\n",
      "0: 480x640 9 persons, 6 bicycles, 9.5ms\n",
      "Speed: 1.1ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 20.10, 165.01, 62.78, 288.41 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 282.28, 148.05, 362.56, 335.71 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 533.56, 143.76, 577.08, 269.42 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 87.56, 167.16, 144.25, 307.64 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 1.0, Confidence: 0.80, Bounding Box: 302.12, 250.87, 365.18, 373.62 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 198.81, 175.88, 225.98, 252.12 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 60.38, 163.53, 99.98, 286.36 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 479.35, 149.51, 522.33, 245.35 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 1.0, Confidence: 0.72, Bounding Box: 559.47, 214.62, 639.65, 337.85 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 157.84, 167.47, 184.97, 253.07 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 179.36, 164.38, 206.09, 251.85 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 1.0, Confidence: 0.54, Bounding Box: 485.25, 212.81, 568.85, 364.58 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 1.0, Confidence: 0.54, Bounding Box: 232.14, 189.59, 252.30, 221.71 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 1.0, Confidence: 0.46, Bounding Box: 438.00, 222.19, 506.44, 370.51 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "Class: 1.0, Confidence: 0.35, Bounding Box: 358.06, 208.64, 378.47, 283.49 for dataset/inria/Train/pos\\person_and_bike_016.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 2 potted plants, 14.6ms\n",
      "Speed: 1.0ms preprocess, 14.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.86, Bounding Box: 261.01, 239.76, 334.62, 303.88 for dataset/inria/Train/pos\\person_and_bike_017.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 271.73, 188.50, 321.35, 288.13 for dataset/inria/Train/pos\\person_and_bike_017.png\n",
      "Class: 58.0, Confidence: 0.43, Bounding Box: 186.79, 172.53, 231.58, 241.63 for dataset/inria/Train/pos\\person_and_bike_017.png\n",
      "Class: 58.0, Confidence: 0.35, Bounding Box: 0.03, 154.89, 88.14, 388.61 for dataset/inria/Train/pos\\person_and_bike_017.png\n",
      "\n",
      "0: 480x640 1 person, 3 bicycles, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 245.00, 39.99, 410.64, 416.88 for dataset/inria/Train/pos\\person_and_bike_019.png\n",
      "Class: 1.0, Confidence: 0.84, Bounding Box: 0.30, 223.90, 287.46, 475.03 for dataset/inria/Train/pos\\person_and_bike_019.png\n",
      "Class: 1.0, Confidence: 0.61, Bounding Box: 165.17, 191.05, 504.26, 412.64 for dataset/inria/Train/pos\\person_and_bike_019.png\n",
      "Class: 1.0, Confidence: 0.49, Bounding Box: 38.46, 184.29, 504.11, 464.28 for dataset/inria/Train/pos\\person_and_bike_019.png\n",
      "\n",
      "0: 480x640 5 persons, 1 bicycle, 42.2ms\n",
      "Speed: 2.0ms preprocess, 42.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 118.08, 173.48, 151.63, 236.12 for dataset/inria/Train/pos\\person_and_bike_020.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 305.05, 174.27, 353.07, 318.83 for dataset/inria/Train/pos\\person_and_bike_020.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 360.86, 196.28, 449.08, 396.02 for dataset/inria/Train/pos\\person_and_bike_020.png\n",
      "Class: 1.0, Confidence: 0.78, Bounding Box: 376.90, 292.57, 442.52, 422.93 for dataset/inria/Train/pos\\person_and_bike_020.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 345.92, 182.22, 385.11, 311.83 for dataset/inria/Train/pos\\person_and_bike_020.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 30.17, 164.37, 53.81, 229.55 for dataset/inria/Train/pos\\person_and_bike_020.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 1 backpack, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 276.89, 164.10, 371.24, 358.68 for dataset/inria/Train/pos\\person_and_bike_021.png\n",
      "Class: 24.0, Confidence: 0.78, Bounding Box: 289.27, 194.91, 358.58, 288.64 for dataset/inria/Train/pos\\person_and_bike_021.png\n",
      "Class: 1.0, Confidence: 0.53, Bounding Box: 313.67, 287.74, 350.25, 394.74 for dataset/inria/Train/pos\\person_and_bike_021.png\n",
      "\n",
      "0: 480x640 2 persons, 4 bicycles, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 297.25, 185.44, 379.51, 370.74 for dataset/inria/Train/pos\\person_and_bike_022.png\n",
      "Class: 1.0, Confidence: 0.58, Bounding Box: 323.21, 302.99, 350.62, 405.64 for dataset/inria/Train/pos\\person_and_bike_022.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 302.06, 177.67, 320.55, 221.07 for dataset/inria/Train/pos\\person_and_bike_022.png\n",
      "Class: 1.0, Confidence: 0.42, Bounding Box: 315.66, 293.80, 366.92, 407.24 for dataset/inria/Train/pos\\person_and_bike_022.png\n",
      "Class: 1.0, Confidence: 0.34, Bounding Box: 314.31, 301.88, 355.39, 405.34 for dataset/inria/Train/pos\\person_and_bike_022.png\n",
      "Class: 1.0, Confidence: 0.30, Bounding Box: 300.58, 248.94, 373.15, 408.39 for dataset/inria/Train/pos\\person_and_bike_022.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 10 cars, 1 stop sign, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.92, Bounding Box: 490.18, 168.83, 639.67, 314.33 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.87, Bounding Box: 86.51, 183.42, 151.27, 235.97 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 293.24, 136.79, 400.78, 351.78 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.85, Bounding Box: 33.66, 183.14, 78.23, 217.41 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.85, Bounding Box: 157.36, 178.10, 313.68, 284.95 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.82, Bounding Box: 147.74, 181.34, 203.31, 244.69 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.80, Bounding Box: 386.69, 177.92, 499.94, 282.30 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.68, Bounding Box: 8.64, 184.01, 30.46, 213.80 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.63, Bounding Box: 21.12, 183.08, 43.35, 215.71 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 1.0, Confidence: 0.61, Bounding Box: 321.43, 261.50, 397.77, 412.51 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.61, Bounding Box: 8.84, 184.13, 43.12, 215.03 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 11.0, Confidence: 0.46, Bounding Box: 429.49, 135.10, 458.83, 167.53 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "Class: 2.0, Confidence: 0.27, Bounding Box: 67.50, 182.65, 104.05, 232.44 for dataset/inria/Train/pos\\person_and_bike_023.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bicycle, 5 cars, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.85, Bounding Box: 281.95, 275.64, 436.86, 458.45 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 570.23, 177.83, 624.93, 304.08 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 2.0, Confidence: 0.68, Bounding Box: 493.35, 188.18, 640.00, 258.44 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 309.26, 166.69, 423.03, 309.27 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 308.12, 166.74, 425.40, 405.03 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 612.46, 191.32, 639.71, 253.64 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 2.0, Confidence: 0.34, Bounding Box: 493.42, 210.33, 588.83, 254.35 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 2.0, Confidence: 0.33, Bounding Box: 599.86, 188.84, 639.80, 257.63 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 494.73, 205.93, 626.81, 255.10 for dataset/inria/Train/pos\\person_and_bike_026.png\n",
      "\n",
      "0: 480x640 4 persons, 2 bicycles, 2 potted plants, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 245.88, 154.79, 311.43, 292.72 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 1.0, Confidence: 0.84, Bounding Box: 475.49, 187.48, 571.83, 296.96 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 330.28, 128.62, 436.49, 382.52 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 1.0, Confidence: 0.63, Bounding Box: 346.42, 267.12, 424.94, 446.97 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 58.0, Confidence: 0.48, Bounding Box: 130.01, 222.43, 218.83, 299.70 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 183.90, 139.62, 213.01, 227.75 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 581.05, 139.57, 601.50, 183.65 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "Class: 58.0, Confidence: 0.28, Bounding Box: 0.00, 230.52, 80.12, 322.75 for dataset/inria/Train/pos\\person_and_bike_030.png\n",
      "\n",
      "0: 480x640 2 persons, 1 bicycle, 1 stop sign, 1 handbag, 1 potted plant, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.92, Bounding Box: 248.96, 239.74, 432.92, 352.09 for dataset/inria/Train/pos\\person_and_bike_031.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 298.53, 167.96, 389.11, 322.18 for dataset/inria/Train/pos\\person_and_bike_031.png\n",
      "Class: 58.0, Confidence: 0.80, Bounding Box: 28.98, 161.80, 111.13, 277.54 for dataset/inria/Train/pos\\person_and_bike_031.png\n",
      "Class: 26.0, Confidence: 0.44, Bounding Box: 381.93, 230.40, 420.92, 274.03 for dataset/inria/Train/pos\\person_and_bike_031.png\n",
      "Class: 11.0, Confidence: 0.33, Bounding Box: 66.28, 84.15, 95.00, 114.94 for dataset/inria/Train/pos\\person_and_bike_031.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 579.19, 178.71, 593.81, 259.94 for dataset/inria/Train/pos\\person_and_bike_031.png\n",
      "\n",
      "0: 480x640 1 person, 4 bicycles, 1 motorcycle, 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.91, Bounding Box: 451.84, 177.36, 596.38, 253.94 for dataset/inria/Train/pos\\person_and_bike_033.png\n",
      "Class: 1.0, Confidence: 0.85, Bounding Box: 246.39, 165.92, 438.00, 291.79 for dataset/inria/Train/pos\\person_and_bike_033.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 478.53, 112.85, 555.96, 261.35 for dataset/inria/Train/pos\\person_and_bike_033.png\n",
      "Class: 1.0, Confidence: 0.80, Bounding Box: 61.15, 177.76, 318.55, 348.04 for dataset/inria/Train/pos\\person_and_bike_033.png\n",
      "Class: 3.0, Confidence: 0.62, Bounding Box: 0.15, 209.19, 232.95, 479.63 for dataset/inria/Train/pos\\person_and_bike_033.png\n",
      "Class: 1.0, Confidence: 0.56, Bounding Box: 451.92, 181.66, 562.21, 250.60 for dataset/inria/Train/pos\\person_and_bike_033.png\n",
      "\n",
      "0: 480x640 1 person, 2 bicycles, 1 car, 95.5ms\n",
      "Speed: 2.0ms preprocess, 95.5ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 292.93, 176.40, 374.90, 320.84 for dataset/inria/Train/pos\\person_and_bike_034.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 479.34, 169.23, 512.94, 198.11 for dataset/inria/Train/pos\\person_and_bike_034.png\n",
      "Class: 1.0, Confidence: 0.82, Bounding Box: 337.80, 232.67, 412.25, 320.66 for dataset/inria/Train/pos\\person_and_bike_034.png\n",
      "Class: 1.0, Confidence: 0.70, Bounding Box: 0.18, 233.35, 133.20, 360.96 for dataset/inria/Train/pos\\person_and_bike_034.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 2 trains, 1 traffic light, 1 backpack, 13.5ms\n",
      "Speed: 1.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 355.61, 182.41, 400.87, 288.52 for dataset/inria/Train/pos\\person_and_bike_036.png\n",
      "Class: 1.0, Confidence: 0.70, Bounding Box: 350.38, 240.42, 395.29, 303.60 for dataset/inria/Train/pos\\person_and_bike_036.png\n",
      "Class: 6.0, Confidence: 0.58, Bounding Box: 0.31, 161.19, 172.78, 207.90 for dataset/inria/Train/pos\\person_and_bike_036.png\n",
      "Class: 9.0, Confidence: 0.50, Bounding Box: 457.90, 74.19, 558.02, 172.92 for dataset/inria/Train/pos\\person_and_bike_036.png\n",
      "Class: 24.0, Confidence: 0.25, Bounding Box: 354.83, 199.00, 393.75, 243.60 for dataset/inria/Train/pos\\person_and_bike_036.png\n",
      "Class: 6.0, Confidence: 0.25, Bounding Box: 0.12, 128.34, 172.60, 208.64 for dataset/inria/Train/pos\\person_and_bike_036.png\n",
      "\n",
      "0: 480x640 2 persons, 4 bicycles, 1 car, 1 truck, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 175.68, 103.89, 247.26, 323.90 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 240.88, 99.95, 338.98, 324.34 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 1.0, Confidence: 0.61, Bounding Box: 298.47, 247.03, 555.49, 479.82 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 1.0, Confidence: 0.60, Bounding Box: 8.13, 216.86, 429.46, 479.68 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 7.0, Confidence: 0.58, Bounding Box: 0.44, 0.71, 213.20, 294.67 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 1.0, Confidence: 0.54, Bounding Box: 467.39, 255.60, 639.08, 479.61 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 1.0, Confidence: 0.38, Bounding Box: 374.25, 246.10, 639.70, 479.43 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "Class: 2.0, Confidence: 0.28, Bounding Box: 307.60, 147.36, 422.91, 254.05 for dataset/inria/Train/pos\\person_and_bike_039.png\n",
      "\n",
      "0: 480x640 6 persons, 4 bicycles, 1 bench, 31.5ms\n",
      "Speed: 1.0ms preprocess, 31.5ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 1.0, Confidence: 0.91, Bounding Box: 0.43, 195.35, 86.43, 270.30 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 1.0, Confidence: 0.88, Bounding Box: 173.15, 190.19, 266.32, 261.75 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 539.52, 152.94, 582.85, 283.97 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 401.11, 142.41, 454.15, 271.42 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 1.0, Confidence: 0.84, Bounding Box: 307.41, 189.12, 369.47, 252.44 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 451.86, 200.91, 487.36, 275.16 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 479.56, 168.79, 517.23, 280.71 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 1.0, Confidence: 0.48, Bounding Box: 505.71, 212.07, 527.04, 277.49 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 366.80, 159.42, 385.81, 222.32 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 13.0, Confidence: 0.37, Bounding Box: 578.72, 213.24, 613.78, 243.43 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "Class: 0.0, Confidence: 0.27, Bounding Box: 367.06, 160.85, 392.95, 223.92 for dataset/inria/Train/pos\\person_and_bike_040.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 3 cars, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.91, Bounding Box: 0.15, 183.78, 209.73, 414.20 for dataset/inria/Train/pos\\person_and_bike_074.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 311.80, 129.35, 360.47, 289.18 for dataset/inria/Train/pos\\person_and_bike_074.png\n",
      "Class: 1.0, Confidence: 0.85, Bounding Box: 289.17, 199.21, 383.44, 287.43 for dataset/inria/Train/pos\\person_and_bike_074.png\n",
      "Class: 2.0, Confidence: 0.70, Bounding Box: 121.26, 153.06, 227.75, 219.44 for dataset/inria/Train/pos\\person_and_bike_074.png\n",
      "Class: 2.0, Confidence: 0.70, Bounding Box: 116.24, 175.10, 211.09, 280.06 for dataset/inria/Train/pos\\person_and_bike_074.png\n",
      "\n",
      "0: 480x640 1 person, 4 bicycles, 2 cars, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 203.02, 76.96, 321.42, 421.35 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "Class: 2.0, Confidence: 0.90, Bounding Box: 300.75, 55.48, 640.00, 214.82 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "Class: 1.0, Confidence: 0.87, Bounding Box: 328.33, 194.50, 473.77, 412.77 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "Class: 2.0, Confidence: 0.84, Bounding Box: 0.79, 59.33, 136.67, 191.09 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "Class: 1.0, Confidence: 0.53, Bounding Box: 0.00, 311.61, 39.14, 479.16 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "Class: 1.0, Confidence: 0.52, Bounding Box: 0.93, 192.31, 113.52, 387.47 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "Class: 1.0, Confidence: 0.50, Bounding Box: 48.79, 202.76, 179.49, 381.77 for dataset/inria/Train/pos\\person_and_bike_075.png\n",
      "\n",
      "0: 480x640 5 persons, 6 bicycles, 7 cars, 1 motorcycle, 88.5ms\n",
      "Speed: 2.0ms preprocess, 88.5ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.93, Bounding Box: 71.31, 42.14, 239.44, 173.33 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 264.29, 60.89, 400.11, 340.56 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 42.72, 0.12, 112.21, 55.94 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 2.0, Confidence: 0.81, Bounding Box: 388.32, 12.01, 639.82, 240.51 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 2.0, Confidence: 0.81, Bounding Box: 0.08, 24.74, 44.13, 180.24 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 1.0, Confidence: 0.78, Bounding Box: 287.80, 200.56, 385.16, 411.14 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 1.0, Confidence: 0.70, Bounding Box: 1.94, 342.02, 279.03, 479.76 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 381.13, 0.00, 418.48, 75.59 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 2.0, Confidence: 0.57, Bounding Box: 408.92, 29.25, 555.71, 105.59 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 1.0, Confidence: 0.53, Bounding Box: 0.12, 199.11, 87.00, 298.19 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 0.00, 0.30, 21.97, 26.75 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 2.0, Confidence: 0.41, Bounding Box: 0.29, 0.00, 32.27, 26.57 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 156.46, 58.51, 191.46, 86.75 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 189.45, 0.04, 219.00, 41.00 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 1.0, Confidence: 0.37, Bounding Box: 312.25, 4.63, 343.68, 67.12 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 1.0, Confidence: 0.34, Bounding Box: 0.04, 312.35, 190.58, 479.92 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 3.0, Confidence: 0.31, Bounding Box: 175.15, 0.33, 235.88, 43.85 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 1.0, Confidence: 0.28, Bounding Box: 0.03, 200.45, 154.11, 378.37 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 380.53, 0.00, 406.71, 73.73 for dataset/inria/Train/pos\\person_and_bike_078.png\n",
      "\n",
      "0: 640x480 12 persons, 1 bicycle, 25.0ms\n",
      "Speed: 1.0ms preprocess, 25.0ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 151.74, 93.10, 321.38, 572.97 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 93.80, 120.43, 137.69, 210.48 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 57.82, 130.88, 83.77, 207.33 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 162.02, 118.80, 195.41, 196.45 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 278.95, 114.15, 336.38, 260.09 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 348.43, 126.39, 382.68, 199.56 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 1.0, Confidence: 0.58, Bounding Box: 403.50, 178.54, 446.95, 294.10 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 39.38, 123.59, 54.48, 173.83 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.47, Bounding Box: 141.83, 123.90, 153.60, 155.48 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 459.22, 130.43, 479.85, 280.22 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 127.31, 119.58, 142.32, 170.36 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 453.41, 121.31, 479.74, 589.14 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 21.93, 120.75, 40.02, 173.28 for dataset/inria/Train/pos\\person_and_bike_117.png\n",
      "\n",
      "0: 640x480 1 person, 2 bicycles, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 116.64, 63.30, 306.66, 636.30 for dataset/inria/Train/pos\\person_and_bike_118.png\n",
      "Class: 1.0, Confidence: 0.62, Bounding Box: 0.09, 253.19, 149.15, 404.92 for dataset/inria/Train/pos\\person_and_bike_118.png\n",
      "Class: 1.0, Confidence: 0.31, Bounding Box: 62.14, 308.82, 156.96, 421.26 for dataset/inria/Train/pos\\person_and_bike_118.png\n",
      "\n",
      "0: 480x640 7 persons, 1 bicycle, 1 umbrella, 2 handbags, 10 chairs, 19.0ms\n",
      "Speed: 1.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 387.43, 88.46, 435.77, 202.45 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 1.0, Confidence: 0.79, Bounding Box: 157.82, 159.01, 228.21, 239.93 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.78, Bounding Box: 581.44, 191.62, 635.04, 265.75 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.77, Bounding Box: 514.14, 194.69, 576.62, 267.01 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.75, Bounding Box: 407.14, 198.30, 462.36, 271.17 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.69, Bounding Box: 10.52, 239.34, 68.60, 335.46 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 0.0, Confidence: 0.68, Bounding Box: 449.69, 103.54, 494.71, 185.41 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 245.27, 98.41, 357.05, 447.95 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 219.15, 104.81, 269.20, 289.11 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.65, Bounding Box: 134.82, 227.50, 216.93, 321.21 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.65, Bounding Box: 467.88, 194.72, 514.74, 262.51 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 617.37, 132.08, 639.77, 187.22 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.45, Bounding Box: 160.01, 227.27, 216.80, 320.06 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 313.05, 104.39, 406.71, 437.61 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.38, Bounding Box: 379.39, 192.86, 418.39, 274.81 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.35, Bounding Box: 512.16, 194.24, 558.51, 267.04 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 25.0, Confidence: 0.33, Bounding Box: 419.18, 67.09, 500.16, 158.74 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 26.0, Confidence: 0.28, Bounding Box: 314.74, 139.98, 392.45, 304.91 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 26.0, Confidence: 0.27, Bounding Box: 342.18, 206.41, 390.30, 303.10 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 332.99, 103.73, 368.44, 156.84 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "Class: 56.0, Confidence: 0.26, Bounding Box: 80.50, 232.66, 129.66, 329.30 for dataset/inria/Train/pos\\person_and_bike_119.png\n",
      "\n",
      "0: 480x640 6 persons, 1 bicycle, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 129.53, 141.50, 214.21, 393.84 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 385.99, 133.19, 467.95, 423.33 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 78.14, 156.42, 125.38, 283.20 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 178.69, 142.74, 205.46, 202.31 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "Class: 1.0, Confidence: 0.47, Bounding Box: 193.49, 286.23, 267.03, 396.00 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 287.06, 190.94, 323.40, 260.00 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 206.63, 121.53, 258.22, 246.92 for dataset/inria/Train/pos\\person_and_bike_120.png\n",
      "\n",
      "0: 480x640 5 persons, 1 bicycle, 1 horse, 2 backpacks, 2 chairs, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 56.0, Confidence: 0.90, Bounding Box: 537.06, 271.41, 639.82, 386.02 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 381.14, 122.46, 456.73, 363.95 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 284.67, 104.11, 371.41, 348.77 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 164.25, 126.36, 244.85, 325.80 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 24.0, Confidence: 0.65, Bounding Box: 332.50, 218.63, 409.77, 318.99 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 1.0, Confidence: 0.53, Bounding Box: 248.49, 167.55, 292.57, 221.78 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 56.0, Confidence: 0.41, Bounding Box: 177.49, 221.07, 298.87, 387.84 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 24.0, Confidence: 0.37, Bounding Box: 187.08, 219.48, 291.34, 340.55 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 0.0, Confidence: 0.36, Bounding Box: 93.99, 122.85, 111.01, 166.90 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 93.65, 122.60, 121.70, 167.99 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "Class: 17.0, Confidence: 0.26, Bounding Box: 0.03, 145.38, 23.09, 209.46 for dataset/inria/Train/pos\\person_and_bike_121.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bicycle, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 323.88, 167.32, 400.76, 335.24 for dataset/inria/Train/pos\\person_and_bike_123.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 283.18, 172.79, 303.19, 225.18 for dataset/inria/Train/pos\\person_and_bike_123.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 262.36, 163.37, 276.98, 203.73 for dataset/inria/Train/pos\\person_and_bike_123.png\n",
      "Class: 1.0, Confidence: 0.56, Bounding Box: 341.99, 260.99, 378.38, 364.47 for dataset/inria/Train/pos\\person_and_bike_123.png\n",
      "\n",
      "0: 480x640 4 persons, 2 bicycles, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 14.72, 178.80, 53.66, 288.94 for dataset/inria/Train/pos\\person_and_bike_124.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 283.88, 162.11, 345.01, 304.79 for dataset/inria/Train/pos\\person_and_bike_124.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 41.74, 165.39, 73.18, 264.66 for dataset/inria/Train/pos\\person_and_bike_124.png\n",
      "Class: 1.0, Confidence: 0.66, Bounding Box: 299.89, 250.83, 337.82, 333.85 for dataset/inria/Train/pos\\person_and_bike_124.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 0.00, 182.36, 21.32, 299.52 for dataset/inria/Train/pos\\person_and_bike_124.png\n",
      "Class: 1.0, Confidence: 0.29, Bounding Box: 309.86, 255.31, 337.36, 333.77 for dataset/inria/Train/pos\\person_and_bike_124.png\n",
      "\n",
      "0: 480x640 8 persons, 1 bicycle, 1 car, 1 traffic light, 1 handbag, 3 chairs, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 196.79, 132.31, 269.49, 336.09 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 452.88, 132.50, 556.51, 403.56 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 331.43, 145.82, 385.17, 281.53 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 77.38, 148.23, 118.67, 255.27 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 56.0, Confidence: 0.54, Bounding Box: 7.12, 208.84, 51.25, 264.62 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 1.0, Confidence: 0.51, Bounding Box: 146.52, 166.15, 197.20, 194.84 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 56.0, Confidence: 0.49, Bounding Box: 2.11, 210.11, 38.33, 265.83 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 404.79, 140.06, 430.89, 190.86 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 26.0, Confidence: 0.46, Bounding Box: 253.69, 233.75, 268.16, 267.19 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 108.35, 144.54, 135.90, 194.79 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 56.0, Confidence: 0.38, Bounding Box: 23.89, 208.90, 51.65, 261.58 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 372.61, 146.90, 388.71, 181.14 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 9.0, Confidence: 0.32, Bounding Box: 515.12, 0.12, 556.26, 45.03 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 428.16, 139.71, 479.47, 266.84 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "Class: 2.0, Confidence: 0.29, Bounding Box: 287.92, 158.85, 331.18, 205.94 for dataset/inria/Train/pos\\person_and_bike_125.png\n",
      "\n",
      "0: 480x640 10 persons, 1 bicycle, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 250.35, 220.51, 333.84, 427.27 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 318.75, 66.73, 429.32, 417.37 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 605.33, 89.17, 629.89, 159.84 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 441.75, 86.48, 459.55, 140.39 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 1.0, Confidence: 0.66, Bounding Box: 476.62, 126.36, 498.65, 164.34 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.65, Bounding Box: 469.38, 88.99, 500.21, 152.11 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 623.20, 92.85, 639.75, 160.52 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.53, Bounding Box: 104.32, 78.90, 120.07, 118.16 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 308.26, 84.72, 320.31, 111.40 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 502.73, 88.38, 520.97, 135.48 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 290.67, 84.77, 307.38, 130.38 for dataset/inria/Train/pos\\person_and_bike_127.png\n",
      "\n",
      "0: 480x640 2 persons, 2 bicycles, 5 cars, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 341.95, 122.55, 438.36, 384.52 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 0.0, Confidence: 0.88, Bounding Box: 255.99, 219.77, 344.98, 394.86 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 2.0, Confidence: 0.87, Bounding Box: 487.55, 126.26, 640.00, 179.85 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 2.0, Confidence: 0.55, Bounding Box: 87.28, 127.25, 219.99, 185.22 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 87.98, 127.58, 219.12, 164.31 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 1.0, Confidence: 0.40, Bounding Box: 292.71, 146.25, 322.32, 192.45 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 1.0, Confidence: 0.37, Bounding Box: 268.32, 146.16, 297.82, 189.66 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 2.0, Confidence: 0.36, Bounding Box: 85.44, 128.17, 192.17, 150.08 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "Class: 2.0, Confidence: 0.32, Bounding Box: 77.56, 114.61, 117.84, 141.98 for dataset/inria/Train/pos\\person_and_bike_128.png\n",
      "\n",
      "0: 480x640 12 persons, 2 bicycles, 1 umbrella, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 476.95, 191.19, 534.16, 334.62 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 202.71, 190.47, 258.25, 344.70 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 286.21, 190.43, 329.09, 304.03 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.80, Bounding Box: 403.37, 194.19, 441.49, 308.07 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 348.52, 192.14, 377.32, 263.33 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 132.23, 192.76, 173.18, 323.34 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 75.83, 195.39, 108.49, 239.32 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 179.72, 198.89, 212.61, 305.18 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 377.21, 191.33, 404.52, 260.90 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 1.0, Confidence: 0.40, Bounding Box: 381.86, 244.25, 413.20, 309.20 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 162.46, 196.09, 186.04, 302.19 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 255.67, 189.34, 292.27, 304.39 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 1.0, Confidence: 0.29, Bounding Box: 381.06, 237.83, 441.27, 308.39 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 338.01, 193.72, 349.16, 230.49 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "Class: 25.0, Confidence: 0.25, Bounding Box: 42.57, 144.01, 179.63, 190.45 for dataset/inria/Train/pos\\person_and_bike_129.png\n",
      "\n",
      "0: 480x640 5 persons, 1 bicycle, 1 tie, 6 potted plants, 18.0ms\n",
      "Speed: 1.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 283.07, 182.86, 335.33, 329.14 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 552.94, 175.98, 582.62, 282.56 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 342.24, 182.13, 406.28, 329.61 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 58.0, Confidence: 0.71, Bounding Box: 175.06, 249.54, 208.50, 295.51 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 58.0, Confidence: 0.51, Bounding Box: 174.53, 188.03, 214.02, 220.85 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 0.0, Confidence: 0.46, Bounding Box: 333.74, 180.17, 364.43, 261.79 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 58.0, Confidence: 0.42, Bounding Box: 127.04, 177.17, 171.79, 305.53 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 58.0, Confidence: 0.37, Bounding Box: 234.77, 35.51, 306.21, 111.74 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 1.0, Confidence: 0.37, Bounding Box: 425.60, 213.30, 451.30, 235.19 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 58.0, Confidence: 0.33, Bounding Box: 254.44, 237.13, 272.63, 267.71 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 0.0, Confidence: 0.32, Bounding Box: 334.02, 181.46, 366.57, 288.06 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 27.0, Confidence: 0.30, Bounding Box: 368.10, 205.08, 379.71, 239.14 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "Class: 58.0, Confidence: 0.29, Bounding Box: 172.19, 188.04, 219.06, 294.13 for dataset/inria/Train/pos\\person_and_bike_130.png\n",
      "\n",
      "0: 480x640 9 persons, 1 bicycle, 1 handbag, 44.5ms\n",
      "Speed: 2.0ms preprocess, 44.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 384.29, 168.28, 489.42, 404.34 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 275.90, 194.05, 329.82, 389.90 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 321.80, 200.36, 363.56, 380.11 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 186.65, 181.87, 220.21, 293.88 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 1.0, Confidence: 0.68, Bounding Box: 215.68, 228.81, 240.55, 292.21 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 513.33, 187.82, 534.92, 250.18 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 502.83, 191.33, 521.95, 250.45 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 247.76, 192.63, 264.20, 244.55 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 503.59, 188.78, 535.31, 251.08 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 26.0, Confidence: 0.48, Bounding Box: 271.23, 294.85, 297.12, 336.05 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 367.46, 226.94, 379.46, 263.52 for dataset/inria/Train/pos\\person_and_bike_132.png\n",
      "\n",
      "0: 480x640 11 persons, 4 bicycles, 2 umbrellas, 1 handbag, 1 potted plant, 31.0ms\n",
      "Speed: 1.0ms preprocess, 31.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 104.94, 123.64, 202.85, 380.41 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 240.06, 118.52, 353.09, 423.03 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 332.62, 107.81, 434.59, 414.66 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 508.75, 121.89, 552.50, 246.03 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 430.56, 144.34, 464.53, 245.78 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 235.79, 121.28, 255.74, 173.74 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 452.91, 122.22, 476.19, 176.04 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 58.0, Confidence: 0.50, Bounding Box: 20.77, 97.25, 81.81, 209.11 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 1.0, Confidence: 0.49, Bounding Box: 19.52, 201.10, 47.79, 263.31 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 25.0, Confidence: 0.49, Bounding Box: 387.27, 107.80, 427.51, 123.53 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 412.42, 124.00, 439.13, 203.73 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.35, Bounding Box: 259.41, 123.85, 275.43, 170.44 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 1.0, Confidence: 0.31, Bounding Box: 17.03, 178.31, 48.37, 262.13 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 26.0, Confidence: 0.29, Bounding Box: 104.75, 266.99, 123.76, 346.18 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 25.0, Confidence: 0.27, Bounding Box: 104.73, 265.90, 123.58, 345.89 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 1.0, Confidence: 0.27, Bounding Box: 11.07, 160.88, 48.77, 262.52 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.26, Bounding Box: 327.70, 118.82, 358.81, 174.14 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 252.28, 124.09, 264.24, 171.45 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "Class: 1.0, Confidence: 0.25, Bounding Box: 0.00, 137.73, 28.91, 300.29 for dataset/inria/Train/pos\\person_and_bike_134.png\n",
      "\n",
      "0: 480x640 10 persons, 1 bicycle, 2 umbrellas, 1 handbag, 1 potted plant, 39.0ms\n",
      "Speed: 2.0ms preprocess, 39.0ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 268.29, 116.32, 358.65, 368.02 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 401.71, 165.37, 455.30, 373.65 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 214.21, 124.35, 282.21, 379.02 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 473.70, 123.08, 521.76, 228.72 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 448.72, 125.29, 466.61, 183.52 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 58.0, Confidence: 0.50, Bounding Box: 7.59, 105.59, 64.00, 207.59 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 25.0, Confidence: 0.50, Bounding Box: 503.12, 57.09, 541.06, 147.58 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 25.0, Confidence: 0.49, Bounding Box: 383.15, 114.52, 443.17, 132.40 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 26.0, Confidence: 0.49, Bounding Box: 256.25, 246.96, 300.37, 329.50 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 1.0, Confidence: 0.45, Bounding Box: 433.23, 157.00, 457.84, 207.40 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 202.69, 129.19, 217.26, 185.17 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.40, Bounding Box: 387.01, 129.80, 433.27, 209.51 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.37, Bounding Box: 366.53, 137.24, 380.85, 175.57 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.34, Bounding Box: 378.47, 130.28, 399.33, 174.08 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 358.68, 134.83, 370.17, 174.12 for dataset/inria/Train/pos\\person_and_bike_135.png\n",
      "\n",
      "0: 480x640 10 persons, 1 bicycle, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 0.07, 174.90, 85.62, 438.99 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 1.0, Confidence: 0.90, Bounding Box: 504.85, 251.89, 639.65, 420.03 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 243.91, 154.35, 347.76, 418.84 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 333.23, 125.87, 429.19, 415.16 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.79, Bounding Box: 145.09, 161.72, 184.68, 276.78 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.78, Bounding Box: 115.16, 153.72, 152.38, 272.84 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 83.53, 158.72, 120.21, 300.65 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.75, Bounding Box: 562.91, 148.88, 614.38, 282.83 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.72, Bounding Box: 173.07, 147.47, 212.31, 273.94 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 200.49, 155.46, 234.63, 271.72 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 320.37, 144.49, 359.06, 194.76 for dataset/inria/Train/pos\\person_and_bike_137.png\n",
      "\n",
      "0: 480x640 4 persons, 1 bicycle, 1 car, 2 traffic lights, 1 stop sign, 12.1ms\n",
      "Speed: 1.7ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 317.81, 173.97, 386.57, 334.00 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 447.84, 164.71, 476.60, 237.73 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 2.0, Confidence: 0.85, Bounding Box: 259.68, 159.39, 316.22, 190.51 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 470.68, 157.22, 489.76, 211.13 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 1.0, Confidence: 0.63, Bounding Box: 331.18, 258.39, 375.71, 355.74 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 9.0, Confidence: 0.57, Bounding Box: 0.00, 19.82, 31.25, 54.94 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 11.0, Confidence: 0.46, Bounding Box: 469.15, 67.88, 513.11, 112.15 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 339.46, 151.84, 354.10, 182.70 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "Class: 9.0, Confidence: 0.30, Bounding Box: 0.00, 67.13, 38.62, 117.40 for dataset/inria/Train/pos\\person_and_bike_138.png\n",
      "\n",
      "0: 480x640 3 persons, 1 bicycle, 3 handbags, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 259.29, 134.31, 361.99, 413.75 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 222.24, 209.29, 252.93, 312.28 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 494.10, 201.18, 520.89, 283.34 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "Class: 26.0, Confidence: 0.44, Bounding Box: 337.62, 286.23, 370.07, 383.33 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "Class: 1.0, Confidence: 0.38, Bounding Box: 178.51, 242.51, 223.63, 329.29 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "Class: 26.0, Confidence: 0.35, Bounding Box: 258.05, 228.53, 297.51, 296.67 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "Class: 26.0, Confidence: 0.32, Bounding Box: 259.81, 246.80, 295.46, 297.22 for dataset/inria/Train/pos\\person_and_bike_139.png\n",
      "\n",
      "0: 480x640 6 persons, 1 umbrella, 1 tie, 9 chairs, 21.5ms\n",
      "Speed: 1.0ms preprocess, 21.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 263.80, 76.62, 380.81, 462.18 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.81, Bounding Box: 441.50, 265.45, 529.89, 379.59 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.78, Bounding Box: 45.93, 263.10, 129.88, 370.22 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 369.65, 77.59, 467.33, 451.35 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.76, Bounding Box: 188.79, 260.45, 244.67, 362.88 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 217.12, 114.66, 289.96, 441.72 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.75, Bounding Box: 509.52, 259.08, 567.55, 374.93 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.73, Bounding Box: 111.60, 258.04, 192.80, 355.12 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 74.10, 125.09, 121.59, 268.45 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 119.13, 119.39, 179.69, 264.32 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 586.63, 160.41, 637.15, 207.96 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.63, Bounding Box: 576.15, 235.39, 639.68, 354.11 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.44, Bounding Box: 569.04, 205.73, 616.03, 252.97 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.39, Bounding Box: 0.00, 263.16, 50.23, 377.25 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 25.0, Confidence: 0.37, Bounding Box: 447.39, 1.08, 533.53, 261.13 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 56.0, Confidence: 0.31, Bounding Box: 463.72, 200.04, 534.97, 266.49 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "Class: 27.0, Confidence: 0.26, Bounding Box: 390.85, 137.54, 404.69, 172.61 for dataset/inria/Train/pos\\person_and_bike_142.png\n",
      "\n",
      "0: 480x640 6 persons, 3 bicycles, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 64.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 533.09, 172.14, 604.71, 352.70 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 278.45, 178.23, 324.33, 288.37 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 0.0, Confidence: 0.74, Bounding Box: 320.58, 178.59, 360.60, 296.30 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 356.49, 203.54, 396.30, 292.49 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 405.48, 182.98, 421.01, 231.59 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 391.83, 180.40, 408.46, 226.41 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 1.0, Confidence: 0.55, Bounding Box: 361.03, 238.25, 388.17, 308.42 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 1.0, Confidence: 0.44, Bounding Box: 279.96, 233.27, 319.13, 311.26 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "Class: 1.0, Confidence: 0.42, Bounding Box: 324.40, 227.23, 350.76, 298.26 for dataset/inria/Train/pos\\person_and_bike_143.png\n",
      "\n",
      "0: 480x640 2 persons, 1 bicycle, 1 chair, 1 clock, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 239.74, 113.31, 344.96, 421.88 for dataset/inria/Train/pos\\person_and_bike_146.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 300.10, 251.77, 390.61, 424.10 for dataset/inria/Train/pos\\person_and_bike_146.png\n",
      "Class: 1.0, Confidence: 0.86, Bounding Box: 515.38, 283.90, 639.66, 438.95 for dataset/inria/Train/pos\\person_and_bike_146.png\n",
      "Class: 74.0, Confidence: 0.40, Bounding Box: 0.02, 106.51, 21.26, 145.39 for dataset/inria/Train/pos\\person_and_bike_146.png\n",
      "Class: 56.0, Confidence: 0.31, Bounding Box: 460.72, 425.40, 640.00, 479.68 for dataset/inria/Train/pos\\person_and_bike_146.png\n",
      "\n",
      "0: 480x640 8 persons, 1 bicycle, 1 truck, 1 backpack, 1 handbag, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 173.85, 154.22, 245.46, 412.74 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 85.59, 147.25, 130.72, 262.43 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 272.06, 140.59, 375.07, 423.33 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 254.18, 119.62, 306.31, 318.56 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.67, Bounding Box: 37.03, 156.34, 56.86, 209.69 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 1.0, Confidence: 0.64, Bounding Box: 351.21, 207.54, 473.87, 347.50 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 12.79, 154.84, 31.98, 208.76 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 7.0, Confidence: 0.54, Bounding Box: 111.59, 124.11, 181.17, 208.36 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 26.0, Confidence: 0.49, Bounding Box: 0.00, 301.88, 47.22, 392.13 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 479.80, 150.37, 509.55, 235.93 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 0.00, 224.72, 29.59, 478.12 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "Class: 24.0, Confidence: 0.27, Bounding Box: 254.89, 152.63, 285.89, 217.02 for dataset/inria/Train/pos\\person_and_bike_147.png\n",
      "\n",
      "0: 480x640 11 persons, 1 bicycle, 1 car, 1 motorcycle, 1 chair, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 152.60, 133.52, 288.43, 467.62 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 490.51, 121.69, 557.72, 325.17 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 356.16, 122.30, 452.55, 400.79 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 558.69, 119.14, 608.01, 264.24 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.81, Bounding Box: 466.44, 127.00, 506.50, 310.98 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 306.69, 127.05, 342.87, 222.03 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 3.0, Confidence: 0.65, Bounding Box: 266.74, 260.43, 392.77, 377.48 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 1.0, Confidence: 0.64, Bounding Box: 471.11, 278.16, 617.20, 479.10 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 271.40, 112.02, 322.30, 261.01 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.56, Bounding Box: 362.12, 124.16, 393.66, 200.66 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 595.91, 146.06, 638.16, 205.05 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 618.24, 144.23, 639.85, 286.54 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.45, Bounding Box: 437.87, 191.19, 465.56, 313.18 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 38.40, 129.39, 69.65, 201.36 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "Class: 56.0, Confidence: 0.34, Bounding Box: 86.22, 125.28, 147.85, 212.81 for dataset/inria/Train/pos\\person_and_bike_148.png\n",
      "\n",
      "0: 480x640 7 persons, 1 bicycle, 1 potted plant, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.90, Bounding Box: 306.44, 171.76, 384.06, 369.23 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 238.28, 165.52, 306.67, 357.43 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 379.15, 190.00, 433.43, 349.08 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 0.0, Confidence: 0.55, Bounding Box: 539.44, 174.97, 565.87, 230.93 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 58.0, Confidence: 0.50, Bounding Box: 118.01, 288.62, 159.16, 385.39 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 0.0, Confidence: 0.42, Bounding Box: 580.59, 177.49, 601.20, 216.99 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 1.0, Confidence: 0.35, Bounding Box: 120.92, 390.03, 339.58, 479.72 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 548.55, 177.29, 567.07, 228.26 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "Class: 0.0, Confidence: 0.30, Bounding Box: 224.43, 153.65, 257.61, 201.79 for dataset/inria/Train/pos\\person_and_bike_150.png\n",
      "\n",
      "0: 480x640 5 persons, 2 bicycles, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.87, Bounding Box: 168.09, 120.79, 249.87, 296.57 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 321.29, 114.91, 397.32, 348.10 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 0.00, 132.54, 46.47, 479.50 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "Class: 0.0, Confidence: 0.64, Bounding Box: 389.61, 148.91, 413.25, 233.32 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 247.96, 127.67, 291.32, 227.12 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "Class: 1.0, Confidence: 0.55, Bounding Box: 245.61, 180.43, 286.56, 254.46 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "Class: 1.0, Confidence: 0.28, Bounding Box: 283.79, 185.87, 306.95, 237.01 for dataset/inria/Train/pos\\person_and_bike_151.png\n",
      "\n",
      "0: 480x640 7 persons, 44.5ms\n",
      "Speed: 2.0ms preprocess, 44.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.85, Bounding Box: 163.87, 108.96, 216.32, 257.80 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "Class: 0.0, Confidence: 0.82, Bounding Box: 135.29, 132.50, 158.00, 187.52 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "Class: 0.0, Confidence: 0.70, Bounding Box: 221.63, 133.39, 301.19, 302.39 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "Class: 0.0, Confidence: 0.62, Bounding Box: 123.84, 130.94, 140.40, 187.26 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "Class: 0.0, Confidence: 0.59, Bounding Box: 109.66, 128.75, 125.28, 181.54 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "Class: 0.0, Confidence: 0.57, Bounding Box: 213.38, 134.30, 246.90, 253.58 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "Class: 0.0, Confidence: 0.31, Bounding Box: 94.82, 137.01, 104.49, 170.43 for dataset/inria/Train/pos\\person_and_bike_152.png\n",
      "\n",
      "0: 480x640 1 person, 2 bicycles, 1 bottle, 22.0ms\n",
      "Speed: 15.0ms preprocess, 22.0ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 206.94, 33.85, 474.45, 474.64 for dataset/inria/Train/pos\\person_and_bike_155.png\n",
      "Class: 39.0, Confidence: 0.73, Bounding Box: 605.97, 298.00, 627.35, 357.49 for dataset/inria/Train/pos\\person_and_bike_155.png\n",
      "Class: 1.0, Confidence: 0.38, Bounding Box: 188.21, 21.73, 222.32, 106.30 for dataset/inria/Train/pos\\person_and_bike_155.png\n",
      "Class: 1.0, Confidence: 0.26, Bounding Box: 127.47, 22.89, 213.45, 106.02 for dataset/inria/Train/pos\\person_and_bike_155.png\n",
      "\n",
      "0: 480x640 1 person, 2 bicycles, 11 cars, 76.5ms\n",
      "Speed: 4.0ms preprocess, 76.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.86, Bounding Box: 0.07, 156.16, 142.68, 374.64 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 0.0, Confidence: 0.84, Bounding Box: 362.75, 118.29, 445.06, 307.56 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.73, Bounding Box: 150.74, 111.64, 269.40, 235.86 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.66, Bounding Box: 207.34, 109.52, 290.25, 196.10 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.56, Bounding Box: 37.27, 149.49, 186.70, 285.93 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.51, Bounding Box: 300.53, 109.52, 354.61, 157.44 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 1.0, Confidence: 0.51, Bounding Box: 377.66, 220.34, 439.00, 334.34 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.47, Bounding Box: 45.30, 122.95, 222.57, 263.45 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.44, Bounding Box: 223.23, 111.04, 283.18, 212.66 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 1.0, Confidence: 0.43, Bounding Box: 387.86, 223.09, 427.37, 334.50 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.42, Bounding Box: 203.24, 81.70, 308.06, 185.50 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.40, Bounding Box: 174.05, 130.43, 230.69, 256.03 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.38, Bounding Box: 331.98, 100.70, 389.95, 136.28 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 153.43, 125.04, 233.04, 249.12 for dataset/inria/Train/pos\\person_and_bike_156.png\n",
      "\n",
      "0: 480x640 3 persons, 3 bicycles, 8 cars, 2 trucks, 1 backpack, 25.4ms\n",
      "Speed: 1.8ms preprocess, 25.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.89, Bounding Box: 378.67, 9.36, 445.66, 82.97 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 0.19, 141.83, 181.37, 438.80 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 323.88, 70.72, 455.51, 387.91 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.84, Bounding Box: 237.30, 42.82, 317.43, 136.63 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.83, Bounding Box: 0.03, 68.27, 128.39, 190.61 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.78, Bounding Box: 533.67, 3.43, 565.90, 37.45 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 0.0, Confidence: 0.77, Bounding Box: 575.29, 0.00, 621.35, 97.60 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 1.0, Confidence: 0.76, Bounding Box: 337.20, 264.53, 399.39, 431.83 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 560.98, 193.10, 639.52, 478.83 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 465.27, 0.00, 540.35, 52.98 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 7.0, Confidence: 0.61, Bounding Box: 237.10, 0.00, 398.36, 105.88 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 24.0, Confidence: 0.56, Bounding Box: 325.37, 109.24, 410.60, 209.52 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 7.0, Confidence: 0.49, Bounding Box: 39.52, 17.57, 251.44, 165.55 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.49, Bounding Box: 39.14, 17.49, 251.53, 165.41 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 1.0, Confidence: 0.37, Bounding Box: 582.04, 47.57, 612.91, 121.57 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 1.0, Confidence: 0.36, Bounding Box: 333.42, 251.03, 419.17, 433.77 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "Class: 2.0, Confidence: 0.32, Bounding Box: 526.06, 3.69, 548.64, 42.17 for dataset/inria/Train/pos\\person_and_bike_157.png\n",
      "\n",
      "0: 480x640 16 persons, 16.2ms\n",
      "Speed: 2.0ms preprocess, 16.2ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 329.41, 104.66, 448.72, 382.76 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 381.21, 90.38, 429.46, 179.02 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.69, Bounding Box: 233.58, 91.57, 247.69, 133.31 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 508.58, 96.90, 523.66, 132.30 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 245.03, 88.81, 259.52, 132.98 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.61, Bounding Box: 196.86, 97.90, 215.01, 156.87 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.60, Bounding Box: 29.20, 103.71, 50.99, 153.51 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 53.10, 100.15, 63.80, 130.74 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.54, Bounding Box: 495.39, 94.96, 509.83, 132.25 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 31.93, 127.31, 51.35, 153.65 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.39, Bounding Box: 79.73, 101.57, 93.80, 134.38 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 209.95, 91.99, 223.88, 131.47 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.38, Bounding Box: 376.22, 88.62, 393.04, 114.38 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 14.55, 103.02, 28.45, 137.02 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.33, Bounding Box: 18.73, 103.31, 29.76, 136.11 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "Class: 0.0, Confidence: 0.25, Bounding Box: 42.92, 103.08, 52.33, 129.72 for dataset/inria/Train/pos\\person_and_bike_166.png\n",
      "\n",
      "0: 480x640 12 persons, 1 potted plant, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.89, Bounding Box: 272.33, 145.40, 384.43, 429.10 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.83, Bounding Box: 254.70, 139.55, 315.12, 386.33 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.76, Bounding Box: 0.00, 157.97, 21.53, 242.18 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.73, Bounding Box: 236.27, 153.29, 253.87, 203.45 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.66, Bounding Box: 91.30, 155.27, 110.50, 221.24 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.63, Bounding Box: 108.30, 168.14, 125.43, 220.55 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.58, Bounding Box: 31.62, 162.88, 48.64, 214.18 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 58.0, Confidence: 0.56, Bounding Box: 425.25, 66.84, 552.10, 310.18 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.49, Bounding Box: 73.50, 155.58, 93.81, 206.89 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 15.56, 153.64, 33.78, 208.54 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.48, Bounding Box: 59.05, 156.73, 74.13, 203.69 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.44, Bounding Box: 148.54, 158.46, 162.99, 204.38 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "Class: 0.0, Confidence: 0.43, Bounding Box: 253.19, 155.11, 267.66, 202.00 for dataset/inria/Train/pos\\person_and_bike_191.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 3 cars, 12.0ms\n",
      "Speed: 5.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.91, Bounding Box: 384.63, 145.27, 552.35, 264.40 for dataset/inria/Train/pos\\person_and_bike_207.png\n",
      "Class: 0.0, Confidence: 0.86, Bounding Box: 300.89, 111.12, 386.22, 310.61 for dataset/inria/Train/pos\\person_and_bike_207.png\n",
      "Class: 1.0, Confidence: 0.74, Bounding Box: 308.24, 197.73, 383.83, 345.30 for dataset/inria/Train/pos\\person_and_bike_207.png\n",
      "Class: 2.0, Confidence: 0.52, Bounding Box: 117.81, 160.39, 415.41, 315.49 for dataset/inria/Train/pos\\person_and_bike_207.png\n",
      "Class: 2.0, Confidence: 0.31, Bounding Box: 117.41, 159.27, 325.75, 315.76 for dataset/inria/Train/pos\\person_and_bike_207.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 5 cars, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 0.0, Confidence: 0.92, Bounding Box: 248.87, 99.45, 411.93, 385.24 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "Class: 2.0, Confidence: 0.88, Bounding Box: 0.01, 152.33, 76.50, 359.96 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "Class: 2.0, Confidence: 0.72, Bounding Box: 3.37, 121.52, 124.50, 217.59 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "Class: 2.0, Confidence: 0.54, Bounding Box: 48.47, 97.03, 108.17, 124.85 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "Class: 2.0, Confidence: 0.39, Bounding Box: 155.85, 135.03, 210.13, 181.27 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "Class: 2.0, Confidence: 0.30, Bounding Box: 97.31, 129.57, 180.90, 205.74 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "Class: 1.0, Confidence: 0.29, Bounding Box: 263.23, 222.89, 346.28, 403.19 for dataset/inria/Train/pos\\person_and_bike_208.png\n",
      "\n",
      "0: 480x640 1 person, 1 bicycle, 3 cars, 1 backpack, 1 handbag, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class: 2.0, Confidence: 0.93, Bounding Box: 74.45, 158.07, 336.86, 273.01 for dataset/inria/Train/pos\\person_and_bike_209.png\n",
      "Class: 0.0, Confidence: 0.91, Bounding Box: 334.25, 111.64, 417.57, 303.91 for dataset/inria/Train/pos\\person_and_bike_209.png\n",
      "Class: 2.0, Confidence: 0.65, Bounding Box: 0.00, 124.06, 88.70, 243.36 for dataset/inria/Train/pos\\person_and_bike_209.png\n",
      "Class: 2.0, Confidence: 0.64, Bounding Box: 75.01, 160.54, 152.04, 216.41 for dataset/inria/Train/pos\\person_and_bike_209.png\n",
      "Class: 1.0, Confidence: 0.62, Bounding Box: 362.50, 214.67, 402.02, 332.76 for dataset/inria/Train/pos\\person_and_bike_209.png\n",
      "Class: 26.0, Confidence: 0.34, Bounding Box: 328.90, 202.92, 356.88, 252.03 for dataset/inria/Train/pos\\person_and_bike_209.png\n",
      "Class: 24.0, Confidence: 0.33, Bounding Box: 350.04, 135.41, 408.63, 219.01 for dataset/inria/Train/pos\\person_and_bike_209.png\n"
     ]
    }
   ],
   "source": [
    "model_name = \"yolov8n\"\n",
    "yolo_model = load_yolo_model(model_name)\n",
    "input_dir = \"dataset/inria/Train/pos/\"\n",
    "output_dir = f\"dataset/inria/Train/pos/yolo-labels_{model_name}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all images\n",
    "for image_file in glob.glob(os.path.join(input_dir, '*.png')):\n",
    "    detect_and_save_labels(image_file, yolo_model, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
