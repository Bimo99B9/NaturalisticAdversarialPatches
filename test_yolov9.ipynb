{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov9t.pt to 'yolov9t.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.70M/4.70M [00:00<00:00, 42.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\danil\\Documents\\Github\\NaturalisticAdversarialPatches\\crop001692.png: 640x224 7 persons, 2 cars, 100.5ms\n",
      "Speed: 3.0ms preprocess, 100.5ms inference, 112.6ms postprocess per image at shape (1, 3, 640, 224)\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model_name = \"yolov9\"\n",
    "cls_id_attacked = 0\n",
    "cls_conf_threshold = 0.5\n",
    "batch_size = 1\n",
    "device = get_default_device()\n",
    "\n",
    "detector = YOLO(\"yolov9t.pt\")\n",
    "\n",
    "# Use the model\n",
    "results = detector(\"crop001692.png\")  # Predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 1 probability: tensor([0.8569], device='cuda:0')\n",
      "Object 2 probability: tensor([0.8447], device='cuda:0')\n",
      "Object 3 probability: tensor([0.8405], device='cuda:0')\n",
      "Object 4 probability: tensor([0.7501], device='cuda:0')\n",
      "Object 5 probability: tensor([0.7456], device='cuda:0')\n",
      "Object 6 probability: tensor([0.6432], device='cuda:0')\n",
      "Object 7 probability: tensor([0.5573], device='cuda:0')\n",
      "Object 8 probability: tensor([0.4001], device='cuda:0')\n",
      "Object 9 probability: tensor([0.3985], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, box in enumerate(results[0].boxes):\n",
    "    obj_prob = box.conf\n",
    "    print(f\"Object {i+1} probability: {obj_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\danil\\Documents\\Github\\NaturalisticAdversarialPatches\\crop001692.png: 640x224 7 persons, 2 cars, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Boxes retrieved: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[ 22,  17,  19],\n",
      "        [ 22,  17,  19],\n",
      "        [ 22,  17,  19],\n",
      "        ...,\n",
      "        [223, 224, 204],\n",
      "        [234, 230, 212],\n",
      "        [235, 230, 215]],\n",
      "\n",
      "       [[ 21,  16,  18],\n",
      "        [ 21,  16,  18],\n",
      "        [ 20,  15,  17],\n",
      "        ...,\n",
      "        [232, 233, 217],\n",
      "        [242, 244, 225],\n",
      "        [245, 248, 232]],\n",
      "\n",
      "       [[ 20,  15,  17],\n",
      "        [ 20,  15,  17],\n",
      "        [ 20,  15,  17],\n",
      "        ...,\n",
      "        [234, 233, 229],\n",
      "        [236, 237, 233],\n",
      "        [235, 240, 238]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[197, 209, 209],\n",
      "        [196, 208, 208],\n",
      "        [192, 204, 204],\n",
      "        ...,\n",
      "        [ 22,  27,  26],\n",
      "        [ 44,  44,  44],\n",
      "        [ 43,  39,  38]],\n",
      "\n",
      "       [[189, 201, 201],\n",
      "        [185, 197, 197],\n",
      "        [185, 197, 197],\n",
      "        ...,\n",
      "        [ 23,  22,  24],\n",
      "        [ 35,  32,  34],\n",
      "        [ 40,  35,  37]],\n",
      "\n",
      "       [[197, 214, 211],\n",
      "        [205, 220, 216],\n",
      "        [215, 229, 225],\n",
      "        ...,\n",
      "        [ 21,  20,  24],\n",
      "        [ 26,  25,  27],\n",
      "        [ 30,  30,  30]]], dtype=uint8)\n",
      "orig_shape: (862, 302)\n",
      "path: 'c:\\\\Users\\\\danil\\\\Documents\\\\Github\\\\NaturalisticAdversarialPatches\\\\crop001692.png'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 0.9708404541015625, 'inference': 24.00040626525879, 'postprocess': 1.9989013671875}]\n"
     ]
    }
   ],
   "source": [
    "# Obtain all bboxes\n",
    "if model_name == \"yolov9\":\n",
    "    bboxes = detector(\"crop001692.png\")\n",
    "\n",
    "print(f\"Boxes retrieved: {bboxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.331988429391621 0.44909448468768126 0.42728040076249485 0.4036171331206497\n",
      "0 0.7956454043356788 0.5453458639972448 0.35470454740208507 0.4196703351014175\n",
      "0 0.6512925103800187 0.1292433506374846 0.3033967681278456 0.2502487253423633\n",
      "0 0.7328669061723924 0.8797611670261746 0.5326259183567881 0.24022354161379786\n",
      "0 0.9143650484400869 0.1917237815060361 0.16819813709385348 0.29691840435125433\n",
      "0 0.4114551165246016 0.09527373701126558 0.21717733421073054 0.19054747402253117\n",
      "0 0.12664231559298686 0.11378315264000417 0.2522693659296099 0.15173936388055576\n",
      "Labels: [[          0     0.33199     0.44909     0.42728     0.40362     0.85693]\n",
      " [          0     0.79565     0.54535      0.3547     0.41967      0.8447]\n",
      " [          0     0.65129     0.12924      0.3034     0.25025     0.75014]\n",
      " [          0     0.73287     0.87976     0.53263     0.24022     0.74556]\n",
      " [          0     0.91437     0.19172      0.1682     0.29692     0.55727]\n",
      " [          0     0.41146    0.095274     0.21718     0.19055      0.4001]\n",
      " [          0     0.12664     0.11378     0.25227     0.15174     0.39854]]\n",
      "Labels rescale: [[          0     0.85693      35.741      213.16      164.78      561.08]\n",
      " [          0      0.8447      186.72      289.21      293.85      650.97]\n",
      " [          0     0.75014      150.88      3.5506       242.5      219.26]\n",
      " [          0     0.74556       140.9      654.82      301.75      861.89]\n",
      " [          0     0.55727      250.74      37.294      301.54      293.24]\n",
      " [          0      0.4001      91.466           0      157.05      164.25]\n",
      " [          0     0.39854      0.1533      32.681      76.339      163.48]]\n"
     ]
    }
   ],
   "source": [
    "# Objective: Retrieve labels and labels_rescale in this format:\n",
    "labels = []  # format:  (label, x_center, y_center, w, h)  ex:(0 0.5 0.6 0.07 0.22)\n",
    "labels_rescale = (\n",
    "    []\n",
    ")  # format:  (label, confidence, left, top, right, bottom)  ex:(person 0.76 0.6 183.1 113.5 240.3 184.7)\n",
    "\n",
    "if len(bboxes) == batch_size:\n",
    "    bbox = bboxes[0]\n",
    "\n",
    "for b in bbox.boxes:\n",
    "    detected_class = int(b.cls.cpu().item())\n",
    "    orig_width, orig_height = bbox.boxes.orig_shape[1], bbox.boxes.orig_shape[0]\n",
    "    if detected_class == int(cls_id_attacked):\n",
    "        conf = b.conf.cpu().item()\n",
    "        # For labels: using xywh format\n",
    "        x_center, y_center, w, h = (\n",
    "            b.xywh[0][0].cpu().item() / orig_width,\n",
    "            b.xywh[0][1].cpu().item() / orig_height,\n",
    "            b.xywh[0][2].cpu().item() / orig_width,\n",
    "            b.xywh[0][3].cpu().item() / orig_height,\n",
    "        )\n",
    "        label = np.array(\n",
    "            [detected_class, x_center, y_center, w, h, conf], dtype=np.float32\n",
    "        )\n",
    "        labels.append(label)\n",
    "        # For labels_rescale: using xyxy format\n",
    "        left, top, right, bottom = (\n",
    "            b.xyxy[0][0].cpu().item(),\n",
    "            b.xyxy[0][1].cpu().item(),\n",
    "            b.xyxy[0][2].cpu().item(),\n",
    "            b.xyxy[0][3].cpu().item(),\n",
    "        )\n",
    "        label_rescale = np.array(\n",
    "            [detected_class, conf, left, top, right, bottom], dtype=np.float32\n",
    "        )\n",
    "        labels_rescale.append(label_rescale)\n",
    "        \n",
    "        print(f\"{detected_class} {x_center} {y_center} {w} {h}\")\n",
    "\n",
    "labels = np.array(labels)\n",
    "labels_rescale = np.array(labels_rescale)\n",
    "\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Labels rescale: {labels_rescale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\danil\\Documents\\Github\\NaturalisticAdversarialPatches\\crop001692.png: 640x224 7 persons, 2 cars, 29.0ms\n",
      "Speed: 2.0ms preprocess, 29.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Loss Det: 0.6504629850387573\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss_det\n",
    "target_class = cls_id_attacked\n",
    "\n",
    "results = detector(\"crop001692.png\")\n",
    "combined_probs = []\n",
    "for box in results[0].boxes:\n",
    "    obj_prob = box.conf.cpu()\n",
    "    if box.cls.cpu().item() == target_class:\n",
    "            combined_probs.append(obj_prob)\n",
    "if combined_probs:\n",
    "        loss_det = torch.mean(torch.stack(combined_probs))\n",
    "else:\n",
    "    loss_det = torch.tensor(0.0).to(device)\n",
    "\n",
    "print(f\"Loss Det: {loss_det}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
